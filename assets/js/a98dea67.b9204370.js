"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[6410],{5016:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-4-vision-language-action/ch16-llms-meet-robotics","title":"ch16-llms-meet-robotics","description":"-----","source":"@site/docs/module-4-vision-language-action/ch16-llms-meet-robotics.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch16-llms-meet-robotics","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch16-llms-meet-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch16-llms-meet-robotics.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch15-reinforcement-learning-sim2real","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real"},"next":{"title":"ch17-voice-to-action-whisper","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch17-voice-to-action-whisper"}}');var t=r(4848),o=r(8453),s=r(7242);const i={},l=void 0,c={},m=[{value:"title: Ch16  LLMs Meet Robotics\r\nmodule: 4\r\nchapter: 16\r\nsidebar_label: Ch16: LLMs Meet Robotics\r\ndescription: Integrating Large Language Models with robotics for natural language interaction and command execution\r\ntags: [llm, robotics, naturallanguage, gpt, visionlanguageaction, embodiedai, transformers]\r\ndifficulty: intermediate\r\nestimated_duration: 90",id:"title-ch16--llms-meet-roboticsmodule-4chapter-16sidebar_label-ch16-llms-meet-roboticsdescription-integrating-large-language-models-with-robotics-for-natural-language-interaction-and-command-executiontags-llm-robotics-naturallanguage-gpt-visionlanguageaction-embodiedai-transformersdifficulty-intermediateestimated_duration-90",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"LLMs in Robotics Context",id:"llms-in-robotics-context",level:3},{value:"Key Components of LLMRobotics Systems",id:"key-components-of-llmrobotics-systems",level:3},{value:"VisionLanguageAction (VLA) Models",id:"visionlanguageaction-vla-models",level:3},{value:"Grounding Language in Physical Reality",id:"grounding-language-in-physical-reality",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up LLM Integration with Robotics",id:"lab-1-setting-up-llm-integration-with-robotics",level:3},{value:"Lab 2: Creating a VisionLanguage Interface",id:"lab-2-creating-a-visionlanguage-interface",level:3},{value:"Lab 3: Implementing LanguageGrounded Navigation",id:"lab-3-implementing-languagegrounded-navigation",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"title-ch16--llms-meet-roboticsmodule-4chapter-16sidebar_label-ch16-llms-meet-roboticsdescription-integrating-large-language-models-with-robotics-for-natural-language-interaction-and-command-executiontags-llm-robotics-naturallanguage-gpt-visionlanguageaction-embodiedai-transformersdifficulty-intermediateestimated_duration-90",children:"title: Ch16  LLMs Meet Robotics\r\nmodule: 4\r\nchapter: 16\r\nsidebar_label: Ch16: LLMs Meet Robotics\r\ndescription: Integrating Large Language Models with robotics for natural language interaction and command execution\r\ntags: [llm, robotics, naturallanguage, gpt, visionlanguageaction, embodiedai, transformers]\r\ndifficulty: intermediate\r\nestimated_duration: 90"}),"\n","\n",(0,t.jsx)(e.h1,{id:"llms-meet-robotics",children:"LLMs Meet Robotics"}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"Understand the integration of Large Language Models with robotic systems\r\nImplement natural language processing for robot command interpretation\r\nDesign visionlanguageaction pipelines for embodied AI\r\nCreate multimodal interfaces combining language, vision, and action\r\nEvaluate the performance of LLMpowered robotic systems\r\nDevelop techniques for grounding language in physical environments\r\nImplement safety and error handling for LLMcontrolled robots"}),"\n",(0,t.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,t.jsx)(e.h3,{id:"llms-in-robotics-context",children:"LLMs in Robotics Context"}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models (LLMs) bring natural language understanding to robotics systems. When combined with robotics, LLMs can:"}),"\n",(0,t.jsx)(s.A,{chart:"\ngraph TD;\n  A[Large Language Model] > B[Language Understanding];\n  A > C[Task Planning];\n  A > D[Command Interpretation];\n  \n  E[Robot System] > F[Sensors];\n  E > G[Actuators];\n  E > H[Navigation];\n  \n  I[VisionLanguageAction] > J[Perception];\n  I > K[Action Selection];\n  I > L[Planning];\n  \n  B > I;\n  C > I;\n  D > I;\n  F > J;\n  G > K;\n  H > L;\n  \n  M[Human] > N[Natural Language Command];\n  N > A;\n  I > O[Robotic Action];\n  O > E;\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style E fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style I fill:#FF9800,stroke:#E65100,color:#fff;\n"}),"\n",(0,t.jsx)(e.h3,{id:"key-components-of-llmrobotics-systems",children:"Key Components of LLMRobotics Systems"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Natural Language Understanding"}),": Processing human commands into actionable robot instructions."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Task Planning"}),": Breaking down highlevel commands into sequences of primitive actions."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Perception Integration"}),": Combining vision data with language understanding."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Action Grounding"}),": Connecting abstract language concepts to specific robot actions."]}),"\n",(0,t.jsx)(e.h3,{id:"visionlanguageaction-vla-models",children:"VisionLanguageAction (VLA) Models"}),"\n",(0,t.jsx)(e.p,{children:"VLA models represent a new paradigm where language understanding, visual perception, and action selection are combined in a single model:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Embodied Learning"}),": Learning from robot interactions with the environment\r\n",(0,t.jsx)(e.strong,{children:"Multimodal Fusion"}),": Combining language, vision, and proprioceptive data\r\n",(0,t.jsx)(e.strong,{children:"Policy Learning"}),": Learning to map multimodal inputs to robot actions"]}),"\n",(0,t.jsx)(e.h3,{id:"grounding-language-in-physical-reality",children:"Grounding Language in Physical Reality"}),"\n",(0,t.jsxs)(e.p,{children:["One of the biggest challenges in LLMrobotics integration is ",(0,t.jsx)(e.strong,{children:"grounding"}),"  connecting abstract language concepts to physical entities and actions in the robot's environment."]}),"\n",(0,t.jsx)(e.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,t.jsx)(e.h3,{id:"lab-1-setting-up-llm-integration-with-robotics",children:"Lab 1: Setting up LLM Integration with Robotics"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Install required dependencies"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"pip install openai anthropic transformers torch torchvision torchaudio\r\npip install gymnasium ros2 rospy geometry_msgs sensor_msgs\r\npip install langchain langchainopenai llamaindex\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create a basic LLMrobot interface class"})," (",(0,t.jsx)(e.code,{children:"llm_robot_interface.py"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport asyncio\r\nimport openai\r\nimport json\r\nimport rospy\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom typing import Dict, List, Optional\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass RobotCommand:\r\n    action: str\r\n    parameters: Dict[str, float]\r\n    confidence: float\r\n\r\n@dataclass\r\nclass PerceptualData:\r\n    laser_scan: Optional[List[float]] = None\r\n    camera_image: Optional[np.ndarray] = None\r\n    robot_pose: Optional[Dict[str, float]] = None\r\n\r\nclass LLMRobotInterface:\r\n    def __init__(self, api_key: str, model_name: str = "gpt3.5turbo"):\r\n        # Initialize LLM\r\n        openai.api_key = api_key\r\n        self.model_name = model_name\r\n        \r\n        # Initialize ROS\r\n        rospy.init_node(\'llm_robot_interface\', anonymous=True)\r\n        \r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Robot state\r\n        self.perceptual_data = PerceptualData()\r\n        self.robot_pose = {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0}\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        \r\n        # Subscribe to robot sensors\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        rospy.Subscriber(\'/camera/image_raw\', Image, self.image_callback)\r\n        \r\n        # Robot control parameters\r\n        self.linear_speed = 0.5\r\n        self.angular_speed = 0.5\r\n        \r\n        print("LLM Robot Interface initialized")\r\n    \r\n    def laser_callback(self, msg):\r\n        """Update laser scan data"""\r\n        self.perceptual_data.laser_scan = list(msg.ranges)\r\n        # Keep only finite values within range\r\n        self.perceptual_data.laser_scan = [\r\n            r for r in self.perceptual_data.laser_scan \r\n            if not (np.isinf(r) or np.isnan(r)) and 0.1 < r < 10.0\r\n        ]\r\n    \r\n    def image_callback(self, msg):\r\n        """Update camera image data"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            self.perceptual_data.camera_image = cv_image\r\n        except Exception as e:\r\n            print(f"Error converting image: {e}")\r\n    \r\n    def update_robot_pose(self, x, y, theta):\r\n        """Update robot pose estimate"""\r\n        self.perceptual_data.robot_pose = {\'x\': x, \'y\': y, \'theta\': theta}\r\n    \r\n    def interpret_command(self, user_command: str) > RobotCommand:\r\n        """Use LLM to interpret natural language command"""\r\n        # Format the prompt with current robot state\r\n        prompt = f"""\r\n        You are a robot command interpreter. Given the user\'s natural language command,\r\n        convert it to a specific robot action with parameters.\r\n        \r\n        Current robot state:\r\n         Pose: x={self.perceptual_data.robot_pose[\'x\']:.2f}, y={self.perceptual_data.robot_pose[\'y\']:.2f}, theta={self.perceptual_data.robot_pose[\'theta\']:.2f}\r\n         Laser scan: {f"Min distance {min(self.perceptual_data.laser_scan):.2f}m" if self.perceptual_data.laser_scan and len(self.perceptual_data.laser_scan) > 0 else \'No scan data\'}\r\n        \r\n        User command: "{user_command}"\r\n        \r\n        Respond with ONLY a JSON object in this exact format:\r\n        {{\r\n          "action": "forward|backward|left|right|stop|approach_object|avoid_object|turn_to_face",\r\n          "parameters": {{\r\n            "linear_speed": float,  # 1.0 to 1.0\r\n            "angular_speed": float, # 1.0 to 1.0\r\n            "distance": float,      # meters, for movements\r\n            "angle": float          # radians, for turns\r\n          }},\r\n          "confidence": float       # 0.0 to 1.0\r\n        }}\r\n        \r\n        Be specific and only respond with the JSON object.\r\n        """\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model=self.model_name,\r\n                messages=[{"role": "user", "content": prompt}],\r\n                temperature=0.1,  # Low temperature for consistency\r\n                max_tokens=200\r\n            )\r\n            \r\n            # Extract response\r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Clean up response to extract only JSON\r\n            if response_text.startswith(\'```\'):\r\n                response_text = response_text.split(\'\\n\', 1)[1]\r\n                response_text = response_text.rstrip(\'`\')\r\n            \r\n            # Parse JSON\r\n            json_data = json.loads(response_text)\r\n            \r\n            # Create RobotCommand object\r\n            command = RobotCommand(\r\n                action=json_data[\'action\'],\r\n                parameters=json_data[\'parameters\'],\r\n                confidence=json_data[\'confidence\']\r\n            )\r\n            \r\n            return command\r\n            \r\n        except Exception as e:\r\n            print(f"Error interpreting command: {e}")\r\n            # Return default command\r\n            return RobotCommand(\r\n                action="stop",\r\n                parameters={"linear_speed": 0.0, "angular_speed": 0.0},\r\n                confidence=0.5\r\n            )\r\n    \r\n    def execute_command(self, command: RobotCommand) > bool:\r\n        """Execute the interpreted command on the robot"""\r\n        msg = Twist()\r\n        \r\n        if command.confidence < 0.3:\r\n            rospy.logwarn(f"Command confidence is low ({command.confidence}), refusing to execute")\r\n            return False\r\n        \r\n        if command.action == "forward":\r\n            msg.linear.x = self.linear_speed * command.parameters.get(\'linear_speed\', 1.0)\r\n            msg.angular.z = 0.0\r\n        elif command.action == "backward":\r\n            msg.linear.x = self.linear_speed * command.parameters.get(\'linear_speed\', 1.0)\r\n            msg.angular.z = 0.0\r\n        elif command.action == "left":\r\n            msg.linear.x = 0.0\r\n            msg.angular.z = self.angular_speed * command.parameters.get(\'angular_speed\', 1.0)\r\n        elif command.action == "right":\r\n            msg.linear.x = 0.0\r\n            msg.angular.z = self.angular_speed * command.parameters.get(\'angular_speed\', 1.0)\r\n        elif command.action == "stop":\r\n            msg.linear.x = 0.0\r\n            msg.angular.z = 0.0\r\n        elif command.action == "turn_to_face":\r\n            # Calculate turn to face a direction\r\n            angle = command.parameters.get(\'angle\', 0.0)\r\n            msg.angular.z = self.angular_speed * np.sign(angle)\r\n        elif command.action == "approach_object":\r\n            # Move forward with obstacle avoidance\r\n            min_distance = min(self.perceptual_data.laser_scan) if self.perceptual_data.laser_scan else float(\'inf\')\r\n            if min_distance > 0.5:  # Safe distance\r\n                msg.linear.x = self.linear_speed * 0.5\r\n            else:\r\n                msg.linear.x = 0.0\r\n        else:\r\n            rospy.logwarn(f"Unknown action: {command.action}")\r\n            return False\r\n        \r\n        self.cmd_vel_pub.publish(msg)\r\n        rospy.loginfo(f"Executed command: {command.action} with params {command.parameters}")\r\n        return True\r\n    \r\n    def process_user_command(self, user_command: str) > bool:\r\n        """Process a complete user command: interpret and execute"""\r\n        rospy.loginfo(f"Processing user command: \'{user_command}\'")\r\n        \r\n        # Interpret command with LLM\r\n        command = self.interpret_command(user_command)\r\n        \r\n        rospy.loginfo(f"Interpreted command: {command.action}, confidence: {command.confidence:.2f}")\r\n        \r\n        # Execute command\r\n        success = self.execute_command(command)\r\n        \r\n        return success\r\n    \r\n    def run(self):\r\n        """Run the main control loop"""\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        \r\n        while not rospy.is_shutdown():\r\n            # Process any pending tasks\r\n            # In a real implementation, this might receive commands from a queue or other source\r\n            rate.sleep()\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-2-creating-a-visionlanguage-interface",children:"Lab 2: Creating a VisionLanguage Interface"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create visionlanguage processing module"})," (",(0,t.jsx)(e.code,{children:"vision_language_processor.py"}),"):","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport openai\r\nfrom PIL import Image\r\nimport base64\r\nimport io\r\nfrom typing import List, Dict, Tuple\r\nimport json\r\n\r\nclass VisionLanguageProcessor:\r\n    def __init__(self, openai_api_key: str):\r\n        openai.api_key = openai_api_key\r\n        self.openai_client = openai.OpenAI(api_key=openai_api_key)\r\n    \r\n    def encode_image(self, image: np.ndarray) > str:\r\n        """Encode a numpy image to base64 string"""\r\n        # Convert BGR to RGB if needed\r\n        if len(image.shape) == 3 and image.shape[2] == 3:\r\n            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n        else:\r\n            image_rgb = image\r\n        \r\n        # Convert to PIL\r\n        pil_image = Image.fromarray(image_rgb)\r\n        \r\n        # Save to bytes\r\n        buffer = io.BytesIO()\r\n        pil_image.save(buffer, format="JPEG")\r\n        img_bytes = buffer.getvalue()\r\n        \r\n        # Encode to base64\r\n        base64_str = base64.b64encode(img_bytes).decode(\'utf8\')\r\n        return base64_str\r\n    \r\n    def describe_scene(self, image: np.ndarray) > str:\r\n        """Generate a textual description of the scene"""\r\n        base64_image = self.encode_image(image)\r\n        \r\n        prompt = "Describe this robot\'s view of the environment in detail. Focus on objects, their positions relative to the robot, potential obstacles, navigable paths, and anything relevant for robot navigation and manipulation."\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4visionpreview",\r\n                messages=[\r\n                    {\r\n                        "role": "user",\r\n                        "content": [\r\n                            {"type": "text", "text": prompt},\r\n                            {\r\n                                "type": "image_url",\r\n                                "image_url": {\r\n                                    "url": f"data:image/jpeg;base64,{base64_image}"\r\n                                }\r\n                            }\r\n                        ]\r\n                    }\r\n                ],\r\n                max_tokens=300\r\n            )\r\n            \r\n            return response.choices[0].message.content\r\n            \r\n        except Exception as e:\r\n            print(f"Error describing scene: {e}")\r\n            return "Unable to describe the scene"\r\n    \r\n    def identify_objects(self, image: np.ndarray) > List[Dict]:\r\n        """Identify objects in the image and their locations"""\r\n        base64_image = self.encode_image(image)\r\n        \r\n        prompt = """\r\n        Identify and locate the following types of objects in the image:\r\n         Doors, windows\r\n         Tables, chairs, desks\r\n         People\r\n         Obstacles that might block robot movement\r\n         Any potentially interesting objects for robot interaction\r\n        \r\n        For each object, provide:\r\n         type: object category\r\n         position_in_image: approximate position (left, center, right, top, bottom)\r\n         estimated_distance: distance estimate if possible\r\n         relevance_for_navigation: how relevant this is for robot navigation\r\n        \r\n        Respond with only a JSON array of objects.\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4visionpreview",\r\n                messages=[\r\n                    {\r\n                        "role": "user",\r\n                        "content": [\r\n                            {"type": "text", "text": prompt},\r\n                            {\r\n                                "type": "image_url",\r\n                                "image_url": {\r\n                                    "url": f"data:image/jpeg;base64,{base64_image}"\r\n                                }\r\n                            }\r\n                        ]\r\n                    }\r\n                ],\r\n                max_tokens=500\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON if wrapped in markdown\r\n            if "```json" in response_text:\r\n                json_start = response_text.find("```json") + 7\r\n                json_end = response_text.find("```", json_start)\r\n                response_text = response_text[json_start:json_end].strip()\r\n            elif "```" in response_text:\r\n                first_json = response_text.find("{")\r\n                last_json = response_text.rfind("}") + 1\r\n                response_text = response_text[first_json:last_json].strip()\r\n            \r\n            objects = json.loads(response_text)\r\n            return objects\r\n            \r\n        except Exception as e:\r\n            print(f"Error identifying objects: {e}")\r\n            return []\r\n    \r\n    def answer_visual_question(self, image: np.ndarray, question: str) > str:\r\n        """Answer a specific question about the visual scene"""\r\n        base64_image = self.encode_image(image)\r\n        \r\n        prompt = f"Answer the following question about the image: {question}"\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4visionpreview",\r\n                messages=[\r\n                    {\r\n                        "role": "user",\r\n                        "content": [\r\n                            {"type": "text", "text": prompt},\r\n                            {\r\n                                "type": "image_url",\r\n                                "image_url": {\r\n                                    "url": f"data:image/jpeg;base64,{base64_image}"\r\n                                }\r\n                            }\r\n                        ]\r\n                    }\r\n                ],\r\n                max_tokens=200\r\n            )\r\n            \r\n            return response.choices[0].message.content\r\n            \r\n        except Exception as e:\r\n            print(f"Error answering question: {e}")\r\n            return "Unable to answer the question"\r\n    \r\n    def detect_navigation_hazards(self, image: np.ndarray) > Dict:\r\n        """Detect potential hazards for navigation"""\r\n        base64_image = self.encode_image(image)\r\n        \r\n        prompt = """\r\n        Analyze the image for potential navigation hazards for a robot. \r\n        Consider:\r\n         Dropoffs or steep edges\r\n         Narrow passages\r\n         Moving obstacles\r\n         Unstable surfaces\r\n         Areas that might trap the robot\r\n        \r\n        Rate the overall navigation safety and list specific hazards.\r\n        Respond with a JSON object containing:\r\n         safety_rating: integer 15 (1=unsafe, 5=very safe)\r\n         hazards: array of hazard descriptions\r\n         safe_paths: description of any clearly safe paths\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4visionpreview",\r\n                messages=[\r\n                    {\r\n                        "role": "user",\r\n                        "content": [\r\n                            {"type": "text", "text": prompt},\r\n                            {\r\n                                "type": "image_url",\r\n                                "image_url": {\r\n                                    "url": f"data:image/jpeg;base64,{base64_image}"\r\n                                }\r\n                            }\r\n                        ]\r\n                    }\r\n                ],\r\n                max_tokens=300\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            json_start = response_text.find("{")\r\n            json_end = response_text.rfind("}") + 1\r\n            response_text = response_text[json_start:json_end].strip()\r\n            \r\n            hazards = json.loads(response_text)\r\n            return hazards\r\n            \r\n        except Exception as e:\r\n            print(f"Error detecting hazards: {e}")\r\n            return {\r\n                "safety_rating": 3,\r\n                "hazards": ["Unable to analyze image"],\r\n                "safe_paths": "Unknown"\r\n            }\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-3-implementing-languagegrounded-navigation",children:"Lab 3: Implementing LanguageGrounded Navigation"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create a languagegrounded navigation system"})," (",(0,t.jsx)(e.code,{children:"language_navigable_planner.py"}),"):","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nimport actionlib\r\nimport tf\r\nimport numpy as np\r\nfrom typing import List, Dict, Optional\r\nfrom llm_robot_interface import LLMRobotInterface\r\nfrom vision_language_processor import VisionLanguageProcessor\r\n\r\nclass LanguageGroundedNavigator:\r\n    def __init__(self, llm_interface: LLMRobotInterface, vision_processor: VisionLanguageProcessor):\r\n        self.llm_interface = llm_interface\r\n        self.vision_processor = vision_processor\r\n        \r\n        # Initialize ROS components\r\n        self.move_base = actionlib.SimpleActionClient("move_base", MoveBaseAction)\r\n        self.move_base.wait_for_server(rospy.Duration(60))  # Wait up to 60 seconds\r\n        \r\n        self.tf_listener = tf.TransformListener()\r\n        \r\n        # Known locations in the environment (would be learned over time)\r\n        self.known_locations = {\r\n            "kitchen": {"x": 5.0, "y": 3.0, "theta": 0.0},\r\n            "living_room": {"x": 2.0, "y": 1.0, "theta": 0.0},\r\n            "bedroom": {"x": 7.0, "y": 1.0, "theta": 0.0},\r\n            "entrance": {"x": 0.0, "y": 0.0, "theta": 0.0}\r\n        }\r\n        \r\n        print("LanguageGrounded Navigator initialized")\r\n    \r\n    def parse_navigation_command(self, command: str) > Dict:\r\n        """Parse natural language navigation command using LLM"""\r\n        current_pos = self.llm_interface.perceptual_data.robot_pose\r\n        \r\n        prompt = f"""\r\n        You are a navigation command parser for a mobile robot. Interpret the user\'s command\r\n        to determine where the robot should go.\r\n        \r\n        Current position: x={current_pos[\'x\']:.1f}, y={current_pos[\'y\']:.1f}\r\n        Known locations: {list(self.known_locations.keys())}\r\n        \r\n        Navigation command: "{command}"\r\n        \r\n        Respond with a JSON object containing:\r\n        {{\r\n          "target_location": "name of target location if known, otherwise \'unknown\'",\r\n          "relative_direction": "forward, backward, left, right, or \'none\' if specific location",\r\n          "distance": float value if relative, else null,\r\n          "description": "brief description of the destination",\r\n          "landmarks": ["list", "of", "identifying", "landmarks"]\r\n        }}\r\n        \r\n        Respond with ONLY the JSON object.\r\n        """\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt3.5turbo",  # or your preferred model\r\n                messages=[{"role": "user", "content": prompt}],\r\n                temperature=0.1,\r\n                max_tokens=200\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            json_start = response_text.find("{")\r\n            json_end = response_text.rfind("}") + 1\r\n            response_text = response_text[json_start:json_end].strip()\r\n            \r\n            return json.loads(response_text)\r\n            \r\n        except Exception as e:\r\n            print(f"Error parsing navigation command: {e}")\r\n            return {\r\n                "target_location": "unknown",\r\n                "relative_direction": "none",\r\n                "distance": None,\r\n                "description": "Error parsing command",\r\n                "landmarks": []\r\n            }\r\n    \r\n    def get_location_coordinates(self, location_name: str) > Optional[Dict]:\r\n        """Get coordinates for a known location"""\r\n        if location_name in self.known_locations:\r\n            return self.known_locations[location_name]\r\n        else:\r\n            # In a complete system, this would use localization or mapping to find the location\r\n            # For now, return None\r\n            return None\r\n    \r\n    def navigate_to_waypoint(self, x: float, y: float, theta: float = 0.0) > bool:\r\n        """Navigate to specific coordinates"""\r\n        goal = MoveBaseGoal()\r\n        goal.target_pose.header.frame_id = "map"\r\n        goal.target_pose.header.stamp = rospy.Time.now()\r\n        \r\n        goal.target_pose.pose.position.x = x\r\n        goal.target_pose.pose.position.y = y\r\n        goal.target_pose.pose.position.z = 0.0\r\n        \r\n        # Convert theta to quaternion\r\n        from tf.transformations import quaternion_from_euler\r\n        q = quaternion_from_euler(0, 0, theta)\r\n        goal.target_pose.pose.orientation.x = q[0]\r\n        goal.target_pose.pose.orientation.y = q[1]\r\n        goal.target_pose.pose.orientation.z = q[2]\r\n        goal.target_pose.pose.orientation.w = q[3]\r\n        \r\n        rospy.loginfo(f"Navigating to x={x}, y={y}, theta={theta}")\r\n        \r\n        # Send goal\r\n        self.move_base.send_goal(goal)\r\n        \r\n        # Wait for result\r\n        finished_within_time = self.move_base.wait_for_result(rospy.Duration(120))  # 2 minutes timeout\r\n        \r\n        if not finished_within_time:\r\n            self.move_base.cancel_goal()\r\n            rospy.logerr("Timed out achieving goal")\r\n            return False\r\n        \r\n        state = self.move_base.get_state()\r\n        result = self.move_base.get_result()\r\n        \r\n        if state == 3:  # SUCCEEDED\r\n            rospy.loginfo("Goal reached successfully")\r\n            return True\r\n        else:\r\n            rospy.logerr(f"Navigation failed with state: {state}")\r\n            return False\r\n    \r\n    def execute_navigation_command(self, command: str) > bool:\r\n        """Execute a natural language navigation command"""\r\n        rospy.loginfo(f"Parsing navigation command: \'{command}\'")\r\n        \r\n        # Parse the command\r\n        parsed_command = self.parse_navigation_command(command)\r\n        rospy.loginfo(f"Parsed command: {parsed_command}")\r\n        \r\n        # Determine target\r\n        target_coords = None\r\n        \r\n        if parsed_command["target_location"] != "unknown":\r\n            # Try to get coordinates for known location\r\n            target_coords = self.get_location_coordinates(parsed_command["target_location"])\r\n            \r\n            if target_coords is None:\r\n                rospy.logwarn(f"Unknown location: {parsed_command[\'target_location\']}")\r\n                # Try to learn the location based on landmarks\r\n                target_coords = self.learn_new_location(parsed_command["landmarks"])\r\n        \r\n        elif parsed_command["relative_direction"] != "none":\r\n            # Calculate relative movement\r\n            current_pos = self.llm_interface.perceptual_data.robot_pose\r\n            distance = parsed_command.get("distance", 1.0)\r\n            \r\n            if parsed_command["relative_direction"] == "forward":\r\n                new_x = current_pos[\'x\'] + distance * np.cos(current_pos[\'theta\'])\r\n                new_y = current_pos[\'y\'] + distance * np.sin(current_pos[\'theta\'])\r\n                target_coords = {"x": new_x, "y": new_y, "theta": current_pos[\'theta\']}\r\n            elif parsed_command["relative_direction"] == "backward":\r\n                new_x = current_pos[\'x\']  distance * np.cos(current_pos[\'theta\'])\r\n                new_y = current_pos[\'y\']  distance * np.sin(current_pos[\'theta\'])\r\n                target_coords = {"x": new_x, "y": new_y, "theta": current_pos[\'theta\']}\r\n            elif parsed_command["relative_direction"] == "left":\r\n                new_theta = current_pos[\'theta\'] + np.pi/2\r\n                new_x = current_pos[\'x\'] + distance * np.cos(new_theta)\r\n                new_y = current_pos[\'y\'] + distance * np.sin(new_theta)\r\n                target_coords = {"x": new_x, "y": new_y, "theta": new_theta}\r\n            elif parsed_command["relative_direction"] == "right":\r\n                new_theta = current_pos[\'theta\']  np.pi/2\r\n                new_x = current_pos[\'x\'] + distance * np.cos(new_theta)\r\n                new_y = current_pos[\'y\'] + distance * np.sin(new_theta)\r\n                target_coords = {"x": new_x, "y": new_y, "theta": new_theta}\r\n        \r\n        if target_coords:\r\n            rospy.loginfo(f"Navigating to: {target_coords}")\r\n            return self.navigate_to_waypoint(\r\n                target_coords["x"], \r\n                target_coords["y"], \r\n                target_coords["theta"]\r\n            )\r\n        else:\r\n            rospy.logerr("Could not determine navigation target")\r\n            return False\r\n    \r\n    def learn_new_location(self, landmarks: List[str]) > Optional[Dict]:\r\n        """Attempt to learn a new location based on landmarks"""\r\n        # In a real system, this would use visual SLAM to identify the location\r\n        # For now, just return current position as a placeholder\r\n        current_pos = self.llm_interface.perceptual_data.robot_pose\r\n        rospy.loginfo(f"Learning new location near: {landmarks}")\r\n        return current_pos\r\n\r\n# Example usage function\r\ndef example_navigation():\r\n    """Example of using the languagegrounded navigator"""\r\n    # Initialize components (API keys would need to be provided)\r\n    # llm_interface = LLMRobotInterface(api_key="youropenaiapikey")\r\n    # vision_processor = VisionLanguageProcessor(openai_api_key="youropenaiapikey")\r\n    # navigator = LanguageGroundedNavigator(llm_interface, vision_processor)\r\n    \r\n    # Example commands that could be processed\r\n    commands = [\r\n        "Go to the kitchen",\r\n        "Move forward 2 meters",\r\n        "Navigate to the living room",\r\n        "Go near the red chair"\r\n    ]\r\n    \r\n    for cmd in commands:\r\n        print(f"Command: {cmd}")\r\n        # result = navigator.execute_navigation_command(cmd)\r\n        print("Navigation command processed (in simulation)\\n")\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,t.jsx)(e.p,{children:"Here's a complete example that brings together all the components:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_llm_robot_integration.py\r\n\r\nimport rospy\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nimport cv2\r\nimport numpy as np\r\nimport openai\r\nimport json\r\nfrom cv_bridge import CvBridge\r\nfrom typing import Dict, List\r\n\r\nclass CompleteLLMRobotSystem:\r\n    """Complete system integrating LLMs with robotics for natural interaction"""\r\n    \r\n    def __init__(self, openai_api_key: str):\r\n        # Initialize OpenAI\r\n        openai.api_key = openai_api_key\r\n        \r\n        # Initialize ROS\r\n        rospy.init_node(\'complete_llm_robot_system\', anonymous=True)\r\n        \r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Robot state\r\n        self.robot_pose = {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0}\r\n        self.laser_scan = []\r\n        self.camera_image = None\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        rospy.Subscriber(\'/camera/image_raw\', Image, self.image_callback)\r\n        \r\n        # Robot parameters\r\n        self.linear_speed = 0.3\r\n        self.angular_speed = 0.3\r\n        \r\n        print("Complete LLM Robot System initialized")\r\n    \r\n    def laser_callback(self, msg):\r\n        """Update laser scan data"""\r\n        self.laser_scan = [r for r in msg.ranges if not (np.isinf(r) or np.isnan(r))]\r\n    \r\n    def image_callback(self, msg):\r\n        """Update camera image data"""\r\n        try:\r\n            self.camera_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n        except Exception as e:\r\n            print(f"Error converting image: {e}")\r\n    \r\n    def analyze_visual_scene(self) > str:\r\n        """Analyze the current visual scene"""\r\n        if self.camera_image is None:\r\n            return "No camera image available"\r\n        \r\n        # Convert image for analysis (simplified  in real implementation would use VLM)\r\n        height, width = self.camera_image.shape[:2]\r\n        \r\n        # Simple analysis based on image data\r\n        analysis = []\r\n        \r\n        # Analyze color distribution (simplified)\r\n        color_distances = np.mean(self.camera_image, axis=(0, 1))\r\n        dominant_color = "unknown"\r\n        if color_distances[2] > color_distances[1] and color_distances[2] > color_distances[0]:\r\n            dominant_color = "red"\r\n        elif color_distances[1] > color_distances[0] and color_distances[1] > color_distances[2]:\r\n            dominant_color = "green"\r\n        elif color_distances[0] > color_distances[1] and color_distances[0] > color_distances[2]:\r\n            dominant_color = "blue"\r\n        \r\n        # Analyze spatial distribution\r\n        center_region = self.camera_image[height//3:2*height//3, width//3:2*width//3]\r\n        center_brightness = np.mean(center_region)\r\n        \r\n        # Analyze obstacles from laser\r\n        if self.laser_scan:\r\n            min_distance = min(self.laser_scan) if self.laser_scan else float(\'inf\')\r\n        else:\r\n            min_distance = float(\'inf\')\r\n        \r\n        analysis.append(f"Observed scene with dominant color {dominant_color}")\r\n        analysis.append(f"Center brightness: {center_brightness:.2f}")\r\n        if min_distance < float(\'inf\'):\r\n            analysis.append(f"Closest obstacle: {min_distance:.2f}m ahead")\r\n        \r\n        return " ".join(analysis)\r\n    \r\n    def interpret_command_with_llm(self, user_command: str) > Dict:\r\n        """Use LLM to interpret command in context of current state"""\r\n        current_state = {\r\n            "pose": self.robot_pose,\r\n            "visual_analysis": self.analyze_visual_scene(),\r\n            "obstacle_distance": min(self.laser_scan) if self.laser_scan else "unknown",\r\n        }\r\n        \r\n        prompt = f"""\r\n        You are a robot command interpreter. Given the user\'s request and the current robot state,\r\n        determine the most appropriate robot action.\r\n        \r\n        Current state:\r\n         Position: x={current_state[\'pose\'][\'x\']:.2f}, y={current_state[\'pose\'][\'y\']:.2f}\r\n         Heading: {current_state[\'pose\'][\'theta\']:.2f} radians\r\n         Visual: {current_state[\'visual_analysis\']}\r\n         Nearest obstacle: {current_state[\'obstacle_distance\']:.2f}m\r\n        \r\n        User command: "{user_command}"\r\n        \r\n        Respond with ONLY a JSON object with this exact structure:\r\n        {{\r\n          "action": "move_forward|move_backward|turn_left|turn_right|stop|approach_object|avoid_obstacle|describe_scene",\r\n          "parameters": {{\r\n            "linear_speed": number,  # 1.0 to 1.0\r\n            "angular_speed": number, # 1.0 to 1.0\r\n            "duration": number       # seconds\r\n          }},\r\n          "explanation": "Brief explanation of why this action was chosen",\r\n          "confidence": number       # 0.0 to 1.0\r\n        }}\r\n        """\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt3.5turbo",\r\n                messages=[{"role": "user", "content": prompt}],\r\n                temperature=0.1,\r\n                max_tokens=300\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Clean up markdown if present\r\n            if response_text.startswith(\'```\'):\r\n                response_text = response_text.split(\'\\n\', 1)[1]\r\n                response_text = response_text.rstrip(\'`\')\r\n            \r\n            # Parse JSON\r\n            parsed_response = json.loads(response_text)\r\n            return parsed_response\r\n            \r\n        except Exception as e:\r\n            print(f"Error calling LLM: {e}")\r\n            return {\r\n                "action": "stop",\r\n                "parameters": {"linear_speed": 0.0, "angular_speed": 0.0, "duration": 1.0},\r\n                "explanation": "Error in processing command",\r\n                "confidence": 0.1\r\n            }\r\n    \r\n    def execute_action(self, action_plan: Dict) > bool:\r\n        """Execute the planned action"""\r\n        cmd = Twist()\r\n        \r\n        if action_plan["confidence"] < 0.3:\r\n            rospy.logwarn(f"Action confidence too low ({action_plan[\'confidence\']}), not executing")\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n            self.cmd_vel_pub.publish(cmd)\r\n            return False\r\n        \r\n        action = action_plan["action"]\r\n        params = action_plan["parameters"]\r\n        \r\n        rospy.loginfo(f"Executing action: {action} with params: {params}")\r\n        \r\n        if action == "move_forward":\r\n            cmd.linear.x = self.linear_speed * params.get("linear_speed", 1.0)\r\n            cmd.angular.z = 0.0\r\n        elif action == "move_backward":\r\n            cmd.linear.x = self.linear_speed * params.get("linear_speed", 1.0)\r\n            cmd.angular.z = 0.0\r\n        elif action == "turn_left":\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = self.angular_speed * abs(params.get("angular_speed", 1.0))\r\n        elif action == "turn_right":\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = self.angular_speed * abs(params.get("angular_speed", 1.0))\r\n        elif action == "stop":\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n        elif action == "approach_object":\r\n            # Move forward cautiously\r\n            if self.laser_scan:\r\n                min_dist = min(self.laser_scan) if self.laser_scan else float(\'inf\')\r\n                if min_dist > 0.5:  # Safe distance\r\n                    cmd.linear.x = self.linear_speed * 0.5\r\n                else:\r\n                    cmd.linear.x = 0.0\r\n                    rospy.logwarn("Too close to obstacle, stopping approach")\r\n            else:\r\n                cmd.linear.x = 0.0\r\n        elif action == "avoid_obstacle":\r\n            # Turn away from nearest obstacle\r\n            if self.laser_scan:\r\n                # Find direction of nearest obstacle (simplified)\r\n                front_range = self.laser_scan[len(self.laser_scan)//2  30 : len(self.laser_scan)//2 + 30]\r\n                if front_range:\r\n                    min_idx = front_range.index(min(front_range))\r\n                    if min_idx < len(front_range) // 2:\r\n                        cmd.angular.z = self.angular_speed  # Turn right\r\n                    else:\r\n                        cmd.angular.z = self.angular_speed  # Turn left\r\n            cmd.linear.x = 0.0  # Don\'t move forward while avoiding\r\n        elif action == "describe_scene":\r\n            # Just analyze and log, don\'t move\r\n            rospy.loginfo(f"Scene description: {self.analyze_visual_scene()}")\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n        else:\r\n            rospy.logwarn(f"Unknown action: {action}")\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n            return False\r\n        \r\n        # Publish command\r\n        self.cmd_vel_pub.publish(cmd)\r\n        \r\n        # For timed actions, publish for duration then stop\r\n        if "duration" in params and params["duration"] > 0:\r\n            rospy.sleep(params["duration"])\r\n            # Send stop command after duration\r\n            stop_cmd = Twist()\r\n            stop_cmd.linear.x = 0.0\r\n            stop_cmd.angular.z = 0.0\r\n            self.cmd_vel_pub.publish(stop_cmd)\r\n        \r\n        return True\r\n    \r\n    def process_command(self, user_command: str) > bool:\r\n        """Process a user command endtoend: interpret and execute"""\r\n        rospy.loginfo(f"Processing command: \'{user_command}\'")\r\n        \r\n        # Interpret command with LLM\r\n        action_plan = self.interpret_command_with_llm(user_command)\r\n        rospy.loginfo(f"LLM action plan: {action_plan}")\r\n        \r\n        # Execute action\r\n        success = self.execute_action(action_plan)\r\n        \r\n        if success:\r\n            rospy.loginfo(f"Command executed successfully: {action_plan[\'explanation\']}")\r\n        else:\r\n            rospy.logerr("Command execution failed")\r\n        \r\n        return success\r\n    \r\n    def run_demo(self):\r\n        """Run a demonstration of the system"""\r\n        rospy.loginfo("Starting LLMRobot demo")\r\n        \r\n        demo_commands = [\r\n            "Tell me what you see",\r\n            "Move forward slowly",\r\n            "Turn left",\r\n            "Stop",\r\n            "Move toward the clear path",\r\n            "Go forward 1 meter",\r\n            "Turn right and move forward"\r\n        ]\r\n        \r\n        for i, command in enumerate(demo_commands):\r\n            rospy.loginfo(f"\\nDemo Step {i+1}: {command}")\r\n            \r\n            if not rospy.is_shutdown():\r\n                success = self.process_command(command)\r\n                \r\n                if not success:\r\n                    rospy.logerr(f"Failed to execute command: {command}")\r\n                    break\r\n                \r\n                # Wait between commands\r\n                rospy.sleep(2.0)\r\n            else:\r\n                break\r\n        \r\n        # Final stop\r\n        stop_cmd = Twist()\r\n        stop_cmd.linear.x = 0.0\r\n        stop_cmd.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(stop_cmd)\r\n        \r\n        rospy.loginfo("Demo completed")\r\n\r\n\r\ndef main():\r\n    """Main function to run the complete system"""\r\n    # Initialize node\r\n    rospy.init_node(\'llm_robot_main\', anonymous=True)\r\n    \r\n    # Get API key from parameter server or environment variable\r\n    api_key = rospy.get_param(\'~openai_api_key\', \'\')\r\n    if not api_key:\r\n        api_key = input("Enter OpenAI API key: ")\r\n    \r\n    if not api_key:\r\n        rospy.logerr("No OpenAI API key provided!")\r\n        return\r\n    \r\n    # Create system\r\n    system = CompleteLLMRobotSystem(api_key)\r\n    \r\n    rospy.loginfo("LLMRobot system ready for commands")\r\n    \r\n    try:\r\n        # Run demo or wait for commands\r\n        system.run_demo()\r\n        \r\n        # Or alternatively, wait for interactive commands\r\n        # rospy.spin()\r\n        \r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down...")\r\n        \r\n        # Ensure robot stops\r\n        stop_cmd = Twist()\r\n        stop_cmd.linear.x = 0.0\r\n        stop_cmd.angular.z = 0.0\r\n        system.cmd_vel_pub.publish(stop_cmd)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,t.jsx)(e.p,{children:"Create a complete languageenabled robot system that:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Integrates a Large Language Model with a mobile robot"}),"\n",(0,t.jsx)(e.li,{children:"Implements visionlanguage processing for scene understanding"}),"\n",(0,t.jsx)(e.li,{children:"Enables navigation based on natural language commands"}),"\n",(0,t.jsx)(e.li,{children:"Demonstrates object recognition and manipulation through language"}),"\n",(0,t.jsx)(e.li,{children:"Handles ambiguous commands through clarification dialogues"}),"\n",(0,t.jsx)(e.li,{children:"Implements safety checks and validation for LLMgenerated commands"}),"\n",(0,t.jsx)(e.li,{children:"Evaluates the system's performance with various natural language inputs"}),"\n",(0,t.jsx)(e.li,{children:"Documents the system's capabilities and limitations"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Your project should include:\r\nComplete LLM integration with robot control\r\nVisionlanguage processing pipeline\r\nNatural language command understanding\r\nNavigation system responding to language\r\nSafety and validation mechanisms\r\nPerformance evaluation\r\nDemo scenarios with various language inputs"}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter covered the integration of Large Language Models with robotics:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"LLM Integration"}),": Connecting language models to robotic systems\r\n",(0,t.jsx)(e.strong,{children:"VisionLanguage Processing"}),": Combining visual perception with language understanding\r\n",(0,t.jsx)(e.strong,{children:"Natural Language Control"}),": Interpreting human commands for robot action\r\n",(0,t.jsx)(e.strong,{children:"Embodied AI"}),": Grounding language in physical robotic behavior\r\n",(0,t.jsx)(e.strong,{children:"Safety Considerations"}),": Ensuring safe execution of LLMgenerated commands"]}),"\n",(0,t.jsx)(e.p,{children:"The combination of LLMs with robotics enables natural humanrobot interaction, allowing users to command robots using everyday language rather than specialized programming. However, careful attention must be paid to grounding language in the physical world and ensuring safe, reliable robot behavior."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);