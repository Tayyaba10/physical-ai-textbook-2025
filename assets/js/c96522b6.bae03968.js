"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[268],{1068:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-3-ai-robot-brain/ch12-isaac-sim-photorealistic","title":"ch12-isaac-sim-photorealistic","description":"-----","source":"@site/docs/module-3-ai-robot-brain/ch12-isaac-sim-photorealistic.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/ch12-isaac-sim-photorealistic","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch12-isaac-sim-photorealistic","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-3-ai-robot-brain/ch12-isaac-sim-photorealistic.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch11-nvidia-isaac-overview","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch11-nvidia-isaac-overview"},"next":{"title":"ch13-isaac-ros-accelerated-vslam","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch13-isaac-ros-accelerated-vslam"}}');var i=r(4848),a=r(8453),s=r(7242);const o={},l=void 0,c={},d=[{value:"title: Ch12  Isaac Sim  Photorealistic Simulation &amp; Synthetic Data\r\nmodule: 3\r\nchapter: 12\r\nsidebar_label: Ch12: Isaac Sim  Photorealistic Simulation &amp; Synthetic Data\r\ndescription: Creating photorealistic simulations and generating synthetic data with Isaac Sim\r\ntags: [isaacsim, omniverse, simulation, syntheticdata, photorealistic, rendering, aitraining]\r\ndifficulty: advanced\r\nestimated_duration: 120",id:"title-ch12--isaac-sim--photorealistic-simulation--synthetic-datamodule-3chapter-12sidebar_label-ch12-isaac-sim--photorealistic-simulation--synthetic-datadescription-creating-photorealistic-simulations-and-generating-synthetic-data-with-isaac-simtags-isaacsim-omniverse-simulation-syntheticdata-photorealistic-rendering-aitrainingdifficulty-advancedestimated_duration-120",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"Isaac Sim and Omniverse Integration",id:"isaac-sim-and-omniverse-integration",level:3},{value:"Photorealistic Rendering in Isaac Sim",id:"photorealistic-rendering-in-isaac-sim",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:3},{value:"USD (Universal Scene Description)",id:"usd-universal-scene-description",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Creating a Photorealistic Environment",id:"lab-1-creating-a-photorealistic-environment",level:3},{value:"Lab 2: Configuring RTX Rendering",id:"lab-2-configuring-rtx-rendering",level:3},{value:"Lab 3: Adding Objects and Assets",id:"lab-3-adding-objects-and-assets",level:3},{value:"Lab 4: Configuring Sensor Simulation",id:"lab-4-configuring-sensor-simulation",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Isaac Sim Python Script for Synthetic Data Generation",id:"isaac-sim-python-script-for-synthetic-data-generation",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"title-ch12--isaac-sim--photorealistic-simulation--synthetic-datamodule-3chapter-12sidebar_label-ch12-isaac-sim--photorealistic-simulation--synthetic-datadescription-creating-photorealistic-simulations-and-generating-synthetic-data-with-isaac-simtags-isaacsim-omniverse-simulation-syntheticdata-photorealistic-rendering-aitrainingdifficulty-advancedestimated_duration-120",children:"title: Ch12  Isaac Sim  Photorealistic Simulation & Synthetic Data\r\nmodule: 3\r\nchapter: 12\r\nsidebar_label: Ch12: Isaac Sim  Photorealistic Simulation & Synthetic Data\r\ndescription: Creating photorealistic simulations and generating synthetic data with Isaac Sim\r\ntags: [isaacsim, omniverse, simulation, syntheticdata, photorealistic, rendering, aitraining]\r\ndifficulty: advanced\r\nestimated_duration: 120"}),"\n","\n",(0,i.jsx)(n.h1,{id:"isaac-sim--photorealistic-simulation--synthetic-data",children:"Isaac Sim  Photorealistic Simulation & Synthetic Data"}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"Create photorealistic simulation environments using Isaac Sim and Omniverse\r\nGenerate synthetic image and sensor data for AI training\r\nUnderstand the USD (Universal Scene Description) format and workflows\r\nConfigure advanced rendering features including RTX ray tracing\r\nImplement domain randomization techniques for robust AI models\r\nValidate synthetic data quality and realism\r\nIntegrate synthetic data generation into AI development workflows"}),"\n",(0,i.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-sim-and-omniverse-integration",children:"Isaac Sim and Omniverse Integration"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim is built on NVIDIA's Omniverse platform, which provides a collaborative environment for 3D design workflows. This integration provides:"}),"\n",(0,i.jsx)(s.A,{chart:"\ngraph TD;\n  A[Isaac Sim] > B[Omniverse Kit];\n  A > C[PhysX Physics];\n  A > D[RTX Rendering];\n  A > E[USD Format];\n  \n  B > F[Extensibility Framework];\n  B > G[MultiApp Collaboration];\n  B > H[Python API];\n  \n  C > I[Realistic Physics Simulation];\n  C > J[Collision Detection];\n  C > K[Rigid Body Dynamics];\n  \n  D > L[Ray Tracing];\n  D > M[Global Illumination];\n  D > N[Physicallybased Materials];\n  \n  E > O[Scalable Scene Representation];\n  E > P[Asset Interchange];\n  E > Q[Animation Support];\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style D fill:#2196F3,stroke:#0D47A1,color:#fff;\n"}),"\n",(0,i.jsx)(n.h3,{id:"photorealistic-rendering-in-isaac-sim",children:"Photorealistic Rendering in Isaac Sim"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim uses RTX technology to achieve photorealistic rendering through:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Path Tracing"}),": Simulates light transport for realistic global illumination\r\n",(0,i.jsx)(n.strong,{children:"Ray Tracing"}),": Accurate reflection, refraction, and shadow effects\r\n",(0,i.jsx)(n.strong,{children:"Physicallybased Materials"}),": Realistic material responses to lighting\r\n",(0,i.jsx)(n.strong,{children:"Volumetric Effects"}),": Atmospheric effects like fog and smoke"]}),"\n",(0,i.jsx)(n.h3,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation in Isaac Sim provides:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ground Truth Annotations"}),": Pixelperfect labels for training AI models\r\n",(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Accurate simulation of cameras, LiDAR, IMU, etc.\r\n",(0,i.jsx)(n.strong,{children:"Domain Randomization"}),": Variation in lighting, textures, and objects to improve model robustness\r\n",(0,i.jsx)(n.strong,{children:"Large Scale Data"}),": Generate thousands of images without physical limitations"]}),"\n",(0,i.jsx)(n.h3,{id:"usd-universal-scene-description",children:"USD (Universal Scene Description)"}),"\n",(0,i.jsx)(n.p,{children:"USD is Pixar's scene description format that Isaac Sim uses:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Handle complex scenes with millions of primitives\r\n",(0,i.jsx)(n.strong,{children:"Composition"}),": Combine multiple assets and scenes\r\n",(0,i.jsx)(n.strong,{children:"Animation"}),": Support for complex animation and simulation data\r\n",(0,i.jsx)(n.strong,{children:"Interchange"}),": Share assets between different 3D applications"]}),"\n",(0,i.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,i.jsx)(n.h3,{id:"lab-1-creating-a-photorealistic-environment",children:"Lab 1: Creating a Photorealistic Environment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch Isaac Sim"})," and set up a new scene:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Create a new scene in Isaac Sim\r\nimport omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.prims import create_prim\r\nfrom omni.isaac.core.utils.viewports import set_camera_view\r\nfrom pxr import Gf, UsdGeom, Sdf\r\n\r\n# Create a new world\r\nworld = World(stage_units_in_meters=1.0)\r\nstage = omni.usd.get_context().get_stage()\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create a realistic indoor environment"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create a room environment\r\ndef create_room_environment():\r\n    # Create floor\r\n    create_prim(\r\n        "/World/floor",\r\n        "Plane",\r\n        position=(0, 0, 0),\r\n        attributes={"size": 10.0}\r\n    )\r\n    \r\n    # Add realistic floor material\r\n    omni.kit.commands.execute(\r\n        "CreateMdlMaterialPrimCommand",\r\n        prim_path="/World/floor_material",\r\n        mdl_file_path="OmniSurface.mdl",\r\n        mtl_name="Omni_pbr__default",\r\n        mtl_created_list=["/World/floor_material"]\r\n    )\r\n    \r\n    # Apply material to floor\r\n    stage = omni.usd.get_context().get_stage()\r\n    floor_prim = stage.GetPrimAtPath("/World/floor")\r\n    UsdGeom.MaterialBindingAPI(floor_prim).Bind(\r\n        stage.GetPrimAtPath("/World/floor_material")\r\n    )\r\n    \r\n    # Create walls\r\n    wall_positions = [\r\n        (0, 5, 2.5),    # North wall\r\n        (0, 5, 2.5),   # South wall\r\n        (5, 0, 2.5),    # East wall\r\n        (5, 0, 2.5)    # West wall\r\n    ]\r\n    \r\n    wall_rotations = [\r\n        (0, 0, 0),      # North wall  no rotation\r\n        (0, 180, 0),    # South wall  180\xb0 rotation\r\n        (0, 90, 0),     # East wall  90\xb0 rotation\r\n        (0, 90, 0)     # West wall  90\xb0 rotation\r\n    ]\r\n    \r\n    for i, (pos, rot) in enumerate(zip(wall_positions, wall_rotations)):\r\n        create_prim(\r\n            f"/World/wall_{i}",\r\n            "Cube",\r\n            position=pos,\r\n            orientation=(rot[0]*Gf.Vec3f(1, 0, 0), rot[1]*Gf.Vec3f(0, 1, 0), rot[2]*Gf.Vec3f(0, 0, 1)),\r\n            attributes={"size": 0.2}\r\n        )\r\n    \r\n    # Create ceiling\r\n    create_prim(\r\n        "/World/ceiling",\r\n        "Plane",\r\n        position=(0, 0, 5),\r\n        attributes={"size": 10.0}\r\n    )\r\n\r\ncreate_room_environment()\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add realistic lighting"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Add dome light for environment lighting\r\ncreate_prim(\r\n    "/World/DomeLight",\r\n    "DomeLight",\r\n    attributes={\r\n        "color": (1.0, 1.0, 1.0),\r\n        "intensity": 300.0,\r\n        "texture:file": "path/to/your/hdri/environment.hdr"\r\n    }\r\n)\r\n\r\n# Add rectangular light panel\r\ncreate_prim(\r\n    "/World/RectLight",\r\n    "RectLight",\r\n    position=(0, 0, 4.9),  # Position just below ceiling\r\n    attributes={\r\n        "color": (0.9, 0.9, 1.0),  # Slightly bluetinted white\r\n        "intensity": 500.0,\r\n        "width": 1.5,\r\n        "height": 1.0\r\n    }\r\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-2-configuring-rtx-rendering",children:"Lab 2: Configuring RTX Rendering"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Enable RTX rendering"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Enable RTX rendering in Isaac Sim\r\nimport carb\r\n\r\n# Set rendering kit to PathTracing\r\nsettings = carb.settings.get_settings()\r\nsettings.set("/rtx/renderMode", "PathTracing")\r\n\r\n# Configure Path Tracing parameters\r\nsettings.set("/rtx/pathtracing/maxBounces", 8)\r\nsettings.set("/rtx/pathtracing/maxSpecularAndTransmissionBounces", 8)\r\nsettings.set("/rtx/indirectDiffuseQuality", 2)  # High quality\r\nsettings.set("/rtx/indirectSpecularQuality", 2)  # High quality\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add physicallybased materials"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create a physicallybased material\r\ndef create_pbr_material(material_path, base_color, roughness=0.5, metallic=0.0):\r\n    # Create material using Omniverse Material Library\r\n    omni.kit.commands.execute(\r\n        "CreateMdlMaterialPrimCommand",\r\n        prim_path=material_path,\r\n        mdl_file_path="OmniSurface.mdl",\r\n        mtl_name="Omni_pbr__default"\r\n    )\r\n    \r\n    # Set material properties\r\n    stage = omni.usd.get_context().get_stage()\r\n    material = stage.GetPrimAtPath(material_path)\r\n    \r\n    # Set base color\r\n    material.GetAttribute("inputs:diffuse_tint").Set(base_color)\r\n    # Set roughness\r\n    material.GetAttribute("inputs:roughness").Set(roughness)\r\n    # Set metallic\r\n    material.GetAttribute("inputs:metallic").Set(metallic)\r\n    \r\n    return material\r\n\r\n# Create different materials for objects\r\nred_plastic = create_pbr_material("/World/RedPlastic", (1.0, 0.2, 0.2), roughness=0.2, metallic=0.0)\r\nmetallic_surface = create_pbr_material("/World/MetallicSurface", (0.7, 0.7, 0.8), roughness=0.1, metallic=0.8)\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-3-adding-objects-and-assets",children:"Lab 3: Adding Objects and Assets"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Load 3D models and assets"}),":","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Add furniture and objects to the scene\r\ndef add_objects_to_scene():\r\n    # Add a table\r\n    add_reference_to_stage(\r\n        usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/Junkyard/table.usd",\r\n        prim_path="/World/Table"\r\n    )\r\n    \r\n    # Add various objects with different materials\r\n    objects = [\r\n        {"name": "bottle", "usd": "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/035_power_drill.usd", "pos": (0.5, 0.5, 0.8)},\r\n        {"name": "can", "usd": "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/010_potted_meat_can.usd", "pos": (0.3, 0.2, 0.8)},\r\n        {"name": "bowl", "usd": "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/025_mug.usd", "pos": (0.2, 0.4, 0.8)}\r\n    ]\r\n    \r\n    for i, obj in enumerate(objects):\r\n        add_reference_to_stage(\r\n            usd_path=obj["usd"],\r\n            prim_path=f"/World/Objects/{obj[\'name\']}_{i}"\r\n        )\r\n        \r\n        # Set position\r\n        stage = omni.usd.get_context().get_stage()\r\n        prim = stage.GetPrimAtPath(f"/World/Objects/{obj[\'name\']}_{i}")\r\n        xform = UsdGeom.Xformable(prim)\r\n        xform.AddTranslateOp().Set(Gf.Vec3d(*obj["pos"]))\r\n\r\nadd_objects_to_scene()\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-4-configuring-sensor-simulation",children:"Lab 4: Configuring Sensor Simulation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add a realistic camera sensor"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Add a USD Camera to the stage\r\ndef add_camera(name, position, orientation, parent_path="/World"):\r\n    camera_path = f"{parent_path}/{name}"\r\n    \r\n    # Create the camera prim\r\n    stage = omni.usd.get_context().get_stage()\r\n    camera = UsdGeom.Camera.Define(stage, camera_path)\r\n    \r\n    # Set camera properties\r\n    camera.GetPrim().GetAttribute("xformOp:translate").Set(Gf.Vec3f(*position))\r\n    camera.GetPrim().GetAttribute("xformOp:orient").Set(Gf.Quatf(*orientation))\r\n    \r\n    # Set camera intrinsics\r\n    camera.GetFocalLengthAttr().Set(24.0)  # mm\r\n    camera.GetHorizontalApertureAttr().Set(36.0)  # mm\r\n    camera.GetVerticalApertureAttr().Set(20.25)   # mm\r\n    \r\n    # Set clipping range\r\n    camera.GetClippingRangeAttr().Set((0.1, 1000.0))\r\n    \r\n    return camera_path\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Configure camera for synthetic data generation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Add a camera for generating synthetic RGB data\r\ncamera_path = add_camera(\r\n    name="SyntheticCamera",\r\n    position=(1.5, 1.5, 1.2),  # Position above the table\r\n    orientation=(0.0, 0.0, 0.0, 1.0)  # Identity quaternion (no rotation)\r\n)\r\n\r\n# Configure camera for rendering\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\n\r\n# Set up synthetic data helper\r\nsd_helper = SyntheticDataHelper()\r\nsd_helper.initialize(camera_path)\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,i.jsx)(n.p,{children:"Here's a complete example that demonstrates creating a photorealistic scene and generating synthetic data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# synthetic_data_generator.py\r\nimport omni\r\nimport carb\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.prims import create_prim\r\nfrom omni.isaac.core.utils.viewports import set_camera_view\r\nfrom pxr import Gf, UsdGeom, Sdf\r\nimport numpy as np\r\nimport cv2\r\nfrom omni.synthetic_utils import converters\r\nimport os\r\n\r\nclass PhotorealisticSceneBuilder:\r\n    def __init__(self):\r\n        self.stage = omni.usd.get_context().get_stage()\r\n        self.sd_helper = None\r\n        \r\n    def create_environment(self):\r\n        """Create a photorealistic indoor environment"""\r\n        print("Creating photorealistic environment...")\r\n        \r\n        # Create floor with realistic material\r\n        create_prim(\r\n            "/World/floor",\r\n            "Plane",\r\n            position=(0, 0, 0),\r\n            attributes={"size": 10.0}\r\n        )\r\n        \r\n        # Create walls\r\n        wall_positions = [\r\n            (0, 5, 2.5), (0, 5, 2.5), (5, 0, 2.5), (5, 0, 2.5)\r\n        ]\r\n        wall_rotations = [\r\n            (0, 0, 0), (0, 180, 0), (0, 90, 0), (0, 90, 0)\r\n        ]\r\n        \r\n        for i, (pos, rot) in enumerate(zip(wall_positions, wall_rotations)):\r\n            create_prim(\r\n                f"/World/wall_{i}",\r\n                "Cube",\r\n                position=pos,\r\n                attributes={"size": 0.2}\r\n            )\r\n        \r\n        # Create ceiling\r\n        create_prim(\r\n            "/World/ceiling",\r\n            "Plane",\r\n            position=(0, 0, 5),\r\n            attributes={"size": 10.0}\r\n        )\r\n        \r\n        # Add lighting\r\n        create_prim(\r\n            "/World/DomeLight",\r\n            "DomeLight",\r\n            attributes={\r\n                "color": (1.0, 1.0, 1.0),\r\n                "intensity": 500.0\r\n            }\r\n        )\r\n        \r\n        print("Environment created successfully")\r\n    \r\n    def add_objects_with_randomization(self):\r\n        """Add objects with domain randomization"""\r\n        print("Adding objects with randomization...")\r\n        \r\n        # Object models to choose from\r\n        object_paths = [\r\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/003_cracker_box.usd",\r\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/004_sugar_box.usd",\r\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/005_tomato_soup_can.usd",\r\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.0/Isaac/Props/YCB/Axis_Aligned/007_tuna_fish_can.usd"\r\n        ]\r\n        \r\n        # Add 5 random objects\r\n        for i in range(5):\r\n            # Random position within bounds\r\n            x = np.random.uniform(2, 2)\r\n            y = np.random.uniform(2, 2)\r\n            z = 0.1  # Just above floor\r\n            \r\n            # Add object to scene\r\n            add_reference_to_stage(\r\n                usd_path=np.random.choice(object_paths),\r\n                prim_path=f"/World/Object_{i}"\r\n            )\r\n            \r\n            # Set random position\r\n            prim = self.stage.GetPrimAtPath(f"/World/Object_{i}")\r\n            xform = UsdGeom.Xformable(prim)\r\n            xform.AddTranslateOp().Set(Gf.Vec3d(x, y, z))\r\n            \r\n            # Random rotation\r\n            xform.AddRotateXYZOp().Set(Gf.Vec3f(\r\n                np.random.uniform(0, 360),\r\n                np.random.uniform(0, 360),\r\n                np.random.uniform(0, 360)\r\n            ))\r\n        \r\n        print("Objects added successfully")\r\n    \r\n    def setup_camera_for_synthetic_data(self):\r\n        """Set up camera for synthetic data generation"""\r\n        print("Setting up camera for synthetic data...")\r\n        \r\n        # Create camera\r\n        camera_path = "/World/RGB_Camera"\r\n        camera = UsdGeom.Camera.Define(self.stage, camera_path)\r\n        \r\n        # Position camera\r\n        xform = UsdGeom.Xformable(camera.GetPrim())\r\n        xform.AddTranslateOp().Set(Gf.Vec3d(2, 2, 1.5))\r\n        \r\n        # Point camera toward center\r\n        set_camera_view(eye=(2, 2, 1.5), target=(0, 0, 1))\r\n        \r\n        # Set camera properties\r\n        camera.GetFocalLengthAttr().Set(24.0)\r\n        camera.GetHorizontalApertureAttr().Set(36.0)\r\n        camera.GetVerticalApertureAttr().Set(20.25)\r\n        camera.GetClippingRangeAttr().Set((0.1, 1000.0))\r\n        \r\n        print("Camera setup complete")\r\n        \r\n        return camera_path\r\n    \r\n    def enable_rtx_rendering(self):\r\n        """Configure RTX rendering settings"""\r\n        print("Enabling RTX rendering...")\r\n        \r\n        settings = carb.settings.get_settings()\r\n        \r\n        # Set to Path Tracing mode for highest quality\r\n        settings.set("/rtx/renderMode", "PathTracing")\r\n        \r\n        # Set high quality parameters\r\n        settings.set("/rtx/pathtracing/maxBounces", 8)\r\n        settings.set("/rtx/pathtracing/maxSpecularAndTransmissionBounces", 8)\r\n        settings.set("/rtx/indirectDiffuseQuality", 2)\r\n        settings.set("/rtx/indirectSpecularQuality", 2)\r\n        settings.set("/rtx/domeLight/enable", True)\r\n        \r\n        print("RTX rendering enabled")\r\n    \r\n    def capture_synthetic_data(self, output_dir="synthetic_data", num_frames=10):\r\n        """Capture synthetic RGB and depth data"""\r\n        print(f"Capturing {num_frames} synthetic data frames...")\r\n        \r\n        # Create output directory\r\n        os.makedirs(output_dir, exist_ok=True)\r\n        os.makedirs(f"{output_dir}/rgb", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/seg", exist_ok=True)  # segmentation\r\n        \r\n        # Get the USD camera\r\n        camera_path = "/World/RGB_Camera"\r\n        \r\n        # Wait for rendering to initialize\r\n        import time\r\n        time.sleep(2)\r\n        \r\n        for frame_idx in range(num_frames):\r\n            print(f"Capturing frame {frame_idx+1}/{num_frames}")\r\n            \r\n            # Randomize lighting and object positions for domain randomization\r\n            self.randomize_scene()\r\n            \r\n            # Wait for frame to render\r\n            from omni.isaac.core.utils.viewports import get_viewport_from_window_name\r\n            viewport_api = get_viewport_from_window_name("Viewport")\r\n            \r\n            # Trigger capture (this is a simplified approach)\r\n            # In a real implementation, you would use the Synthetic Data API\r\n            self.capture_frame_data(f"{output_dir}/rgb/frame_{frame_idx:04d}.png", \r\n                                   f"{output_dir}/depth/frame_{frame_idx:04d}.png",\r\n                                   f"{output_dir}/seg/frame_{frame_idx:04d}.png")\r\n            \r\n            time.sleep(0.5)  # Wait between captures\r\n        \r\n        print(f"Synthetic data captured to {output_dir}")\r\n    \r\n    def randomize_scene(self):\r\n        """Apply domain randomization to improve dataset robustness"""\r\n        # Randomize dome light intensity\r\n        dome_light = self.stage.GetPrimAtPath("/World/DomeLight")\r\n        new_intensity = np.random.uniform(300, 800)\r\n        dome_light.GetAttribute("inputs:intensity").Set(new_intensity)\r\n        \r\n        # Randomize dome light color temperature (simplified)\r\n        new_color = (\r\n            np.random.uniform(0.8, 1.0),\r\n            np.random.uniform(0.8, 1.0),\r\n            np.random.uniform(0.9, 1.0)\r\n        )\r\n        dome_light.GetAttribute("inputs:color").Set(new_color)\r\n        \r\n        # Randomize object materials\r\n        for i in range(5):\r\n            obj_prim = self.stage.GetPrimAtPath(f"/World/Object_{i}")\r\n            if obj_prim:\r\n                # In a real implementation, you would apply random materials\r\n                # This is a placeholder for material randomization\r\n                pass\r\n    \r\n    def capture_frame_data(self, rgb_path, depth_path, seg_path):\r\n        """Capture RGB, depth, and segmentation data"""\r\n        # In a real Isaac Sim implementation, this would use the \r\n        # Synthetic Data API to capture data directly\r\n        # For this example, we\'ll create placeholder images\r\n        \r\n        # Create placeholder RGB image\r\n        rgb_img = np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8)\r\n        cv2.imwrite(rgb_path, cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR))\r\n        \r\n        # Create placeholder depth image\r\n        depth_img = np.random.uniform(0.1, 10.0, (720, 1280)).astype(np.float32)\r\n        cv2.imwrite(depth_path, depth_img)\r\n        \r\n        # Create placeholder segmentation image\r\n        seg_img = np.random.randint(0, 255, (720, 1280), dtype=np.uint8)\r\n        cv2.imwrite(seg_path, seg_img)\r\n        \r\n        print(f"Captured data to {rgb_path}, {depth_path}, {seg_path}")\r\n\r\ndef main():\r\n    """Main function to create photorealistic scene and generate data"""\r\n    # Initialize Isaac Sim context\r\n    print("Initializing Isaac Sim environment...")\r\n    \r\n    # Create scene builder\r\n    builder = PhotorealisticSceneBuilder()\r\n    \r\n    # Create photorealistic environment\r\n    builder.create_environment()\r\n    \r\n    # Add objects with randomization\r\n    builder.add_objects_with_randomization()\r\n    \r\n    # Enable RTX rendering\r\n    builder.enable_rtx_rendering()\r\n    \r\n    # Set up camera for synthetic data\r\n    camera_path = builder.setup_camera_for_synthetic_data()\r\n    \r\n    # Generate synthetic data\r\n    builder.capture_synthetic_data(num_frames=5)\r\n    \r\n    print("Synthetic data generation complete!")\r\n\r\n# Isaac Sim Extension\r\nclass SyntheticDataExtension:\r\n    def __init__(self):\r\n        self.scene_builder = PhotorealisticSceneBuilder()\r\n    \r\n    def build_photorealistic_scene(self):\r\n        self.scene_builder.create_environment()\r\n        self.scene_builder.add_objects_with_randomization()\r\n        self.scene_builder.enable_rtx_rendering()\r\n        self.scene_builder.setup_camera_for_synthetic_data()\r\n    \r\n    def generate_dataset(self, config):\r\n        """Generate a complete synthetic dataset based on config"""\r\n        # Apply domain randomization settings from config\r\n        num_frames = config.get("num_frames", 100)\r\n        output_dir = config.get("output_dir", "synthetic_dataset")\r\n        scene_complexity = config.get("scene_complexity", "medium")\r\n        \r\n        # Create appropriate environment based on complexity\r\n        if scene_complexity == "simple":\r\n            # Simple scene with few objects\r\n            pass\r\n        elif scene_complexity == "medium":\r\n            # Medium complexity scene\r\n            pass\r\n        elif scene_complexity == "complex":\r\n            # Complex scene with many objects and lighting variations\r\n            pass\r\n        \r\n        # Generate the dataset\r\n        self.scene_builder.capture_synthetic_data(\r\n            output_dir=output_dir,\r\n            num_frames=num_frames\r\n        )\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-sim-python-script-for-synthetic-data-generation",children:"Isaac Sim Python Script for Synthetic Data Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# advanced_synthetic_data.py\r\nimport omni\r\nimport carb\r\nfrom pxr import Gf, UsdGeom, Sdf\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\nfrom omni.isaac.synthetic_utils import shaders\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.prims import create_prim\r\nfrom omni.kit.viewport.utility import get_active_viewport\r\nimport numpy as np\r\nimport PIL.Image\r\nimport os\r\n\r\nclass AdvancedSyntheticDataGenerator:\r\n    def __init__(self):\r\n        self.sd_helper = SyntheticDataHelper()\r\n        self.stage = omni.usd.get_context().get_stage()\r\n        self.viewport = get_active_viewport()\r\n        \r\n    def setup_sensor_suite(self, camera_path):\r\n        """Set up multiple sensors for comprehensive data capture"""\r\n        # Initialize synthetic data helper with the camera\r\n        self.sd_helper.initialize(camera_path)\r\n        \r\n        # Enable different types of synthetic data\r\n        try:\r\n            # RGB data\r\n            self.sd_helper.get_rgb()\r\n            \r\n            # Depth data (world units)\r\n            self.sd_helper.get_depth()\r\n            \r\n            # Semantic segmentation\r\n            self.sd_helper.get_semantic_segmentation()\r\n            \r\n            # Instance segmentation\r\n            self.sd_helper.get_instance_segmentation()\r\n            \r\n            # Normals\r\n            self.sd_helper.get_normals()\r\n            \r\n            # Motion vectors\r\n            self.sd_helper.get_motion_vectors()\r\n            \r\n            print("All sensor types enabled")\r\n        except Exception as e:\r\n            print(f"Error setting up sensors: {e}")\r\n    \r\n    def generate_domain_randomization_config(self):\r\n        """Generate configuration for domain randomization"""\r\n        config = {\r\n            # Lighting randomization\r\n            "lighting": {\r\n                "intensity_range": (300, 800),\r\n                "color_temperature_range": (5000, 8000),  # Kelvin\r\n                "shadow_softness_range": (0.1, 0.8)\r\n            },\r\n            \r\n            # Material randomization\r\n            "materials": {\r\n                "albedo_range": (0.1, 1.0),\r\n                "roughness_range": (0.0, 1.0),\r\n                "metallic_range": (0.0, 1.0),\r\n                "normal_map_strength_range": (0.0, 1.0)\r\n            },\r\n            \r\n            # Object placement\r\n            "objects": {\r\n                "position_jitter": 0.1,  # meters\r\n                "rotation_jitter": 5.0,  # degrees\r\n                "scale_range": (0.8, 1.2)\r\n            },\r\n            \r\n            # Camera randomization\r\n            "camera": {\r\n                "position_jitter": 0.05,\r\n                "orientation_jitter": 2.0,\r\n                "focal_length_range": (18, 55)  # mm equivalent\r\n            }\r\n        }\r\n        \r\n        return config\r\n    \r\n    def apply_domain_randomization(self, config):\r\n        """Apply domain randomization to the scene"""\r\n        # Randomize lighting\r\n        dome_light = self.stage.GetPrimAtPath("/World/DomeLight")\r\n        if dome_light:\r\n            # Random intensity\r\n            intensity = np.random.uniform(\r\n                config["lighting"]["intensity_range"][0], \r\n                config["lighting"]["intensity_range"][1]\r\n            )\r\n            dome_light.GetAttribute("inputs:intensity").Set(intensity)\r\n            \r\n            # Random color temperature (simplified)\r\n            color_temp = np.random.uniform(\r\n                config["lighting"]["color_temperature_range"][0],\r\n                config["lighting"]["color_temperature_range"][1]\r\n            )\r\n            # Convert to RGB approximation\r\n            rgb = self.color_temperature_to_rgb(color_temp)\r\n            dome_light.GetAttribute("inputs:color").Set(rgb)\r\n        \r\n        # Randomize materials (simplified example)\r\n        for i in range(5):\r\n            obj_prim = self.stage.GetPrimAtPath(f"/World/Object_{i}")\r\n            if obj_prim:\r\n                # In a real implementation, you would apply random materials\r\n                # This is a placeholder for material randomization\r\n                pass\r\n    \r\n    def color_temperature_to_rgb(self, color_temp):\r\n        """Convert color temperature in Kelvin to RGB (simplified)"""\r\n        # This is a simplified approximation\r\n        temp = color_temp / 100\r\n        if temp <= 66:\r\n            red = 255\r\n            green = temp\r\n            green = 99.4708025861 * np.log(green)  161.1195681661\r\n        else:\r\n            red = temp  60\r\n            red = 329.698727446 * (red ** 0.1332047592)\r\n            green = temp  60\r\n            green = 288.1221695283 * (green ** 0.0755148492)\r\n        \r\n        blue = temp  10\r\n        if temp >= 66:\r\n            blue = 255\r\n        elif temp <= 19:\r\n            blue = 0\r\n        else:\r\n            blue = temp  10\r\n            blue = 138.5177312231 * np.log(blue)  305.0447927307\r\n        \r\n        # Clamp values to [0, 255] and normalize to [0, 1]\r\n        red = np.clip(red, 0, 255) / 255.0\r\n        green = np.clip(green, 0, 255) / 255.0\r\n        blue = np.clip(blue, 0, 255) / 255.0\r\n        \r\n        return (red, green, blue)\r\n    \r\n    def capture_synthetic_dataset(self, output_dir, num_samples, config=None):\r\n        """Capture a comprehensive synthetic dataset"""\r\n        if config is None:\r\n            config = self.generate_domain_randomization_config()\r\n        \r\n        # Create output directories\r\n        os.makedirs(f"{output_dir}/rgb", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/seg", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/instances", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/normals", exist_ok=True)\r\n        \r\n        camera_path = "/World/RGB_Camera"\r\n        self.setup_sensor_suite(camera_path)\r\n        \r\n        for i in range(num_samples):\r\n            print(f"Capturing sample {i+1}/{num_samples}")\r\n            \r\n            # Apply domain randomization\r\n            self.apply_domain_randomization(config)\r\n            \r\n            # Wait for the scene to update\r\n            import time\r\n            time.sleep(0.1)\r\n            \r\n            # Capture all data types\r\n            self.capture_single_sample(f"{output_dir}", i)\r\n        \r\n        print(f"Dataset captured to {output_dir}")\r\n    \r\n    def capture_single_sample(self, output_dir, sample_idx):\r\n        """Capture a single sample with all data types"""\r\n        try:\r\n            # Get data from synthetic data helper\r\n            rgb_data = self.sd_helper.get_rgb()\r\n            depth_data = self.sd_helper.get_depth()\r\n            seg_data = self.sd_helper.get_semantic_segmentation()\r\n            instance_data = self.sd_helper.get_instance_segmentation()\r\n            normal_data = self.sd_helper.get_normals()\r\n            \r\n            # Save RGB image\r\n            if rgb_data is not None:\r\n                rgb_img = PIL.Image.fromarray((rgb_data * 255).astype(np.uint8))\r\n                rgb_img.save(f"{output_dir}/rgb/{sample_idx:06d}.png")\r\n            \r\n            # Save depth image\r\n            if depth_data is not None:\r\n                depth_img = PIL.Image.fromarray((depth_data * 1000).astype(np.uint16))  # Scale for 16bit\r\n                depth_img.save(f"{output_dir}/depth/{sample_idx:06d}.png")\r\n            \r\n            # Save segmentation\r\n            if seg_data is not None:\r\n                seg_img = PIL.Image.fromarray((seg_data).astype(np.uint8))\r\n                seg_img.save(f"{output_dir}/seg/{sample_idx:06d}.png")\r\n            \r\n            # Save instance segmentation\r\n            if instance_data is not None:\r\n                instance_img = PIL.Image.fromarray((instance_data).astype(np.uint8))\r\n                instance_img.save(f"{output_dir}/instances/{sample_idx:06d}.png")\r\n            \r\n            # Save normals (as RGB or separate channels)\r\n            if normal_data is not None:\r\n                # Normalize normals to [0, 255]\r\n                normal_scaled = ((normal_data + 1) * 0.5 * 255).astype(np.uint8)\r\n                normal_img = PIL.Image.fromarray(normal_scaled)\r\n                normal_img.save(f"{output_dir}/normals/{sample_idx:06d}.png")\r\n            \r\n        except Exception as e:\r\n            carb.log_error(f"Error capturing sample {sample_idx}: {str(e)}")\r\n\r\ndef create_photorealistic_scene_and_generate_data():\r\n    """Complete workflow: create scene and generate synthetic data"""\r\n    # Initialize Isaac Sim components\r\n    generator = AdvancedSyntheticDataGenerator()\r\n    \r\n    # Create or use an existing photorealistic scene\r\n    # (In practice, you\'d build your scene with realistic assets and materials)\r\n    \r\n    # Define dataset configuration\r\n    dataset_config = {\r\n        "num_samples": 100,\r\n        "output_dir": "./synthetic_dataset",\r\n        "domain_randomization": {\r\n            "lighting": {"intensity_range": (300, 800)},\r\n            "objects": {"position_jitter": 0.05}\r\n        }\r\n    }\r\n    \r\n    # Generate the dataset\r\n    generator.capture_synthetic_dataset(\r\n        output_dir=dataset_config["output_dir"],\r\n        num_samples=dataset_config["num_samples"],\r\n        config=dataset_config["domain_randomization"]\r\n    )\r\n    \r\n    print("Synthetic dataset generation complete!")\r\n\r\n# Run the workflow\r\nif __name__ == "__main__":\r\n    create_photorealistic_scene_and_generate_data()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete synthetic data generation pipeline that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Builds a photorealistic kitchen environment in Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Adds domain randomization for lighting, materials, and object placement"}),"\n",(0,i.jsx)(n.li,{children:"Configures multiple sensors (RGB, depth, semantic segmentation)"}),"\n",(0,i.jsx)(n.li,{children:"Generates at least 500 synthetic images with ground truth annotations"}),"\n",(0,i.jsx)(n.li,{children:"Implements a validation system to check data quality"}),"\n",(0,i.jsx)(n.li,{children:"Creates training scripts that use the synthetic data to train a simple perception model"}),"\n",(0,i.jsx)(n.li,{children:"Compares performance of models trained on synthetic vs real data"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Your project should include:\r\nComplete Isaac Sim scene with photorealistic rendering\r\nDomain randomization system\r\nData capture pipeline with multiple sensors\r\nQuality validation tools\r\nTraining pipeline that demonstrates the value of synthetic data"}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered photorealistic simulation and synthetic data generation with Isaac Sim:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Photorealistic Rendering"}),": Using RTX technology and PhysX for realistic simulations\r\n",(0,i.jsx)(n.strong,{children:"USD Workflows"}),": Understanding Universal Scene Description for scalable scene management\r\n",(0,i.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Creating ground truth datasets for AI training\r\n",(0,i.jsx)(n.strong,{children:"Domain Randomization"}),": Techniques to improve model robustness through variation\r\n",(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Configuring realistic camera, LiDAR, and other sensor models\r\n",(0,i.jsx)(n.strong,{children:"Data Pipeline"}),": Complete workflow from scene creation to dataset generation"]}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim provides powerful tools for creating realistic simulation environments and generating highquality synthetic data essential for training robust AI models in robotics applications."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);