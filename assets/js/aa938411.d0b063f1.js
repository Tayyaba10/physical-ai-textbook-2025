"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[4891],{2110:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>f,frontMatter:()=>i,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"module-4-vision-language-action/ch18-voice-to-action-whisper","title":"ch18-voice-to-action-whisper","description":"-----","source":"@site/docs/module-4-vision-language-action/ch18-voice-to-action-whisper.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch18-voice-to-action-whisper","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch18-voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch18-voice-to-action-whisper.md","tags":[],"version":"current","frontMatter":{}}');var a=r(4848),s=r(8453),o=r(7242);const i={},c=void 0,l={},m=[{value:"title: Ch18  VoicetoAction with OpenAI Whisper\r\nmodule: 4\r\nchapter: 18\r\nsidebar_label: Ch18: VoicetoAction with OpenAI Whisper\r\ndescription: Implementing speech recognition and voice command processing for robotics using OpenAI Whisper\r\ntags: [whisper, speechrecognition, voicecontrol, robotics, naturallanguage, audioprocessing]\r\ndifficulty: advanced\r\nestimated_duration: 120",id:"title-ch18--voicetoaction-with-openai-whispermodule-4chapter-18sidebar_label-ch18-voicetoaction-with-openai-whisperdescription-implementing-speech-recognition-and-voice-command-processing-for-robotics-using-openai-whispertags-whisper-speechrecognition-voicecontrol-robotics-naturallanguage-audioprocessingdifficulty-advancedestimated_duration-120",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"OpenAI Whisper Architecture",id:"openai-whisper-architecture",level:3},{value:"ASR in Robotics Context",id:"asr-in-robotics-context",level:3},{value:"Audio Preprocessing Pipeline",id:"audio-preprocessing-pipeline",level:3},{value:"Voice Command Understanding",id:"voice-command-understanding",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up Whisper for Robotics",id:"lab-1-setting-up-whisper-for-robotics",level:3},{value:"Lab 2: Advanced Voice Command Processing",id:"lab-2-advanced-voice-command-processing",level:3},{value:"Lab 3: Creating a Voice Control Safety System",id:"lab-3-creating-a-voice-control-safety-system",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Launch file for the complete system:",id:"launch-file-for-the-complete-system",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"title-ch18--voicetoaction-with-openai-whispermodule-4chapter-18sidebar_label-ch18-voicetoaction-with-openai-whisperdescription-implementing-speech-recognition-and-voice-command-processing-for-robotics-using-openai-whispertags-whisper-speechrecognition-voicecontrol-robotics-naturallanguage-audioprocessingdifficulty-advancedestimated_duration-120",children:"title: Ch18  VoicetoAction with OpenAI Whisper\r\nmodule: 4\r\nchapter: 18\r\nsidebar_label: Ch18: VoicetoAction with OpenAI Whisper\r\ndescription: Implementing speech recognition and voice command processing for robotics using OpenAI Whisper\r\ntags: [whisper, speechrecognition, voicecontrol, robotics, naturallanguage, audioprocessing]\r\ndifficulty: advanced\r\nestimated_duration: 120"}),"\n","\n",(0,a.jsx)(n.h1,{id:"voicetoaction-with-openai-whisper",children:"VoicetoAction with OpenAI Whisper"}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"Understand speech recognition systems and OpenAI Whisper architecture\r\nImplement voice command processing for robotic systems\r\nProcess realtime audio for continuous robot interaction\r\nDesign voice command grammars for robotic tasks\r\nIntegrate speech recognition with robot control systems\r\nHandle voice command ambiguities and context\r\nCreate multimodal feedback systems for voice interactions\r\nImplement safety checks and validation for voice commands\r\nEvaluate speech recognition performance in robotic contexts"}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.h3,{id:"openai-whisper-architecture",children:"OpenAI Whisper Architecture"}),"\n",(0,a.jsx)(n.p,{children:"OpenAI Whisper is a robust automatic speech recognition (ASR) system that has revolutionized speech processing. Unlike traditional ASR systems, Whisper was trained on a vast dataset of audio and text from the internet, making it highly versatile and accurate across different domains."}),"\n",(0,a.jsx)(o.A,{chart:"\ngraph TD;\n  A[Audio Input] > B[Audio Preprocessing];\n  B > C[Mel Spectrogram];\n  C > D[Whisper Encoder];\n  D > E[Whisper Decoder];\n  E > F[Text Output];\n  \n  G[Robot Control] > H[Command Parser];\n  H > I[NLP Processor];\n  I > J[Action Generator];\n  J > K[Robot Execution];\n  \n  F > I;\n  K > L[Safety Validator];\n  L > M[Execution Confirmation];\n  M > K;\n  \n  N[Voice Command] > A;\n  O[Robot Action] > P[Confirmation];\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style D fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style K fill:#FF9800,stroke:#E65100,color:#fff;\n  style N fill:#E91E63,stroke:#AD1457,color:#fff;\n"}),"\n",(0,a.jsx)(n.h3,{id:"asr-in-robotics-context",children:"ASR in Robotics Context"}),"\n",(0,a.jsx)(n.p,{children:"Automatic Speech Recognition in robotics differs from traditional applications in several key ways:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Environmental Noise"}),": Robots operate in noisy environments with motor noise, fan noise, and other acoustic interference\r\n",(0,a.jsx)(n.strong,{children:"Realtime Requirements"}),": Robot systems often require immediate responses to user commands\r\n",(0,a.jsx)(n.strong,{children:"Limited Vocabulary"}),": Robot commands typically come from a predefined set of actions\r\n",(0,a.jsx)(n.strong,{children:"Context Dependency"}),": Commands often depend on robot state and environment\r\n",(0,a.jsx)(n.strong,{children:"Safety Considerations"}),": Voice commands might lead to physical actions that need validation"]}),"\n",(0,a.jsx)(n.h3,{id:"audio-preprocessing-pipeline",children:"Audio Preprocessing Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"For robotic applications, audio preprocessing becomes critical due to the challenging acoustic environment:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Noise Reduction"}),": Filtering environmental noise to improve speech clarity\r\n",(0,a.jsx)(n.strong,{children:"Voice Activity Detection"}),": Identifying when speech is actually occurring\r\n",(0,a.jsx)(n.strong,{children:"Audio Enhancement"}),": Improving signaltonoise ratio\r\n",(0,a.jsx)(n.strong,{children:"Echo Cancellation"}),": Removing selfgenerated robot sounds from audio input"]}),"\n",(0,a.jsx)(n.h3,{id:"voice-command-understanding",children:"Voice Command Understanding"}),"\n",(0,a.jsx)(n.p,{children:"The transformation from speech to robot action involves several processing steps:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning of text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Command Mapping"}),": Converting natural language to robotspecific commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Context Integration"}),": Considering robot state and environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety Validation"}),": Ensuring commands are safe to execute"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution"}),": Sending validated commands to robot"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,a.jsx)(n.h3,{id:"lab-1-setting-up-whisper-for-robotics",children:"Lab 1: Setting up Whisper for Robotics"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Install Whisper and dependencies"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openaiwhisper\r\npip install torch torchaudio\r\npip install pyaudio sounddevice\r\npip install SpeechRecognition\r\npip install rospy sensor_msgs std_msgs\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a basic Whisper interface"})," (",(0,a.jsx)(n.code,{children:"whisper_robot_interface.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport whisper\r\nimport torch\r\nimport pyaudio\r\nimport wave\r\nimport numpy as np\r\nimport rospy\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom typing import Optional, Callable, Dict\r\nimport tempfile\r\nimport os\r\n\r\nclass WhisperRobotInterface:\r\n    def __init__(self, model_size="base", device="cuda" if torch.cuda.is_available() else "cpu"):\r\n        # Initialize Whisper model\r\n        print(f"Loading Whisper model \'{model_size}\' on {device}...")\r\n        self.model = whisper.load_model(model_size).to(device)\r\n        self.device = device\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000  # Whisper expects 16kHz\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        \r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n        \r\n        # Initialize ROS\r\n        rospy.init_node(\'whisper_robot_interface\', anonymous=True)\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.transcript_pub = rospy.Publisher(\'/voice_transcript\', String, queue_size=10)\r\n        self.command_pub = rospy.Publisher(\'/voice_command\', String, queue_size=10)\r\n        \r\n        # Internal state\r\n        self.transcript_queue = queue.Queue()\r\n        self.is_listening = False\r\n        self.recording_thread = None\r\n        self.processing_thread = None\r\n        \r\n        # Voice activity threshold\r\n        self.voice_activity_threshold = 0.01\r\n        self.min_voice_duration = 0.5  # seconds\r\n        self.max_silence_duration = 1.0  # seconds before stopping recording\r\n        \r\n        print("Whisper Robot Interface initialized")\r\n    \r\n    def start_listening(self):\r\n        """Start continuous listening for voice commands"""\r\n        if self.is_listening:\r\n            print("Already listening")\r\n            return\r\n        \r\n        self.is_listening = True\r\n        \r\n        # Start audio recording thread\r\n        self.recording_thread = threading.Thread(target=self._record_audio_continuously)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n        \r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._process_recordings)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        rospy.loginfo("Started listening for voice commands")\r\n    \r\n    def stop_listening(self):\r\n        """Stop listening for voice commands"""\r\n        self.is_listening = False\r\n        \r\n        if self.recording_thread is not None:\r\n            self.recording_thread.join(timeout=2.0)\r\n        \r\n        if self.processing_thread is not None:\r\n            self.processing_thread.join(timeout=2.0)\r\n        \r\n        rospy.loginfo("Stopped listening for voice commands")\r\n    \r\n    def _is_voice_active(self, audio_data):\r\n        """Detect if voice is active in audio chunk"""\r\n        # Convert to numpy array\r\n        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\r\n        \r\n        # Calculate RMS amplitude\r\n        rms = np.sqrt(np.mean(audio_array ** 2))\r\n        \r\n        return rms > self.voice_activity_threshold\r\n    \r\n    def _record_audio_continuously(self):\r\n        """Continuously record audio with voice activity detection"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        silence_counter = 0\r\n        frames = []\r\n        silence_chunks = int(self.rate / self.chunk * 0.1)  # 0.1s chunks for silence detection\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                if self._is_voice_active(data):\r\n                    if not recording:\r\n                        # Start recording\r\n                        rospy.loginfo("Voice activity detected, starting recording...")\r\n                        recording = True\r\n                        frames = [data]\r\n                        silence_counter = 0\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(data)\r\n                        silence_counter = 0\r\n                else:\r\n                    if recording:\r\n                        # Add to silence counter\r\n                        silence_counter += 1\r\n                        \r\n                        # Append to frames anyway (might be trailing speech)\r\n                        frames.append(data)\r\n                        \r\n                        # Check if silence duration exceeds threshold\r\n                        if silence_counter > int(self.max_silence_duration * self.rate / self.chunk):\r\n                            # End of speech detected\r\n                            rospy.loginfo(f"End of speech detected, recorded {len(frames)} frames")\r\n                            \r\n                            # Save recorded audio to temporary file\r\n                            temp_filename = self._save_audio_to_temp_file(frames)\r\n                            \r\n                            # Add to processing queue\r\n                            self.transcript_queue.put(temp_filename)\r\n                            \r\n                            # Reset for next recording\r\n                            recording = False\r\n                            frames = []\r\n                            silence_counter = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in audio recording: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _save_audio_to_temp_file(self, frames):\r\n        """Save audio frames to a temporary WAV file"""\r\n        # Create temporary WAV file\r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        temp_file = wave.open(temp_path, \'wb\')\r\n        temp_file.setnchannels(self.channels)\r\n        temp_file.setsampwidth(self.audio.get_sample_size(self.format))\r\n        temp_file.setframerate(self.rate)\r\n        temp_file.writeframes(b\'\'.join(frames))\r\n        temp_file.close()\r\n        os.close(temp_fd)\r\n        \r\n        return temp_path\r\n    \r\n    def _process_recordings(self):\r\n        """Process recorded audio files with Whisper"""\r\n        while self.is_listening or not self.transcript_queue.empty():\r\n            try:\r\n                # Get audio file from queue\r\n                audio_file = self.transcript_queue.get(timeout=1.0)\r\n                \r\n                # Transcribe audio\r\n                result = self._transcribe_audio(audio_file)\r\n                \r\n                if result and result.strip():\r\n                    rospy.loginfo(f"Transcribed: {result}")\r\n                    \r\n                    # Publish transcript\r\n                    transcript_msg = String()\r\n                    transcript_msg.data = result\r\n                    self.transcript_pub.publish(transcript_msg)\r\n                    \r\n                    # Process command\r\n                    self._process_command(result)\r\n                else:\r\n                    rospy.loginfo("No speech detected or transcription failed")\r\n                \r\n                # Clean up temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.transcript_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing audio: {e}")\r\n    \r\n    def _transcribe_audio(self, audio_file_path):\r\n        """Transcribe audio file using Whisper"""\r\n        try:\r\n            result = self.model.transcribe(\r\n                audio_file_path,\r\n                language="english",  # Specify language for better accuracy\r\n                fp16=torch.cuda.is_available()  # Use fp16 if CUDA available\r\n            )\r\n            return result[\'text\'].strip()\r\n        except Exception as e:\r\n            rospy.logerr(f"Transcription error: {e}")\r\n            return ""\r\n    \r\n    def _process_command(self, transcription):\r\n        """Process transcribed command and convert to robot action"""\r\n        # Simple command parsing (in a real system, use more sophisticated NLU)\r\n        command = self._parse_voice_command(transcription)\r\n        \r\n        if command:\r\n            rospy.loginfo(f"Executed command: {command}")\r\n            self.command_pub.publish(String(data=command))\r\n            self._execute_robot_action(command)\r\n        else:\r\n            rospy.logwarn(f"Unrecognized command: {transcription}")\r\n    \r\n    def _parse_voice_command(self, text):\r\n        """Simple command parser  in real system, use NLP/LLM"""\r\n        text_lower = text.lower().strip()\r\n        \r\n        # Navigation commands\r\n        if "move forward" in text_lower or "go forward" in text_lower or "forward" in text_lower:\r\n            return "MOVE_FORWARD"\r\n        elif "move backward" in text_lower or "go backward" in text_lower or "backward" in text_lower:\r\n            return "MOVE_BACKWARD"\r\n        elif "turn left" in text_lower or "left" in text_lower:\r\n            return "TURN_LEFT"\r\n        elif "turn right" in text_lower or "right" in text_lower:\r\n            return "TURN_RIGHT"\r\n        elif "stop" in text_lower or "halt" in text_lower:\r\n            return "STOP"\r\n        elif "come here" in text_lower or "come to me" in text_lower:\r\n            return "COME_HERE"\r\n        elif "follow me" in text_lower:\r\n            return "FOLLOW_ME"\r\n        elif "pick up" in text_lower or "grasp" in text_lower or "grab" in text_lower:\r\n            return "GRASP_OBJECT"\r\n        elif "put down" in text_lower or "release" in text_lower:\r\n            return "RELEASE_OBJECT"\r\n        \r\n        return None\r\n    \r\n    def _execute_robot_action(self, command):\r\n        """Execute robot action based on command"""\r\n        twist = Twist()\r\n        \r\n        if command == "MOVE_FORWARD":\r\n            twist.linear.x = 0.3  # m/s\r\n        elif command == "MOVE_BACKWARD":\r\n            twist.linear.x = 0.3\r\n        elif command == "TURN_LEFT":\r\n            twist.angular.z = 0.5  # rad/s\r\n        elif command == "TURN_RIGHT":\r\n            twist.angular.z = 0.5\r\n        elif command == "STOP":\r\n            twist.linear.x = 0.0\r\n            twist.angular.z = 0.0\r\n        elif command == "COME_HERE":\r\n            # For this example, just turn in place\r\n            twist.angular.z = 0.2\r\n        elif command == "FOLLOW_ME":\r\n            # This would require person tracking\r\n            pass\r\n        elif command == "GRASP_OBJECT":\r\n            # This would require gripper control\r\n            pass\r\n        elif command == "RELEASE_OBJECT":\r\n            # This would require gripper control\r\n            pass\r\n        else:\r\n            rospy.logwarn(f"Unknown command: {command}")\r\n            return\r\n        \r\n        # Publish command\r\n        self.cmd_vel_pub.publish(twist)\r\n    \r\n    def run(self):\r\n        """Run the main loop"""\r\n        rospy.loginfo("Whisper Robot Interface running...")\r\n        self.start_listening()\r\n        \r\n        try:\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down...")\r\n        finally:\r\n            self.stop_listening()\r\n\r\ndef main():\r\n    interface = WhisperRobotInterface(model_size="base")\r\n    interface.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-2-advanced-voice-command-processing",children:"Lab 2: Advanced Voice Command Processing"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create an advanced voice command processor"})," (",(0,a.jsx)(n.code,{children:"advanced_voice_processor.py"}),"):","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport whisper\r\nimport torch\r\nimport pyaudio\r\nimport numpy as np\r\nimport librosa\r\nimport rospy\r\nimport threading\r\nimport queue\r\nimport json\r\nimport re\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist, Point\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom typing import Dict, List, Optional, Tuple\r\nfrom dataclasses import dataclass\r\nimport tempfile\r\nimport os\r\n\r\n@dataclass\r\nclass VoiceCommand:\r\n    action: str\r\n    parameters: Dict\r\n    confidence: float\r\n    original_text: str\r\n\r\nclass AdvancedVoiceProcessor:\r\n    def __init__(self, model_size="base", device="cuda" if torch.cuda.is_available() else "cpu"):\r\n        # Initialize Whisper model\r\n        self.model = whisper.load_model(model_size).to(device)\r\n        self.device = device\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        \r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n        \r\n        # Initialize ROS\r\n        rospy.init_node(\'advanced_voice_processor\', anonymous=True)\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.voice_cmd_pub = rospy.Publisher(\'/parsed_voice_command\', String, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/voice_control_status\', String, queue_size=10)\r\n        self.feedback_pub = rospy.Publisher(\'/voice_feedback\', String, queue_size=10)\r\n        \r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        rospy.Subscriber(\'/voice_command_raw\', String, self.voice_command_callback)\r\n        \r\n        # State management\r\n        self.laser_data = None\r\n        self.robot_position = Point(x=0.0, y=0.0, z=0.0)\r\n        \r\n        # Command queue\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Voice activity detection\r\n        self.voice_activity_threshold = 0.005\r\n        self.min_voice_duration = 0.5  # seconds\r\n        self.min_silence_duration = 1.0\r\n        \r\n        # Thread management\r\n        self.is_listening = False\r\n        self.recording_thread = None\r\n        self.processing_thread = None\r\n        \r\n        # Command vocabularies\r\n        self.navigation_commands = [\r\n            "move forward", "move backward", "go forward", "go back",\r\n            "go straight", "move straight", "turn left", "turn right",\r\n            "pivot left", "pivot right", "rotate left", "rotate right", \r\n            "stop", "halt", "wait", "go to", "navigate to", "move to",\r\n            "approach", "come here", "move closer", "go away"\r\n        ]\r\n        \r\n        self.object_commands = [\r\n            "pick up", "grasp", "grab", "take", "lift", "drop",\r\n            "put down", "release", "place", "move", "bring", "fetch"\r\n        ]\r\n        \r\n        # Context memory for disambiguation\r\n        self.context_memory = []\r\n        self.max_context_items = 50\r\n        \r\n        rospy.loginfo("Advanced Voice Processor initialized")\r\n    \r\n    def laser_callback(self, msg: LaserScan):\r\n        """Update laser scan data for contextaware processing"""\r\n        self.laser_data = msg\r\n    \r\n    def voice_command_callback(self, msg: String):\r\n        """Handle voice command from another source (if needed)"""\r\n        # This could be used for preprocessed audio or simulation\r\n        pass\r\n    \r\n    def start_listening(self):\r\n        """Start voice command processing"""\r\n        if self.is_listening:\r\n            return\r\n        \r\n        self.is_listening = True\r\n        \r\n        # Start threads\r\n        self.recording_thread = threading.Thread(target=self._continuous_recording)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n        \r\n        self.processing_thread = threading.Thread(target=self._process_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        self.status_pub.publish(String(data="Voice control active"))\r\n        rospy.loginfo("Advanced voice processor started")\r\n    \r\n    def stop_listening(self):\r\n        """Stop voice command processing"""\r\n        self.is_listening = False\r\n        self.status_pub.publish(String(data="Voice control inactive"))\r\n    \r\n    def _detect_voice_activity(self, audio_chunk):\r\n        """Enhanced voice activity detection with multiple features"""\r\n        # Convert to numpy\r\n        audio_np = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0\r\n        \r\n        # Calculate multiple features for robust VAD\r\n        rms = np.sqrt(np.mean(audio_np ** 2))\r\n        \r\n        # Use librosa for more sophisticated features\r\n        try:\r\n            # Spectral features\r\n            stft = librosa.stft(audio_np)\r\n            spectral_centroids = librosa.feature.spectral_centroid(S=np.abs(stft))[0]\r\n            spectral_rolloff = librosa.feature.spectral_rolloff(S=np.abs(stft))[0]\r\n            \r\n            # Calculate average spectral centroid and rolloff\r\n            avg_centroid = np.mean(spectral_centroids)\r\n            avg_rolloff = np.mean(spectral_rolloff)\r\n            \r\n            # Combined voice activity score\r\n            va_score = (\r\n                0.4 * (rms / 0.01) +  # Normalize RMS\r\n                0.3 * (avg_centroid / 2000) +  # Normalize spectral centroid\r\n                0.3 * (avg_rolloff / 5000)   # Normalize rolloff\r\n            ) / 3.0\r\n            \r\n            return va_score > self.voice_activity_threshold\r\n            \r\n        except:\r\n            # Fallback to simple RMS if librosa fails\r\n            return rms > self.voice_activity_threshold\r\n    \r\n    def _continuous_recording(self):\r\n        """Continuously record audio with enhanced voice activity detection"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        silence_counter = 0\r\n        frames = []\r\n        silence_chunks = int(self.min_silence_duration * self.rate / self.chunk)\r\n        min_voice_chunks = int(self.min_voice_duration * self.rate / self.chunk)\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                if self._detect_voice_activity(data):\r\n                    if not recording:\r\n                        # Potential start of speech\r\n                        rospy.loginfo("Potential speech start detected")\r\n                        recording = True\r\n                        frames = [data]\r\n                        silence_counter = 0\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(data)\r\n                        silence_counter = 0\r\n                else:\r\n                    if recording:\r\n                        # Accumulate silence\r\n                        frames.append(data)  # Add to frames for trailing audio\r\n                        silence_counter += 1\r\n                        \r\n                        # End recording if silence exceeds threshold\r\n                        if silence_counter >= silence_chunks:\r\n                            if len(frames) >= min_voice_chunks:  # Ensure minimum speech duration\r\n                                # Create temp file and add to queue\r\n                                temp_file = self._save_audio_chunk(frames)\r\n                                self.command_queue.put(temp_file)\r\n                                rospy.loginfo(f"Recorded speech segment: {len(frames)} frames")\r\n                            \r\n                            # Reset for next segment\r\n                            recording = False\r\n                            frames = []\r\n                            silence_counter = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in continuous recording: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _save_audio_chunk(self, frames):\r\n        """Save audio chunk to temporary file"""\r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        wf = wave.open(temp_path, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        os.close(temp_fd)\r\n        return temp_path\r\n    \r\n    def _process_commands(self):\r\n        """Process audio files in the queue with Whisper"""\r\n        while self.is_listening or not self.command_queue.empty():\r\n            try:\r\n                audio_file = self.command_queue.get(timeout=1.0)\r\n                \r\n                # Transcribe with Whisper\r\n                transcription = self._transcribe_audio(audio_file)\r\n                \r\n                if transcription and transcription.strip():\r\n                    rospy.loginfo(f"Transcribed: {transcription}")\r\n                    \r\n                    # Process with advanced command parsing\r\n                    command = self._parse_advanced_command(transcription)\r\n                    \r\n                    if command:\r\n                        # Validate command with context\r\n                        if self._validate_command(command):\r\n                            self._execute_command(command)\r\n                            self._publish_command(command)\r\n                        else:\r\n                            rospy.logwarn(f"Command validation failed: {command}")\r\n                            self._publish_feedback("Command unsafe or invalid", level="warning")\r\n                    else:\r\n                        rospy.logwarn(f"Could not parse command: {transcription}")\r\n                        self._publish_feedback("Could not understand command", level="error")\r\n                else:\r\n                    rospy.loginfo("No speech detected")\r\n                \r\n                # Clean up temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command: {e}")\r\n    \r\n    def _transcribe_audio(self, audio_path):\r\n        """Transcribe audio with error handling and retry"""\r\n        max_retries = 3\r\n        for attempt in range(max_retries):\r\n            try:\r\n                result = self.model.transcribe(audio_path, language="english")\r\n                return result[\'text\'].strip()\r\n            except Exception as e:\r\n                rospy.logwarn(f"Transcription attempt {attempt+1} failed: {e}")\r\n                if attempt == max_retries  1:\r\n                    return ""  # Return empty string after all retries\r\n    \r\n    def _parse_advanced_command(self, text: str) > Optional[VoiceCommand]:\r\n        """Advanced command parsing with context awareness"""\r\n        original_text = text\r\n        \r\n        # Clean text\r\n        text = re.sub(r\'[^\\w\\s]\', \' \', text.lower())\r\n        text = \' \'.join(text.split())  # Remove extra whitespace\r\n        \r\n        # Define command patterns with parameters\r\n        command_patterns = [\r\n            # Navigation with parameters\r\n            (r\'.*\\b(move|go|navigate)\\b.*\\b(forward|ahead|straight)\\b.*\\b(\\d+(?:\\.\\d+)?)\\b.*\\b(meter|meters|metre|metres)\\b\', \r\n             self._parse_move_distance),\r\n            (r\'.*\\b(turn|rotate|pivot)\\b.*\\b(left|right)\\b.*\\b(\\d+(?:\\.\\d+)?)\\b.*\\b(degrees|deg)\\b\', \r\n             self._parse_turn_degrees),\r\n            (r\'.*\\b(approach|move to|go to)\\b.*\\b(\\w+)\\b\', self._parse_approach_object),\r\n            \r\n            # Simple navigation\r\n            (r\'.*\\b(forward|ahead|straight)\\b.*\', lambda m, t: VoiceCommand("move_forward", {"distance": 1.0}, 0.8, t)),\r\n            (r\'.*\\b(backward|reverse|back)\\b.*\', lambda m, t: VoiceCommand("move_backward", {"distance": 1.0}, 0.8, t)),\r\n            (r\'.*\\b(left|port)\\b.*\', lambda m, t: VoiceCommand("turn_left", {"angle": 90.0}, 0.8, t)),\r\n            (r\'.*\\b(right|starboard)\\b.*\', lambda m, t: VoiceCommand("turn_right", {"angle": 90.0}, 0.8, t)),\r\n            \r\n            # Object manipulation\r\n            (r\'.*\\b(pick up|grasp|grab|take|lift)\\b.*\\b(\\w+)\\b\', self._parse_pick_object),\r\n            (r\'.*\\b(place|put down|drop|release)\\b.*\\b(\\w+)\\b\', self._parse_place_object),\r\n            \r\n            # Stop/abort\r\n            (r\'.*\\b(stop|halt|park|abort|cancel)\\b.*\', lambda m, t: VoiceCommand("stop", {}, 0.9, t)),\r\n        ]\r\n        \r\n        # Try each pattern\r\n        for pattern, handler in command_patterns:\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                try:\r\n                    command = handler(match, original_text)\r\n                    if command:\r\n                        return command\r\n                except Exception as e:\r\n                    rospy.logerr(f"Error in command handler: {e}")\r\n        \r\n        # If no specific pattern matched, try general classification\r\n        return self._classify_general_command(text, original_text)\r\n    \r\n    def _parse_move_distance(self, match, text):\r\n        """Parse distancebased movement command"""\r\n        distance = float(match.group(3))\r\n        return VoiceCommand(\r\n            "move_distance",\r\n            {"direction": "forward", "distance": distance},  # Would need more context to determine direction\r\n            0.7,\r\n            text\r\n        )\r\n    \r\n    def _parse_turn_degrees(self, match, text):\r\n        """Parse degreebased turn command"""\r\n        direction = match.group(2)\r\n        angle = float(match.group(3))\r\n        return VoiceCommand(\r\n            "turn_degrees",\r\n            {"direction": direction, "angle": angle},\r\n            0.7,\r\n            text\r\n        )\r\n    \r\n    def _parse_approach_object(self, match, text):\r\n        """Parse approach object command"""\r\n        object_name = match.group(2)\r\n        return VoiceCommand(\r\n            "approach_object",\r\n            {"object_name": object_name},\r\n            0.6,\r\n            text\r\n        )\r\n    \r\n    def _parse_pick_object(self, match, text):\r\n        """Parse pick up object command"""\r\n        object_name = match.group(2)\r\n        return VoiceCommand(\r\n            "pick_object",\r\n            {"object_name": object_name},\r\n            0.6,\r\n            text\r\n        )\r\n    \r\n    def _parse_place_object(self, match, text):\r\n        """Parse place object command"""\r\n        object_name = match.group(2)\r\n        return VoiceCommand(\r\n            "place_object", \r\n            {"object_name": object_name, "location": "current_position"},  # Would need more context\r\n            0.6,\r\n            text\r\n        )\r\n    \r\n    def _classify_general_command(self, clean_text: str, original_text: str):\r\n        """Classify commands that don\'t match specific patterns"""\r\n        # Use keyword matching for general classification\r\n        text_lower = clean_text.lower()\r\n        \r\n        # Check for navigationrelated keywords\r\n        if any(keyword in text_lower for keyword in [\'move\', \'go\', \'walk\', \'drive\', \'navigate\', \'forward\', \'backward\', \'left\', \'right\']):\r\n            return VoiceCommand("navigate", {}, 0.5, original_text)\r\n        elif any(keyword in text_lower for keyword in [\'turn\', \'rotate\', \'pivot\', \'spin\', \'around\']):\r\n            return VoiceCommand("rotate", {}, 0.5, original_text)\r\n        elif any(keyword in text_lower for keyword in [\'stop\', \'halt\', \'pause\', \'wait\', \'freeze\']):\r\n            return VoiceCommand("stop", {}, 0.8, original_text)\r\n        elif any(keyword in text_lower for keyword in [\'pick\', \'grasp\', \'grab\', \'take\', \'lift\', \'hold\']):\r\n            return VoiceCommand("manipulate", {"action": "grasp"}, 0.5, original_text)\r\n        elif any(keyword in text_lower for keyword in [\'place\', \'put\', \'set\', \'drop\', \'release\']):\r\n            return VoiceCommand("manipulate", {"action": "release"}, 0.5, original_text)\r\n        else:\r\n            return None  # Unknown command\r\n    \r\n    def _validate_command(self, command: VoiceCommand) > bool:\r\n        """Validate command considering current state and environment"""\r\n        # Check if environment is safe for this command\r\n        if command.action == "move_forward" and self.laser_data:\r\n            # Check for obstacles ahead (simplified  check middle third of scan)\r\n            middle_start = len(self.laser_data.ranges) // 3\r\n            middle_end = 2 * len(self.laser_data.ranges) // 3\r\n            middle_ranges = self.laser_data.ranges[middle_start:middle_end]\r\n            \r\n            # Filter invalid ranges\r\n            valid_ranges = [r for r in middle_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            if valid_ranges and min(valid_ranges) < 0.5:  # 0.5m safety distance\r\n                rospy.logwarn("Obstacle detected ahead, aborting forward movement")\r\n                return False\r\n        \r\n        # Additional validation checks could go here\r\n        #  Check robot state (battery level, joint limits, etc.)\r\n        #  Check environmental constraints\r\n        #  Check safety zones\r\n        \r\n        return True\r\n    \r\n    def _execute_command(self, command: VoiceCommand):\r\n        """Execute validated command"""\r\n        rospy.loginfo(f"Executing: {command.action} with params: {command.parameters}")\r\n        \r\n        if command.action in ["move_forward", "move_distance"]:\r\n            self._execute_movement_command(command)\r\n        elif command.action in ["turn_left", "turn_right", "turn_degrees"]:\r\n            self._execute_rotation_command(command)\r\n        elif command.action in ["stop", "halt"]:\r\n            self._execute_stop_command()\r\n        elif command.action in ["approach_object", "navigate_to"]:\r\n            self._execute_navigation_command(command)\r\n        elif command.action in ["pick_object", "place_object"]:\r\n            self._execute_manipulation_command(command)\r\n        else:\r\n            rospy.logwarn(f"Unknown command action: {command.action}")\r\n    \r\n    def _execute_movement_command(self, command: VoiceCommand):\r\n        """Execute movement command"""\r\n        twist = Twist()\r\n        distance = command.parameters.get("distance", 1.0)\r\n        speed = 0.3  # m/s\r\n        \r\n        # Calculate time needed: distance / speed\r\n        duration = distance / speed\r\n        \r\n        # Move for calculated duration\r\n        twist.linear.x = speed\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        start_time = rospy.Time.now()\r\n        \r\n        while (rospy.Time.now()  start_time).to_sec() < duration and not rospy.is_shutdown():\r\n            self.cmd_vel_pub.publish(twist)\r\n            rate.sleep()\r\n        \r\n        # Stop robot\r\n        self._send_stop_command()\r\n    \r\n    def _execute_rotation_command(self, command: VoiceCommand):\r\n        """Execute rotation command"""\r\n        twist = Twist()\r\n        angle = command.parameters.get("angle", 90.0)\r\n        direction = command.parameters.get("direction", "left")\r\n        \r\n        # Convert angle to time (assuming 0.5 rad/s angular velocity)\r\n        angular_vel = 0.5  # rad/s\r\n        duration = np.radians(angle) / angular_vel\r\n        \r\n        # Set rotation direction\r\n        twist.angular.z = angular_vel if direction == "left" else angular_vel\r\n        \r\n        rate = rospy.Rate(10)\r\n        start_time = rospy.Time.now()\r\n        \r\n        while (rospy.Time.now()  start_time).to_sec() < duration and not rospy.is_shutdown():\r\n            self.cmd_vel_pub.publish(twist)\r\n            rate.sleep()\r\n        \r\n        # Stop rotation\r\n        self._send_stop_command()\r\n    \r\n    def _execute_stop_command(self):\r\n        """Execute stop command"""\r\n        self._send_stop_command()\r\n    \r\n    def _send_stop_command(self):\r\n        """Send stop command to robot"""\r\n        twist = Twist()\r\n        self.cmd_vel_pub.publish(twist)\r\n    \r\n    def _publish_command(self, command: VoiceCommand):\r\n        """Publish parsed command"""\r\n        cmd_dict = {\r\n            "action": command.action,\r\n            "parameters": command.parameters,\r\n            "confidence": command.confidence,\r\n            "original_text": command.original_text,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        }\r\n        \r\n        cmd_msg = String()\r\n        cmd_msg.data = json.dumps(cmd_dict)\r\n        self.voice_cmd_pub.publish(cmd_msg)\r\n    \r\n    def _publish_feedback(self, message: str, level: str = "info"):\r\n        """Publish voice processing feedback"""\r\n        feedback_msg = String()\r\n        feedback_msg.data = json.dumps({\r\n            "message": message,\r\n            "level": level,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.feedback_pub.publish(feedback_msg)\r\n        \r\n        # Log message with appropriate level\r\n        if level == "error":\r\n            rospy.logerr(message)\r\n        elif level == "warning":\r\n            rospy.logwarn(message)\r\n        else:\r\n            rospy.loginfo(message)\r\n    \r\n    def run(self):\r\n        """Run the main processing loop"""\r\n        rospy.loginfo("Starting advanced voice processor...")\r\n        self.start_listening()\r\n        \r\n        try:\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down voice processor...")\r\n        finally:\r\n            self.stop_listening()\r\n\r\ndef main():\r\n    processor = AdvancedVoiceProcessor(model_size="base")\r\n    processor.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-3-creating-a-voice-control-safety-system",children:"Lab 3: Creating a Voice Control Safety System"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create a safetyaware voice control system"})," (",(0,a.jsx)(n.code,{children:"voice_control_safety.py"}),"):","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport pyaudio\r\nimport numpy as np\r\nimport threading\r\nimport queue\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan, Imu\r\nfrom voice_recognition_module import WhisperRobotInterface\r\nimport time\r\nimport json\r\n\r\nclass VoiceControlSafetySystem:\r\n    def __init__(self):\r\n        rospy.init_node(\'voice_control_safety_system\', anonymous=True)\r\n        \r\n        # Initialize whisper interface\r\n        self.whisper_interface = WhisperRobotInterface(model_size="base")\r\n        \r\n        # Publishers\r\n        self.safed_cmd_pub = rospy.Publisher(\'/cmd_vel_safe\', Twist, queue_size=10)\r\n        self.safety_status_pub = rospy.Publisher(\'/voice_control_safety_status\', String, queue_size=10)\r\n        self.emergency_stop_pub = rospy.Publisher(\'/emergency_stop\', Bool, queue_size=10)\r\n        \r\n        # Subscribers\r\n        self.laser_sub = rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        self.imu_sub = rospy.Subscriber(\'/imu/data\', Imu, self.imu_callback)\r\n        self.voice_cmd_sub = rospy.Subscriber(\'/voice_command\', String, self.voice_command_callback)\r\n        \r\n        # Safety parameters\r\n        self.safety_distance = rospy.get_param(\'~safety_distance\', 0.5)  # meters\r\n        self.max_linear_speed = rospy.get_param(\'~max_linear_speed\', 0.5)  # m/s\r\n        self.max_angular_speed = rospy.get_param(\'~max_angular_speed\', 0.8)  # rad/s\r\n        self.emergency_stop_enabled = True\r\n        \r\n        # State variables\r\n        self.laser_data = None\r\n        self.imu_data = None\r\n        self.current_command = None\r\n        self.is_safe_to_proceed = True\r\n        self.last_safe_check = 0\r\n        self.safe_check_interval = 0.1  # seconds\r\n        \r\n        # Emergency stop state\r\n        self.emergency_stop_active = False\r\n        self.last_emergency_time = 0\r\n        self.emergency_reset_time = 5.0  # seconds to wait before reset\r\n        \r\n        # Command queue for processing\r\n        self.command_queue = queue.Queue()\r\n        \r\n        rospy.loginfo("Voice Control Safety System initialized")\r\n    \r\n    def laser_callback(self, msg: LaserScan):\r\n        """Update laser scan data"""\r\n        self.laser_data = msg\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """Update IMU data"""\r\n        self.imu_data = msg\r\n    \r\n    def voice_command_callback(self, msg: String):\r\n        """Process voice command with safety validation"""\r\n        if self.emergency_stop_active:\r\n            rospy.logwarn("Emergency stop active, ignoring voice command")\r\n            self.publish_safety_status("EMERGENCY_STOP_ACTIVE")\r\n            return\r\n        \r\n        try:\r\n            command_data = json.loads(msg.data)\r\n            command_type = command_data.get(\'action\', \'unknown\')\r\n            command_params = command_data.get(\'parameters\', {})\r\n            \r\n            # Validate command for safety\r\n            if self.is_command_safe(command_type, command_params):\r\n                # Add to processing queue\r\n                self.command_queue.put({\r\n                    \'type\': command_type,\r\n                    \'params\': command_params,\r\n                    \'original_message\': msg.data,\r\n                    \'timestamp\': rospy.Time.now().to_sec()\r\n                })\r\n                rospy.loginfo(f"Safe command queued: {command_type}")\r\n            else:\r\n                rospy.logwarn(f"Unsafe command blocked: {command_type}")\r\n                self.publish_feedback("Command blocked for safety reasons", "warning")\r\n                \r\n        except json.JSONDecodeError:\r\n            # Handle as simple command string\r\n            command_str = msg.data\r\n            \r\n            # Simple validation based on command content\r\n            if self.is_simple_command_safe(command_str):\r\n                self.command_queue.put({\r\n                    \'type\': \'simple_command\',\r\n                    \'params\': {\'command\': command_str},\r\n                    \'original_message\': msg.data,\r\n                    \'timestamp\': rospy.Time.now().to_sec()\r\n                })\r\n                rospy.loginfo(f"Safe command queued: {command_str}")\r\n            else:\r\n                rospy.logwarn(f"Unsafe simple command blocked: {command_str}")\r\n                self.publish_feedback("Command blocked for safety reasons", "warning")\r\n    \r\n    def is_command_safe(self, command_type: str, parameters: dict) > bool:\r\n        """Validate if command is safe to execute"""\r\n        # Check for emergency stop keywords\r\n        if any(keyword in command_type.lower() for keyword in [\'emergency\', \'stop\', \'halt\', \'danger\']):\r\n            return True  # These are valid safety commands\r\n        \r\n        # Check environmental safety for movement commands\r\n        if command_type in [\'move_forward\', \'move_backward\', \'move_distance\']:\r\n            return self.is_movement_safe(parameters)\r\n        \r\n        # Check for rotation commands\r\n        if command_type in [\'turn_left\', \'turn_right\', \'turn_degrees\']:\r\n            # Rotations are generally safer than translations\r\n            return True\r\n        \r\n        # Check for manipulation commands in safe areas\r\n        if command_type in [\'pick_object\', \'place_object\']:\r\n            return self.is_manipulation_safe(parameters)\r\n        \r\n        return True  # Default to safe for other commands\r\n    \r\n    def is_simple_command_safe(self, command_str: str) > bool:\r\n        """Validate simple string command for safety"""\r\n        command_lower = command_str.lower()\r\n        \r\n        # Check for dangerous words that should trigger safety checks\r\n        dangerous_indicators = [\r\n            \'danger\', \'emergency\', \'crash\', \'break\', \'smash\', \r\n            \'damage\', \'destroy\', \'hurt\', \'injure\', \'kill\'\r\n        ]\r\n        \r\n        # Check if command has dangerous indicators but no safety context\r\n        has_danger = any(indicator in command_lower for indicator in dangerous_indicators)\r\n        has_request = any(phrase in command_lower for phrase in [\'please\', \'could you\', \'can you\'])\r\n        has_stop_phrase = any(phrase in command_lower for phrase in [\'stop\', \'emergency\', \'help\'])\r\n        \r\n        if has_danger and not has_request and not has_stop_phrase:\r\n            rospy.logwarn(f"Potentially dangerous command detected: {command_str}")\r\n            return False\r\n        \r\n        # Allow commands that include safety words with context\r\n        if has_stop_phrase:\r\n            return True  # Stop requests are always safe\r\n        \r\n        # Check if movement command is safe\r\n        if any(phrase in command_lower for phrase in [\'forward\', \'ahead\', \'move\', \'go\']):\r\n            return self.is_movement_safe({\'direction\': \'forward\', \'distance\': 1.0})\r\n        \r\n        return True\r\n    \r\n    def is_movement_safe(self, params: dict) > bool:\r\n        """Check if movement command is safe"""\r\n        if not self.laser_data:\r\n            rospy.logwarn("No laser data available, assuming unsafe for movement")\r\n            return False\r\n        \r\n        # Check for obstacles in the movement direction\r\n        if params.get(\'direction\') == \'forward\' or params.get(\'action\') == \'move_forward\':\r\n            # Check front sector (\xb130 degrees)\r\n            front_sector = self.laser_data.ranges[\r\n                len(self.laser_data.ranges)//2  30 :\r\n                len(self.laser_data.ranges)//2 + 30\r\n            ]\r\n            \r\n            # Filter valid ranges\r\n            valid_ranges = [r for r in front_sector if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            if valid_ranges:\r\n                min_distance = min(valid_ranges)\r\n                if min_distance < self.safety_distance:\r\n                    rospy.logwarn(f"Obstacle detected: {min_distance:.2f}m ahead, unsafe to move forward")\r\n                    return False\r\n        \r\n        # Check for other directions based on command\r\n        return True\r\n    \r\n    def is_manipulation_safe(self, params: dict) > bool:\r\n        """Check if manipulation command is safe"""\r\n        # For manipulation, we\'d need to check joint limits, collisions, etc.\r\n        # This is a simplified check  in practice, would use MoveIt! collision checking\r\n        return True\r\n    \r\n    def process_command_queue(self):\r\n        """Process commands from the safetyvalidated queue"""\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        \r\n        while not rospy.is_shutdown():\r\n            try:\r\n                # Check for commands to process\r\n                try:\r\n                    cmd = self.command_queue.get_nowait()\r\n                    \r\n                    # Execute the validated command\r\n                    self.execute_validated_command(cmd)\r\n                    \r\n                except queue.Empty:\r\n                    pass  # No commands to process\r\n                \r\n                # Perform periodic safety checks\r\n                current_time = rospy.Time.now().to_sec()\r\n                if (current_time  self.last_safe_check) > self.safe_check_interval:\r\n                    self.perform_periodic_safety_check()\r\n                    self.last_safe_check = current_time\r\n                \r\n                rate.sleep()\r\n                \r\n            except KeyboardInterrupt:\r\n                rospy.loginfo("Voice Control Safety System interrupted")\r\n                break\r\n    \r\n    def execute_validated_command(self, cmd: dict):\r\n        """Execute a safetyvalidated command"""\r\n        command_type = cmd[\'type\']\r\n        \r\n        # Create Twist message based on command type\r\n        twist_cmd = Twist()\r\n        \r\n        if command_type == "move_forward":\r\n            twist_cmd.linear.x = min(self.max_linear_speed, cmd[\'params\'].get(\'speed\', 0.3))\r\n        elif command_type == "move_backward":\r\n            twist_cmd.linear.x = min(self.max_linear_speed, cmd[\'params\'].get(\'speed\', 0.3))\r\n        elif command_type == "turn_left":\r\n            twist_cmd.angular.z = min(self.max_angular_speed, cmd[\'params\'].get(\'speed\', 0.5))\r\n        elif command_type == "turn_right":\r\n            twist_cmd.angular.z = min(self.max_angular_speed, cmd[\'params\'].get(\'speed\', 0.5))\r\n        elif command_type == "stop":\r\n            twist_cmd.linear.x = 0.0\r\n            twist_cmd.angular.z = 0.0\r\n        \r\n        # Apply safety limits\r\n        twist_cmd.linear.x = max(self.max_linear_speed, min(self.max_linear_speed, twist_cmd.linear.x))\r\n        twist_cmd.angular.z = max(self.max_angular_speed, min(self.max_angular_speed, twist_cmd.angular.z))\r\n        \r\n        # Publish the command\r\n        self.safed_cmd_pub.publish(twist_cmd)\r\n        rospy.loginfo(f"Executed safe command: {command_type}")\r\n    \r\n    def perform_periodic_safety_check(self):\r\n        """Perform periodic safety checks on the environment"""\r\n        if not self.laser_data:\r\n            return\r\n        \r\n        # Check for emergency conditions\r\n        emergency_detected = self.check_for_emergency_conditions()\r\n        \r\n        if emergency_detected:\r\n            self.trigger_emergency_stop()\r\n        elif self.emergency_stop_active:\r\n            # Check if it\'s time to reset emergency stop\r\n            if rospy.Time.now().to_sec()  self.last_emergency_time > self.emergency_reset_time:\r\n                self.reset_emergency_stop()\r\n    \r\n    def check_for_emergency_conditions(self) > bool:\r\n        """Check if emergency conditions exist"""\r\n        if not self.laser_data:\r\n            return False\r\n        \r\n        # Check if obstacles are very close (within safety distance)\r\n        close_ranges = [r for r in self.laser_data.ranges \r\n                       if not (np.isinf(r) or np.isnan(r)) and r < self.safety_distance * 0.5]\r\n        \r\n        return len(close_ranges) > 3  # More than 3 close obstacles is an emergency\r\n    \r\n    def trigger_emergency_stop(self):\r\n        """Trigger emergency stop"""\r\n        if not self.emergency_stop_active:\r\n            self.emergency_stop_active = True\r\n            self.last_emergency_time = rospy.Time.now().to_sec()\r\n            \r\n            # Publish emergency stop message\r\n            emergency_msg = Bool()\r\n            emergency_msg.data = True\r\n            self.emergency_stop_pub.publish(emergency_msg)\r\n            \r\n            # Stop the robot immediately\r\n            stop_cmd = Twist()\r\n            self.safed_cmd_pub.publish(stop_cmd)\r\n            \r\n            rospy.logerr("EMERGENCY STOP ACTIVATED")\r\n            self.publish_safety_status("EMERGENCY_STOP_ACTIVATED")\r\n    \r\n    def reset_emergency_stop(self):\r\n        """Reset emergency stop"""\r\n        if self.emergency_stop_active:\r\n            self.emergency_stop_active = False\r\n            \r\n            # Publish reset message\r\n            reset_msg = Bool()\r\n            reset_msg.data = False\r\n            self.emergency_stop_pub.publish(reset_msg)\r\n            \r\n            rospy.loginfo("Emergency stop reset")\r\n            self.publish_safety_status("EMERGENCY_STOP_RESET")\r\n    \r\n    def publish_safety_status(self, status: str):\r\n        """Publish safety system status"""\r\n        status_msg = String()\r\n        status_msg.data = json.dumps({\r\n            "status": status,\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "emergency_active": self.emergency_stop_active,\r\n            "safety_distance": self.safety_distance\r\n        })\r\n        self.safety_status_pub.publish(status_msg)\r\n    \r\n    def publish_feedback(self, message: str, level: str = "info"):\r\n        """Publish feedback message"""\r\n        feedback_msg = String()\r\n        feedback_msg.data = json.dumps({\r\n            "message": message,\r\n            "level": level,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        # Publish to a generic feedback topic (would need to be defined elsewhere)\r\n        # For this example, we\'ll just log it\r\n        if level == "error":\r\n            rospy.logerr(message)\r\n        elif level == "warning":\r\n            rospy.logwarn(message)\r\n        else:\r\n            rospy.loginfo(message)\r\n    \r\n    def run(self):\r\n        """Run the safety system"""\r\n        rospy.loginfo("Voice Control Safety System starting...")\r\n        self.publish_safety_status("SYSTEM_STARTED")\r\n        \r\n        try:\r\n            self.process_command_queue()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down Voice Control Safety System...")\r\n\r\ndef main():\r\n    safety_system = VoiceControlSafetySystem()\r\n    safety_system.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,a.jsx)(n.p,{children:"Here's a complete working example of the voicetoaction system with safety validation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_voice_control_system.py\r\n\r\nimport rospy\r\nimport openai\r\nimport whisper\r\nimport torch\r\nimport pyaudio\r\nimport numpy as np\r\nimport json\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist, Point\r\nfrom sensor_msgs.msg import LaserScan, Imu\r\nfrom cv_bridge import CvBridge\r\n\r\nclass CompleteVoiceControlSystem:\r\n    """Complete voice control system with safety validation"""\r\n    \r\n    def __init__(self, api_key: str):\r\n        rospy.init_node(\'complete_voice_control_system\', anonymous=True)\r\n        \r\n        # Initialize Whisper model\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        self.whisper_model = whisper.load_model("base").to(self.device)\r\n        \r\n        # Initialize OpenAI client if API key provided\r\n        self.openai_client = None\r\n        if api_key:\r\n            from openai import OpenAI\r\n            self.openai_client = OpenAI(api_key=api_key)\r\n        \r\n        # Initialize OpenCV bridge for potential visual integration\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Audio parameters\r\n        self.audio = pyaudio.PyAudio()\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.voice_transcript_pub = rospy.Publisher(\'/voice_transcript\', String, queue_size=10)\r\n        self.parsed_command_pub = rospy.Publisher(\'/parsed_voice_command\', String, queue_size=10)\r\n        self.system_status_pub = rospy.Publisher(\'/voice_system_status\', String, queue_size=10)\r\n        self.safety_status_pub = rospy.Publisher(\'/voice_safety_status\', String, queue_size=10)\r\n        \r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        rospy.Subscriber(\'/imu/data\', Imu, self.imu_callback)\r\n        \r\n        # System state\r\n        self.laser_data = None\r\n        self.imu_data = None\r\n        self.is_listening = False\r\n        self.recording_thread = None\r\n        self.processing_thread = None\r\n        \r\n        # Safety parameters\r\n        self.safety_distance = rospy.get_param(\'~safety_distance\', 0.5)\r\n        self.max_linear_speed = rospy.get_param(\'~max_linear_speed\', 0.4)\r\n        self.max_angular_speed = rospy.get_param(\'~max_angular_speed\', 0.6)\r\n        \r\n        # Voice activity detection\r\n        self.voice_threshold = 0.005\r\n        self.min_voice_duration = 0.3\r\n        self.min_silence_duration = 0.8\r\n        \r\n        # Command and safety queues\r\n        self.command_queue = queue.Queue()\r\n        self.safety_queue = queue.Queue()\r\n        \r\n        # Internal state\r\n        self.current_command = None\r\n        self.emergency_stop_active = False\r\n        \r\n        rospy.loginfo("Complete Voice Control System initialized")\r\n    \r\n    def laser_callback(self, msg: LaserScan):\r\n        """Update laser scan data"""\r\n        self.laser_data = msg\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """Update IMU data"""\r\n        self.imu_data = msg\r\n    \r\n    def start_system(self):\r\n        """Start the complete voice control system"""\r\n        self.is_listening = True\r\n        \r\n        # Start audio recording thread\r\n        self.recording_thread = threading.Thread(target=self._audio_recording_loop)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n        \r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._command_processing_loop)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        # Start safety monitoring thread\r\n        self.safety_thread = threading.Thread(target=self._safety_monitoring_loop)\r\n        self.safety_thread.daemon = True\r\n        self.safety_thread.start()\r\n        \r\n        rospy.loginfo("Complete Voice Control System started")\r\n        self._publish_status("System active and listening")\r\n    \r\n    def stop_system(self):\r\n        """Stop the complete voice control system"""\r\n        self.is_listening = False\r\n        self._publish_status("System stopped")\r\n        rospy.loginfo("Complete Voice Control System stopped")\r\n    \r\n    def _audio_recording_loop(self):\r\n        """Audio recording loop with voice activity detection"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        frames = []\r\n        voice_active_count = 0\r\n        silence_count = 0\r\n        min_voice_frames = int(self.min_voice_duration * self.rate / self.chunk)\r\n        min_silence_frames = int(self.min_silence_duration * self.rate / self.chunk)\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                if self._is_voice_active(data):\r\n                    if not recording:\r\n                        # Start recording when voice is detected\r\n                        voice_active_count += 1\r\n                        if voice_active_count >= min_voice_frames:\r\n                            recording = True\r\n                            frames = [data] * min_voice_frames  # Include preroll\r\n                            voice_active_count = 0\r\n                            silence_count = 0\r\n                            rospy.loginfo("Voice activity confirmed, recording started")\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(data)\r\n                        silence_count = 0\r\n                        voice_active_count = 0\r\n                else:\r\n                    if recording:\r\n                        # Add to silence counter\r\n                        frames.append(data)\r\n                        silence_count += 1\r\n                        \r\n                        if silence_count >= min_silence_frames:\r\n                            # End of speech detected\r\n                            if len(frames) >= min_voice_frames:\r\n                                # Save to temp file and add to processing queue\r\n                                temp_file = self._save_audio_frames(frames)\r\n                                self.command_queue.put(temp_file)\r\n                                rospy.loginfo(f"Recorded audio segment with {len(frames)} frames")\r\n                            \r\n                            # Reset for next recording\r\n                            recording = False\r\n                            frames = []\r\n                            silence_count = 0\r\n                            voice_active_count = 0\r\n                    else:\r\n                        # Reset voice counter when not recording\r\n                        voice_active_count = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in audio recording loop: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _is_voice_active(self, audio_data):\r\n        """Detect voice activity in audio chunk"""\r\n        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\r\n        rms = np.sqrt(np.mean(audio_np ** 2))\r\n        return rms > self.voice_threshold\r\n    \r\n    def _save_audio_frames(self, frames):\r\n        """Save audio frames to temporary file"""\r\n        import tempfile\r\n        import wave\r\n        \r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        wf = wave.open(temp_path, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        os.close(temp_fd)\r\n        return temp_path\r\n    \r\n    def _command_processing_loop(self):\r\n        """Process commands from the queue"""\r\n        while self.is_listening or not self.command_queue.empty():\r\n            try:\r\n                audio_file = self.command_queue.get(timeout=1.0)\r\n                \r\n                # Transcribe audio\r\n                transcription = self._transcribe_audio(audio_file)\r\n                \r\n                if transcription and transcription.strip():\r\n                    rospy.loginfo(f"Transcribed: {transcription}")\r\n                    \r\n                    # Publish transcription\r\n                    transcript_msg = String()\r\n                    transcript_msg.data = transcription\r\n                    self.voice_transcript_pub.publish(transcript_msg)\r\n                    \r\n                    # Parse and validate command\r\n                    parsed_command = self._parse_and_validate_command(transcription)\r\n                    \r\n                    if parsed_command:\r\n                        # Add to safety queue for validation\r\n                        self.safety_queue.put(parsed_command)\r\n                        \r\n                        # Publish parsed command\r\n                        cmd_msg = String()\r\n                        cmd_msg.data = json.dumps(parsed_command)\r\n                        self.parsed_command_pub.publish(cmd_msg)\r\n                \r\n                # Clean up temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command: {e}")\r\n    \r\n    def _safety_monitoring_loop(self):\r\n        """Monitor safety and execute validated commands"""\r\n        rate = rospy.Rate(20)  # 20 Hz\r\n        \r\n        while self.is_listening or not self.safety_queue.empty():\r\n            try:\r\n                # Check for commands to validate and execute\r\n                try:\r\n                    cmd = self.safety_queue.get_nowait()\r\n                    \r\n                    # Validate command for safety\r\n                    if self._is_command_safe(cmd):\r\n                        # Execute command\r\n                        self._execute_command(cmd)\r\n                        rospy.loginfo(f"Executed safe command: {cmd[\'action\']}")\r\n                    else:\r\n                        rospy.logwarn(f"Unsafe command blocked: {cmd[\'action\']}")\r\n                        self._publish_safety_status(f"Command blocked: {cmd[\'action\']}")\r\n                        \r\n                except queue.Empty:\r\n                    pass  # No commands to process\r\n                \r\n                rate.sleep()\r\n                \r\n            except Exception as e:\r\n                rospy.logerr(f"Error in safety monitoring: {e}")\r\n    \r\n    def _transcribe_audio(self, audio_file_path) > str:\r\n        """Transcribe audio using Whisper"""\r\n        try:\r\n            result = self.whisper_model.transcribe(audio_file_path, language="english")\r\n            return result[\'text\'].strip()\r\n        except Exception as e:\r\n            rospy.logerr(f"Transcription error: {e}")\r\n            return ""\r\n    \r\n    def _parse_and_validate_command(self, transcription: str) > Optional[Dict]:\r\n        """Parse transcription into command with context"""\r\n        if not transcription:\r\n            return None\r\n        \r\n        # Get environmental context\r\n        context = self._get_environmental_context()\r\n        \r\n        # Use OpenAI for advanced command parsing if available\r\n        if self.openai_client:\r\n            return self._parse_with_openai(transcription, context)\r\n        else:\r\n            # Use basic parsing\r\n            return self._parse_basic_command(transcription)\r\n    \r\n    def _get_environmental_context(self) > Dict:\r\n        """Get environmental context for command validation"""\r\n        context = {\r\n            "robot_state": {\r\n                "has_laser_data": self.laser_data is not None,\r\n                "has_imu_data": self.imu_data is not None\r\n            }\r\n        }\r\n        \r\n        # Add laserbased context if available\r\n        if self.laser_data:\r\n            # Check front, left, right sectors\r\n            n_ranges = len(self.laser_data.ranges)\r\n            front_ranges = self.laser_data.ranges[n_ranges//2  30:n_ranges//2 + 30]\r\n            left_ranges = self.laser_data.ranges[:n_ranges//4]\r\n            right_ranges = self.laser_data.ranges[3*n_ranges//4:]\r\n            \r\n            # Filter valid ranges\r\n            valid_front = [r for r in front_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            valid_left = [r for r in left_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            valid_right = [r for r in right_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            context["surroundings"] = {\r\n                "front_clear_m": min(valid_front) if valid_front else float(\'inf\'),\r\n                "left_clear_m": min(valid_left) if valid_left else float(\'inf\'),\r\n                "right_clear_m": min(valid_right) if valid_right else float(\'inf\')\r\n            }\r\n        \r\n        return context\r\n    \r\n    def _parse_with_openai(self, transcription: str, context: Dict) > Optional[Dict]:\r\n        """Use OpenAI to parse command with context"""\r\n        try:\r\n            prompt = f"""\r\n            Transcription: "{transcription}"\r\n            \r\n            Environment Context: {json.dumps(context, indent=2)}\r\n            \r\n            Parse this natural language command into a robot action.\r\n            Consider the environmental context when interpreting the command.\r\n            If the command seems unsafe given the environmental context, mark it as unsafe.\r\n            \r\n            Return only a JSON object in this format:\r\n            {{\r\n              "action": "move_forward|move_backward|turn_left|turn_right|stop|pick_object|place_object|other",\r\n              "parameters": {{\r\n                "linear_speed": float,\r\n                "angular_speed": float,\r\n                "distance": float,\r\n                "angle": float,\r\n                "object_name": str\r\n              }},\r\n              "confidence": float,\r\n              "is_safe": boolean,\r\n              "reasoning": "brief explanation"\r\n            }}\r\n            """\r\n            \r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4o",\r\n                messages=[\r\n                    {\r\n                        "role": "system", \r\n                        "content": "You are a robotics command parser. Parse natural language commands into robot actions. Consider environmental context for safety."\r\n                    },\r\n                    {\r\n                        "role": "user", \r\n                        "content": prompt\r\n                    }\r\n                ],\r\n                temperature=0.1,\r\n                max_tokens=500\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON from response if wrapped in code blocks\r\n            if response_text.startswith(\'```\'):\r\n                start_idx = response_text.find(\'{\')\r\n                end_idx = response_text.rfind(\'}\') + 1\r\n                if start_idx != 1 and end_idx != 1:\r\n                    response_text = response_text[start_idx:end_idx]\r\n            \r\n            command_data = json.loads(response_text)\r\n            return command_data\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error with OpenAI command parsing: {e}")\r\n            # Fall back to basic parsing\r\n            return self._parse_basic_command(transcription)\r\n    \r\n    def _parse_basic_command(self, transcription: str) > Optional[Dict]:\r\n        """Basic regularexpressionbased command parsing"""\r\n        text_lower = transcription.lower()\r\n        \r\n        # Navigation commands\r\n        if any(word in text_lower for word in [\'forward\', \'ahead\', \'straight\', \'go\']):\r\n            return {\r\n                "action": "move_forward",\r\n                "parameters": {"linear_speed": 0.3, "distance": 1.0},\r\n                "confidence": 0.7,\r\n                "is_safe": True,\r\n                "reasoning": "Basic forward movement command"\r\n            }\r\n        elif any(word in text_lower for word in [\'backward\', \'back\', \'reverse\']):\r\n            return {\r\n                "action": "move_backward", \r\n                "parameters": {"linear_speed": 0.3, "distance": 1.0},\r\n                "confidence": 0.7,\r\n                "is_safe": True,\r\n                "reasoning": "Basic backward movement command"\r\n            }\r\n        elif any(word in text_lower for word in [\'left\', \'port\']):\r\n            return {\r\n                "action": "turn_left",\r\n                "parameters": {"angular_speed": 0.5, "angle": 90.0},\r\n                "confidence": 0.7,\r\n                "is_safe": True,\r\n                "reasoning": "Basic left turn command"\r\n            }\r\n        elif any(word in text_lower for word in [\'right\', \'starboard\']):\r\n            return {\r\n                "action": "turn_right",\r\n                "parameters": {"angular_speed": 0.5, "angle": 90.0},\r\n                "confidence": 0.7,\r\n                "is_safe": True,\r\n                "reasoning": "Basic right turn command"\r\n            }\r\n        elif any(word in text_lower for word in [\'stop\', \'halt\', \'pause\']):\r\n            return {\r\n                "action": "stop",\r\n                "parameters": {},\r\n                "confidence": 0.9,\r\n                "is_safe": True,\r\n                "reasoning": "Stop command is always safe"\r\n            }\r\n        else:\r\n            return {\r\n                "action": "unknown",\r\n                "parameters": {},\r\n                "confidence": 0.3,\r\n                "is_safe": False,\r\n                "reasoning": "Unknown command"\r\n            }\r\n    \r\n    def _is_command_safe(self, command: Dict) > bool:\r\n        """Validate if command is safe to execute in current environment"""\r\n        if not self.laser_data:\r\n            # If no sensor data, be conservative\r\n            action = command.get(\'action\', \'unknown\')\r\n            if action in [\'move_forward\', \'move_backward\', \'move_distance\']:\r\n                return command.get(\'is_safe\', False)  # Trust parsed safety flag\r\n            else:\r\n                return True  # Other commands are generally safe\r\n        \r\n        # Check for movement commands with obstacle consideration\r\n        action = command.get(\'action\')\r\n        if action in [\'move_forward\', \'move_distance\'] and command.get(\'parameters\', {}).get(\'linear_speed\', 0) > 0:\r\n            # Check front for obstacles\r\n            n_ranges = len(self.laser_data.ranges)\r\n            front_sector = self.laser_data.ranges[n_ranges//2  30 : n_ranges//2 + 30]\r\n            valid_ranges = [r for r in front_sector if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            if valid_ranges and min(valid_ranges) < self.safety_distance:\r\n                rospy.logwarn(f"Obstacle detected ahead ({min(valid_ranges):.2f}m), command unsafe")\r\n                return False\r\n        \r\n        elif action == \'move_backward\' and command.get(\'parameters\', {}).get(\'linear_speed\', 0) < 0:\r\n            # Check rear for obstacles (simplified  check back 30 degrees)\r\n            n_ranges = len(self.laser_data.ranges)\r\n            back_sector = self.laser_data.ranges[3*n_ranges//4  15 : n_ranges] + self.laser_data.ranges[0 : n_ranges//4 + 15]\r\n            valid_ranges = [r for r in back_sector if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            if valid_ranges and min(valid_ranges) < self.safety_distance:\r\n                rospy.logwarn(f"Obstacle detected behind ({min(valid_ranges):.2f}m), command unsafe")\r\n                return False\r\n        \r\n        # Check if command has been marked unsafe by parser\r\n        if not command.get(\'is_safe\', True):\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def _execute_command(self, command: Dict):\r\n        """Execute validated command"""\r\n        action = command.get(\'action\', \'unknown\')\r\n        params = command.get(\'parameters\', {})\r\n        \r\n        twist_cmd = Twist()\r\n        \r\n        if action == \'move_forward\':\r\n            linear_speed = params.get(\'linear_speed\', 0.3)\r\n            twist_cmd.linear.x = min(linear_speed, self.max_linear_speed)\r\n        elif action == \'move_backward\':\r\n            linear_speed = params.get(\'linear_speed\', 0.3)\r\n            twist_cmd.linear.x = min(linear_speed, self.max_linear_speed)\r\n        elif action == \'turn_left\':\r\n            angular_speed = params.get(\'angular_speed\', 0.5)\r\n            twist_cmd.angular.z = min(angular_speed, self.max_angular_speed)\r\n        elif action == \'turn_right\':\r\n            angular_speed = params.get(\'angular_speed\', 0.5)\r\n            twist_cmd.angular.z = min(angular_speed, self.max_angular_speed)\r\n        elif action == \'stop\':\r\n            # Twist is already zeroed\r\n            pass\r\n        else:\r\n            # For unknown actions, just stop\r\n            pass\r\n        \r\n        # Apply safety limits\r\n        twist_cmd.linear.x = max(self.max_linear_speed, min(self.max_linear_speed, twist_cmd.linear.x))\r\n        twist_cmd.angular.z = max(self.max_angular_speed, min(self.max_angular_speed, twist_cmd.angular.z))\r\n        \r\n        # Publish command\r\n        self.cmd_vel_pub.publish(twist_cmd)\r\n    \r\n    def _publish_status(self, status: str):\r\n        """Publish system status"""\r\n        status_msg = String()\r\n        status_msg.data = json.dumps({\r\n            "status": status,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.system_status_pub.publish(status_msg)\r\n    \r\n    def _publish_safety_status(self, status: str):\r\n        """Publish safety status"""\r\n        safety_msg = String()\r\n        safety_msg.data = json.dumps({\r\n            "status": status,\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "emergency_active": self.emergency_stop_active\r\n        })\r\n        self.safety_status_pub.publish(safety_msg)\r\n    \r\n    def run(self):\r\n        """Run the complete system"""\r\n        self.start_system()\r\n        \r\n        try:\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down Complete Voice Control System...")\r\n        finally:\r\n            self.stop_system()\r\n\r\ndef main():\r\n    # Get OpenAI API key from parameter or user input\r\n    api_key = rospy.get_param(\'~openai_api_key\', \'\')\r\n    if not api_key:\r\n        api_key = input("Enter OpenAI API key (or press Enter to skip): ").strip()\r\n    \r\n    system = CompleteVoiceControlSystem(api_key if api_key else None)\r\n    system.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"launch-file-for-the-complete-system",children:"Launch file for the complete system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<launch>\r\n  <! Complete Voice Control System >\r\n  <node name="complete_voice_control_system" pkg="robot_voice_control" type="complete_voice_control_system.py" output="screen">\r\n    <param name="openai_api_key" value="" />\r\n    <param name="safety_distance" value="0.5" />\r\n    <param name="max_linear_speed" value="0.4" />\r\n    <param name="max_angular_speed" value="0.6" />\r\n  </node>\r\n  \r\n  <! Example sensor nodes (needed for safety validation) >\r\n  <node name="fake_laser_scan" pkg="topic_tools" type="relay" args="/scan /fake_scan" />\r\n  <node name="fake_imu" pkg="topic_tools" type="relay" args="/imu/data /fake_imu" />\r\n  \r\n  <! TF broadcasters for coordinate systems >\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0 0 0 0 0 0 1 base_link laser_frame" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0 0 0 0 0 0 1 base_link imu_link" />\r\n</launch>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,a.jsx)(n.p,{children:"Create a complete voiceactivated robot system that:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implements Whisperbased speech recognition for robot commands"}),"\n",(0,a.jsx)(n.li,{children:"Integrates with ROS navigation stack for voiceguided navigation"}),"\n",(0,a.jsx)(n.li,{children:"Creates a natural language interface for robot manipulation tasks"}),"\n",(0,a.jsx)(n.li,{children:"Implements safety validation for all voice commands"}),"\n",(0,a.jsx)(n.li,{children:"Develops contextaware command interpretation using LLMs"}),"\n",(0,a.jsx)(n.li,{children:"Creates multimodal feedback system (voice + visual)"}),"\n",(0,a.jsx)(n.li,{children:"Evaluates system performance with various speakers and accents"}),"\n",(0,a.jsx)(n.li,{children:"Implements fallback mechanisms for misunderstood commands"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Your project should include:\r\nComplete Whisper integration with audio preprocessing\r\nNatural language command parsing and validation\r\nSafety system to prevent dangerous robot movements\r\nContextaware interpretation using environmental sensors\r\nVoice and visual feedback mechanisms\r\nPerformance evaluation metrics\r\nDemo scenarios with various natural language commands"}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered voicetoaction systems using OpenAI Whisper for robotics:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Whisper Integration"}),": Using OpenAI's Whisper model for speech recognition in robotics\r\n",(0,a.jsx)(n.strong,{children:"Audio Processing"}),": Techniques for realtime audio processing and voice activity detection",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Command Parsing"}),": Converting natural language to robot commands\r\n",(0,a.jsx)(n.strong,{children:"Safety Validation"}),": Ensuring voice commands are safe before execution\r\n",(0,a.jsx)(n.strong,{children:"Environmental Context"}),": Using sensor data to validate command appropriateness\r\n",(0,a.jsx)(n.strong,{children:"Multimodal Integration"}),": Combining voice with other sensory inputs for robust operation\r\n",(0,a.jsx)(n.strong,{children:"Realtime Processing"}),": Optimizing for realtime robotic interaction"]}),"\n",(0,a.jsx)(n.p,{children:"Voice command systems enable more natural humanrobot interaction, allowing users to command robots using everyday language rather than specialized interfaces. However, special attention must be paid to safety, validation, and environmental context to ensure reliable operation in dynamic environments."})]})}function f(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);