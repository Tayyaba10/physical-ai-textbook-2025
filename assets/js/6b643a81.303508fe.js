"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[4944],{5498:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>f,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vision-language-action/ch17-voice-to-action-whisper","title":"ch17-voice-to-action-whisper","description":"-----","source":"@site/docs/module-4-vision-language-action/ch17-voice-to-action-whisper.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch17-voice-to-action-whisper","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch17-voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch17-voice-to-action-whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch16-llms-meet-robotics","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch16-llms-meet-robotics"},"next":{"title":"ch18-cognitive-task-planning-gpt4o","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch18-cognitive-task-planning-gpt4o"}}');var o=r(4848),i=r(8453),s=r(7242);const a={},c=void 0,l={},d=[{value:"title: Ch17  VoicetoAction with OpenAI Whisper\r\nmodule: 4\r\nchapter: 17\r\nsidebar_label: Ch17: VoicetoAction with OpenAI Whisper\r\ndescription: Implementing voice command processing with OpenAI Whisper for robotics applications\r\ntags: [whisper, speechrecognition, voicecontrol, robotics, naturallanguage, audioprocessing]\r\ndifficulty: advanced\r\nestimated_duration: 120",id:"title-ch17--voicetoaction-with-openai-whispermodule-4chapter-17sidebar_label-ch17-voicetoaction-with-openai-whisperdescription-implementing-voice-command-processing-with-openai-whisper-for-robotics-applicationstags-whisper-speechrecognition-voicecontrol-robotics-naturallanguage-audioprocessingdifficulty-advancedestimated_duration-120",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"OpenAI Whisper for Robotics",id:"openai-whisper-for-robotics",level:3},{value:"Challenges in Robotics Audio Processing",id:"challenges-in-robotics-audio-processing",level:3},{value:"Audio Preprocessing for Robotics",id:"audio-preprocessing-for-robotics",level:3},{value:"Voice Command Context Processing",id:"voice-command-context-processing",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up Whisper for Robotics",id:"lab-1-setting-up-whisper-for-robotics",level:3},{value:"Lab 2: Advanced Voice Command Processing",id:"lab-2-advanced-voice-command-processing",level:3},{value:"Lab 3: Integrating with ROS and Robot Control",id:"lab-3-integrating-with-ros-and-robot-control",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Launch file for the system (<code>voice_control_system.launch</code>):",id:"launch-file-for-the-system-voice_control_systemlaunch",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"title-ch17--voicetoaction-with-openai-whispermodule-4chapter-17sidebar_label-ch17-voicetoaction-with-openai-whisperdescription-implementing-voice-command-processing-with-openai-whisper-for-robotics-applicationstags-whisper-speechrecognition-voicecontrol-robotics-naturallanguage-audioprocessingdifficulty-advancedestimated_duration-120",children:"title: Ch17  VoicetoAction with OpenAI Whisper\r\nmodule: 4\r\nchapter: 17\r\nsidebar_label: Ch17: VoicetoAction with OpenAI Whisper\r\ndescription: Implementing voice command processing with OpenAI Whisper for robotics applications\r\ntags: [whisper, speechrecognition, voicecontrol, robotics, naturallanguage, audioprocessing]\r\ndifficulty: advanced\r\nestimated_duration: 120"}),"\n","\n",(0,o.jsx)(n.h1,{id:"voicetoaction-with-openai-whisper",children:"VoicetoAction with OpenAI Whisper"}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"Implement speech recognition using OpenAI Whisper in robotics applications\r\nIntegrate voice commands with robot control systems\r\nProcess realtime audio for continuous interaction\r\nDesign voice command grammars for robotic tasks\r\nImplement audio preprocessing for robotics environment conditions\r\nHandle voice command ambiguities and context\r\nCreate multimodal feedback systems for voice interactions\r\nEvaluate speech recognition performance in robotics contexts"}),"\n",(0,o.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(n.h3,{id:"openai-whisper-for-robotics",children:"OpenAI Whisper for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is a robust speech recognition model that performs well in various conditions. For robotics applications, Whisper offers:"}),"\n",(0,o.jsx)(s.A,{chart:"\ngraph TD;\n  A[Audio Input] > B[Preprocessing];\n  B > C[Whisper ASR];\n  C > D[SpeechtoText];\n  D > E[NLP Processing];\n  E > F[Command Interpretation];\n  F > G[Robot Action];\n  \n  H[Robot Feedback] > I[Audio Feedback];\n  H > J[Visual Feedback];\n  I > A;\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style G fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style C fill:#FF9800,stroke:#E65100,color:#fff;\n"}),"\n",(0,o.jsx)(n.h3,{id:"challenges-in-robotics-audio-processing",children:"Challenges in Robotics Audio Processing"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Acoustic Environment"}),": Robots operate in noisy environments where background sounds, motor noise, and other robots can interfere with speech recognition."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"RealTime Requirements"}),": Robot systems often require immediate responses to user commands."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vocabulary Constraints"}),": Robot commands are typically limited to specific actions and objects in the environment."]}),"\n",(0,o.jsx)(n.h3,{id:"audio-preprocessing-for-robotics",children:"Audio Preprocessing for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Preprocessing is critical for effective speech recognition in robotics environments:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Noise Reduction"}),": Filtering ambient noise\r\n",(0,o.jsx)(n.strong,{children:"Audio Enhancement"}),": Improving signaltonoise ratio\r\n",(0,o.jsx)(n.strong,{children:"Voice Activity Detection"}),": Identifying when speech is present\r\n",(0,o.jsx)(n.strong,{children:"Echo Cancellation"}),": Managing audio feedback in robot systems"]}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-context-processing",children:"Voice Command Context Processing"}),"\n",(0,o.jsx)(n.p,{children:"Robotics applications often require contextual understanding where commands depend on current robot state, environment, and previous interactions."}),"\n",(0,o.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,o.jsx)(n.h3,{id:"lab-1-setting-up-whisper-for-robotics",children:"Lab 1: Setting up Whisper for Robotics"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Install Whisper and related dependencies"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openaiwhisper torch torchaudio\r\npip install pyaudio sounddevice librosa numpy\r\npip install SpeechRecognition\r\npip install ros2 rospy\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Create a basic Whisper interface for robotics"})," (",(0,o.jsx)(n.code,{children:"whisper_robot_interface.py"}),"):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport whisper\r\nimport torch\r\nimport pyaudio\r\nimport wave\r\nimport numpy as np\r\nimport rospy\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom typing import Optional, Callable, Dict\r\nimport tempfile\r\nimport os\r\n\r\nclass WhisperRobotInterface:\r\n    def __init__(self, model_size="base", device="cuda" if torch.cuda.is_available() else "cpu"):\r\n        # Initialize Whisper model\r\n        print(f"Loading Whisper model \'{model_size}\' on {device}...")\r\n        self.model = whisper.load_model(model_size).to(device)\r\n        self.device = device\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000  # Sample rate (Whisper expects 16kHz)\r\n        self.chunk = 1024  # Buffer size\r\n        self.format = pyaudio.paInt16  # Audio format\r\n        self.channels = 1  # Mono\r\n        \r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n        \r\n        # Command queue for processing\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Robot control\r\n        rospy.init_node(\'whisper_robot_interface\', anonymous=True)\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        \r\n        # Publishers for results\r\n        self.transcript_pub = rospy.Publisher(\'/voice_transcript\', String, queue_size=10)\r\n        self.command_pub = rospy.Publisher(\'/voice_command\', String, queue_size=10)\r\n        \r\n        # State variables\r\n        self.is_listening = False\r\n        self.recording_thread = None\r\n        self.processing_thread = None\r\n        self.voice_activity_threshold = 0.01\r\n        self.min_silence_duration = 1.0  # seconds of silence to end recording\r\n        \r\n        print("Whisper Robot Interface initialized")\r\n    \r\n    def start_listening(self):\r\n        """Start listening for voice commands"""\r\n        if self.is_listening:\r\n            print("Already listening")\r\n            return\r\n        \r\n        self.is_listening = True\r\n        \r\n        # Start audio recording thread\r\n        self.recording_thread = threading.Thread(target=self._record_audio_continuously)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n        \r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._process_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        rospy.loginfo("Started listening for voice commands")\r\n    \r\n    def stop_listening(self):\r\n        """Stop listening for voice commands"""\r\n        self.is_listening = False\r\n        \r\n        if self.recording_thread is not None:\r\n            self.recording_thread.join(timeout=2.0)\r\n        \r\n        if self.processing_thread is not None:\r\n            self.processing_thread.join(timeout=2.0)\r\n        \r\n        rospy.loginfo("Stopped listening for voice commands")\r\n    \r\n    def _is_voice_active(self, audio_data):\r\n        """Detect if voice is active in audio chunk"""\r\n        # Convert to numpy array\r\n        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)\r\n        \r\n        # Calculate RMS amplitude\r\n        rms = np.sqrt(np.mean(audio_array ** 2))\r\n        \r\n        return rms > self.voice_activity_threshold\r\n    \r\n    def _record_audio_chunk(self, seconds=1.0):\r\n        """Record a single chunk of audio"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        frames = []\r\n        chunks = int(self.rate / self.chunk * seconds)\r\n        \r\n        for _ in range(chunks):\r\n            data = stream.read(self.chunk)\r\n            frames.append(data)\r\n        \r\n        stream.stop_stream()\r\n        stream.close()\r\n        \r\n        return b\'\'.join(frames)\r\n    \r\n    def _record_audio_continuously(self):\r\n        """Continuously record audio and detect voice activity"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        silence_counter = 0\r\n        frames = []\r\n        silence_chunks = int(self.rate / self.chunk * 0.1)  # 0.1s chunks for silence detection\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                if self._is_voice_active(data):\r\n                    if not recording:\r\n                        # Start recording\r\n                        rospy.loginfo("Voice activity detected, starting recording...")\r\n                        recording = True\r\n                        frames = [data]\r\n                        silence_counter = 0\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(data)\r\n                        silence_counter = 0\r\n                else:\r\n                    if recording:\r\n                        # Add to silence counter\r\n                        silence_counter += 1\r\n                        \r\n                        # Append to frames anyway (might be trailing speech)\r\n                        frames.append(data)\r\n                        \r\n                        # Check if silence duration exceeds threshold\r\n                        if silence_counter > int(self.min_silence_duration * self.rate / self.chunk):\r\n                            # End of speech detected\r\n                            rospy.loginfo(f"End of speech detected, recorded {len(frames)} frames")\r\n                            \r\n                            # Save recorded audio to temporary file\r\n                            temp_filename = self._save_audio_to_temp_file(frames)\r\n                            \r\n                            # Add to processing queue\r\n                            self.command_queue.put(temp_filename)\r\n                            \r\n                            # Reset for next recording\r\n                            recording = False\r\n                            frames = []\r\n                            silence_counter = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in audio recording: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _save_audio_to_temp_file(self, frames):\r\n        """Save audio frames to a temporary WAV file"""\r\n        # Create temporary WAV file\r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        temp_file = wave.open(temp_path, \'wb\')\r\n        temp_file.setnchannels(self.channels)\r\n        temp_file.setsampwidth(self.audio.get_sample_size(self.format))\r\n        temp_file.setframerate(self.rate)\r\n        temp_file.writeframes(b\'\'.join(frames))\r\n        temp_file.close()\r\n        os.close(temp_fd)\r\n        \r\n        return temp_path\r\n    \r\n    def _process_commands(self):\r\n        """Process audio files in the queue with Whisper"""\r\n        while self.is_listening or not self.command_queue.empty():\r\n            try:\r\n                # Get audio file from queue\r\n                audio_file = self.command_queue.get(timeout=1.0)\r\n                \r\n                # Process with Whisper\r\n                result = self._transcribe_audio(audio_file)\r\n                \r\n                if result and result.strip():\r\n                    rospy.loginfo(f"Transcribed: {result}")\r\n                    \r\n                    # Publish transcript\r\n                    transcript_msg = String()\r\n                    transcript_msg.data = result\r\n                    self.transcript_pub.publish(transcript_msg)\r\n                    \r\n                    # Process command\r\n                    self._execute_command(result)\r\n                else:\r\n                    rospy.loginfo("No speech detected or transcription failed")\r\n                \r\n                # Clean up temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command: {e}")\r\n    \r\n    def _transcribe_audio(self, audio_file_path):\r\n        """Transcribe audio using Whisper"""\r\n        try:\r\n            result = self.model.transcribe(\r\n                audio_file_path,\r\n                language="english",\r\n                fp16=False  # Set to True if using CUDA for faster inference\r\n            )\r\n            return result[\'text\'].strip()\r\n        except Exception as e:\r\n            rospy.logerr(f"Transcription error: {e}")\r\n            return ""\r\n    \r\n    def _execute_command(self, transcription):\r\n        """Execute command based on transcription"""\r\n        # Publish raw command\r\n        cmd_msg = String()\r\n        cmd_msg.data = transcription\r\n        self.command_pub.publish(cmd_msg)\r\n        \r\n        # Simple command parsing (in real system, use NLP/RAG)\r\n        command = self._parse_voice_command(transcription)\r\n        \r\n        if command:\r\n            rospy.loginfo(f"Executing command: {command}")\r\n            self._send_robot_command(command)\r\n        else:\r\n            rospy.logwarn(f"Unrecognized command: {transcription}")\r\n    \r\n    def _parse_voice_command(self, text):\r\n        """Parse voice command into robot action"""\r\n        text = text.lower()\r\n        \r\n        # Define simple command mappings\r\n        commands = {\r\n            "move forward": {"action": "move", "linear": 0.5, "angular": 0.0},\r\n            "move backward": {"action": "move", "linear": 0.5, "angular": 0.0},\r\n            "turn left": {"action": "turn", "linear": 0.0, "angular": 0.5},\r\n            "turn right": {"action": "turn", "linear": 0.0, "angular": 0.5},\r\n            "stop": {"action": "stop", "linear": 0.0, "angular": 0.0},\r\n            "go straight": {"action": "move", "linear": 0.3, "angular": 0.0},\r\n            "halt": {"action": "stop", "linear": 0.0, "angular": 0.0}\r\n        }\r\n        \r\n        # Find best matching command\r\n        for cmd_text, cmd_def in commands.items():\r\n            if cmd_text in text:\r\n                return cmd_def\r\n        \r\n        # Check for other variants using simple fuzzy matching\r\n        if "forward" in text or "ahead" in text:\r\n            return {"action": "move", "linear": 0.3, "angular": 0.0}\r\n        elif "back" in text or "reverse" in text:\r\n            return {"action": "move", "linear": 0.3, "angular": 0.0}\r\n        elif "left" in text:\r\n            return {"action": "turn", "linear": 0.0, "angular": 0.3}\r\n        elif "right" in text:\r\n            return {"action": "turn", "linear": 0.0, "angular": 0.3}\r\n        elif "stop" in text or "halt" in text:\r\n            return {"action": "stop", "linear": 0.0, "angular": 0.0}\r\n        \r\n        return None\r\n    \r\n    def _send_robot_command(self, command):\r\n        """Send command to robot"""\r\n        twist = Twist()\r\n        \r\n        if command["action"] == "move" or command["action"] == "turn":\r\n            twist.linear.x = command["linear"]\r\n            twist.angular.z = command["angular"]\r\n        elif command["action"] == "stop":\r\n            twist.linear.x = 0.0\r\n            twist.angular.z = 0.0\r\n        \r\n        self.cmd_vel_pub.publish(twist)\r\n    \r\n    def test_transcription(self, audio_file_path):\r\n        """Test transcription on a specific audio file"""\r\n        result = self._transcribe_audio(audio_file_path)\r\n        rospy.loginfo(f"Test transcription: {result}")\r\n        return result\r\n    \r\n    def __del__(self):\r\n        """Cleanup audio resources"""\r\n        if hasattr(self, \'audio\'):\r\n            self.audio.terminate()\r\n\r\n# Example usage\r\nif __name__ == "__main__":\r\n    # Initialize interface (you\'ll need to provide your OpenAI API key for certain features)\r\n    interface = WhisperRobotInterface(model_size="base")\r\n    \r\n    try:\r\n        interface.start_listening()\r\n        rospy.spin()  # Keep node alive\r\n    except KeyboardInterrupt:\r\n        interface.stop_listening()\r\n        print("Shutting down...")\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"lab-2-advanced-voice-command-processing",children:"Lab 2: Advanced Voice Command Processing"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create a more sophisticated voice command processor"})," (",(0,o.jsx)(n.code,{children:"advanced_voice_processor.py"}),"):","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport whisper\r\nimport torch\r\nimport pyaudio\r\nimport numpy as np\r\nimport librosa\r\nimport rospy\r\nimport threading\r\nimport queue\r\nimport json\r\nimport re\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist, Point\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom typing import Dict, List, Optional, Tuple\r\nfrom dataclasses import dataclass\r\nimport tempfile\r\nimport os\r\n\r\n@dataclass\r\nclass VoiceCommand:\r\n    action: str\r\n    parameters: Dict\r\n    confidence: float\r\n    original_text: str\r\n\r\nclass AdvancedVoiceProcessor:\r\n    def __init__(self, model_size="base", device="cuda" if torch.cuda.is_available() else "cpu"):\r\n        # Initialize Whisper model\r\n        self.model = whisper.load_model(model_size).to(device)\r\n        self.device = device\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        \r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n        \r\n        # ROS initialization\r\n        rospy.init_node(\'advanced_voice_processor\', anonymous=True)\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.voice_cmd_pub = rospy.Publisher(\'/parsed_voice_command\', String, queue_size=10)\r\n        self.transcript_pub = rospy.Publisher(\'/voice_transcript\', String, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/voice_control_status\', String, queue_size=10)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        \r\n        # State management\r\n        self.laser_scan = None\r\n        self.robot_position = Point(x=0.0, y=0.0, z=0.0)\r\n        self.robot_heading = 0.0\r\n        \r\n        # Command queue\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Voice activity detection\r\n        self.voice_activity_threshold = 0.005\r\n        self.min_voice_duration = 0.5  # seconds\r\n        self.min_silence_duration = 1.0\r\n        \r\n        # Thread management\r\n        self.is_listening = False\r\n        self.recording_thread = None\r\n        self.processing_thread = None\r\n        \r\n        # Command vocabularies\r\n        self.navigation_commands = [\r\n            "move forward", "move backward", "go forward", "go back",\r\n            "go straight", "move straight", "turn left", "turn right",\r\n            "pivot left", "pivot right", "rotate left", "rotate right", \r\n            "stop", "halt", "wait", "go to", "navigate to", "move to",\r\n            "approach", "come here", "move closer", "go away"\r\n        ]\r\n        \r\n        self.object_commands = [\r\n            "pick up", "grasp", "grab", "take", "lift", "drop",\r\n            "put down", "release", "place", "move", "bring", "fetch"\r\n        ]\r\n        \r\n        # Contextaware command processor\r\n        self.context_memory = []\r\n        self.max_context_items = 50\r\n        \r\n        print("Advanced Voice Processor initialized")\r\n    \r\n    def laser_callback(self, msg):\r\n        """Update laser scan data"""\r\n        self.laser_scan = msg\r\n    \r\n    def update_robot_state(self, position: Point, heading: float):\r\n        """Update robot state for contextual processing"""\r\n        self.robot_position = position\r\n        self.robot_heading = heading\r\n    \r\n    def start_listening(self):\r\n        """Start voice command processing"""\r\n        if self.is_listening:\r\n            return\r\n        \r\n        self.is_listening = True\r\n        \r\n        # Start threads\r\n        self.recording_thread = threading.Thread(target=self._continuous_recording)\r\n        self.recording_thread.daemon = True\r\n        self.recording_thread.start()\r\n        \r\n        self.processing_thread = threading.Thread(target=self._process_audio_queue)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        self.status_pub.publish(String(data="Voice control active"))\r\n        rospy.loginfo("Advanced voice processor started")\r\n    \r\n    def stop_listening(self):\r\n        """Stop voice command processing"""\r\n        self.is_listening = False\r\n        self.status_pub.publish(String(data="Voice control inactive"))\r\n    \r\n    def _detect_voice_activity(self, audio_chunk):\r\n        """Detect voice activity in audio chunk"""\r\n        # Convert to numpy\r\n        audio_np = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0\r\n        \r\n        # Calculate RMS energy\r\n        rms = np.sqrt(np.mean(audio_np ** 2))\r\n        \r\n        # Use librosa for more sophisticated features if needed\r\n        # spectral_centroids = librosa.feature.spectral_centroid(y=audio_np, sr=self.rate)[0]\r\n        \r\n        return rms > self.voice_activity_threshold\r\n    \r\n    def _continuous_recording(self):\r\n        """Continuously record and detect speech segments"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        frames = []\r\n        voice_active_count = 0\r\n        silence_count = 0\r\n        min_voice_frames = int(self.min_voice_duration * self.rate / self.chunk)\r\n        min_silence_frames = int(self.min_silence_duration * self.rate / self.chunk)\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                chunk = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                if self._detect_voice_activity(chunk):\r\n                    if not recording:\r\n                        # Potential start of speech\r\n                        voice_active_count += 1\r\n                        if voice_active_count >= min_voice_frames:\r\n                            # Confirmed start of speech\r\n                            recording = True\r\n                            frames = [chunk] * min_voice_frames  # Include preroll\r\n                            voice_active_count = 0\r\n                            silence_count = 0\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(chunk)\r\n                        silence_count = 0\r\n                else:\r\n                    if recording:\r\n                        # Accumulate silence\r\n                        frames.append(chunk)  # Add to recording buffer\r\n                        silence_count += 1\r\n                        \r\n                        if silence_count >= min_silence_frames:\r\n                            # End of speech segment\r\n                            if len(frames) >= min_voice_frames:  # Ensure minimum length\r\n                                # Save to temp file\r\n                                temp_file = self._save_recording(frames)\r\n                                self.command_queue.put(temp_file)\r\n                                rospy.loginfo(f"Recorded speech segment ({len(frames)} frames)")\r\n                            \r\n                            # Reset for next segment\r\n                            recording = False\r\n                            frames = []\r\n                            silence_count = 0\r\n                            voice_active_count = 0\r\n                    else:\r\n                        # Reset counters when not recording\r\n                        voice_active_count = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Recording error: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _save_recording(self, frames):\r\n        """Save recorded frames to temporary WAV file"""\r\n        import wave\r\n        \r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        wf = wave.open(temp_path, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        os.close(temp_fd)\r\n        \r\n        return temp_path\r\n    \r\n    def _process_audio_queue(self):\r\n        """Process audio files from the queue"""\r\n        while self.is_listening or not self.command_queue.empty():\r\n            try:\r\n                audio_file = self.command_queue.get(timeout=1.0)\r\n                \r\n                # Transcribe audio\r\n                transcript = self._transcribe_with_retry(audio_file)\r\n                \r\n                if transcript and transcript.strip():\r\n                    rospy.loginfo(f"Transcribed: {transcript}")\r\n                    \r\n                    # Publish transcript\r\n                    self.transcript_pub.publish(String(data=transcript))\r\n                    \r\n                    # Parse and execute command\r\n                    command = self._parse_command(transcript)\r\n                    if command:\r\n                        self._execute_parsed_command(command)\r\n                        self._publish_parsed_command(command)\r\n                        \r\n                        # Update context\r\n                        self._update_context(command)\r\n                    \r\n                # Clean up temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Processing error: {e}")\r\n    \r\n    def _transcribe_with_retry(self, audio_file, max_retries=3):\r\n        """Transcribe audio with retry logic"""\r\n        for attempt in range(max_retries):\r\n            try:\r\n                result = self.model.transcribe(audio_file, language="english")\r\n                return result.get(\'text\', \'\').strip()\r\n            except Exception as e:\r\n                rospy.logwarn(f"Transcription attempt {attempt + 1} failed: {e}")\r\n                if attempt == max_retries  1:\r\n                    return ""  # Return empty string if all retries failed\r\n    \r\n    def _parse_command(self, text: str) > Optional[VoiceCommand]:\r\n        """Parse natural language command using regex and semantic analysis"""\r\n        original_text = text\r\n        text = text.lower().strip()\r\n        \r\n        # Clean text\r\n        text = re.sub(r\'[^\\w\\s]\', \' \', text)\r\n        text = \' \'.join(text.split())  # Remove extra whitespace\r\n        \r\n        # Define command patterns\r\n        patterns = [\r\n            # Movement commands\r\n            (r\'.*\\b(forward|ahead|straight)\\b.*\', self._parse_move_forward),\r\n            (r\'.*\\b(backward|back|reverse|behind)\\b.*\', self._parse_move_backward),\r\n            (r\'.*\\b(left|counterclockwise|port)\\b.*\', self._parse_turn_left),\r\n            (r\'.*\\b(right|clockwise|starboard)\\b.*\', self._parse_turn_right),\r\n            (r\'.*\\b(stop|halt|wait|pause)\\b.*\', self._parse_stop),\r\n            \r\n            # Distancebased movement\r\n            (r\'.*\\b(move|go|travel)\\b.*\\b(\\d+(?:\\.\\d+)?)\\b.*\\b(meter|meters|metre|metres)\\b\', \r\n             self._parse_move_distance),\r\n            \r\n            # Direction and distance\r\n            (r\'.*\\b(turn|rotate|pivot)\\b.*\\b(\\d+(?:\\.\\d+)?)\\b.*\\b(degrees|deg)\\b\', \r\n             self._parse_turn_degrees),\r\n        ]\r\n        \r\n        # Try each pattern\r\n        for pattern, parser_func in patterns:\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                command = parser_func(match, text)\r\n                if command:\r\n                    # Calculate confidence based on match strength and vocabulary\r\n                    confidence = self._calculate_confidence(original_text, command.action)\r\n                    return VoiceCommand(\r\n                        action=command.action,\r\n                        parameters=command.parameters,\r\n                        confidence=confidence,\r\n                        original_text=original_text\r\n                    )\r\n        \r\n        # If no specific pattern matched, try general classification\r\n        return self._classify_general_command(text, original_text)\r\n    \r\n    def _parse_move_forward(self, match, text):\r\n        """Parse forward movement command"""\r\n        return VoiceCommand(\r\n            action="move_forward",\r\n            parameters={"speed": 0.3, "duration": 1.0},\r\n            confidence=0.8,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_move_backward(self, match, text):\r\n        """Parse backward movement command"""\r\n        return VoiceCommand(\r\n            action="move_backward", \r\n            parameters={"speed": 0.3, "duration": 1.0},\r\n            confidence=0.8,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_turn_left(self, match, text):\r\n        """Parse left turn command"""\r\n        return VoiceCommand(\r\n            action="turn_left",\r\n            parameters={"speed": 0.4, "angle": 90.0},\r\n            confidence=0.8,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_turn_right(self, match, text):\r\n        """Parse right turn command"""\r\n        return VoiceCommand(\r\n            action="turn_right", \r\n            parameters={"speed": 0.4, "angle": 90.0},\r\n            confidence=0.8,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_stop(self, match, text):\r\n        """Parse stop command"""\r\n        return VoiceCommand(\r\n            action="stop",\r\n            parameters={"speed": 0.0, "duration": 0.0},\r\n            confidence=0.9,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_move_distance(self, match, text):\r\n        """Parse distancebased movement command"""\r\n        distance = float(match.group(2))\r\n        return VoiceCommand(\r\n            action="move_distance",\r\n            parameters={"distance": distance, "speed": 0.3},\r\n            confidence=0.7,\r\n            original_text=text\r\n        )\r\n    \r\n    def _parse_turn_degrees(self, match, text):\r\n        """Parse degreebased turn command"""\r\n        angle = float(match.group(2))\r\n        return VoiceCommand(\r\n            action="turn_degrees",\r\n            parameters={"angle": angle, "speed": 0.3},\r\n            confidence=0.7,\r\n            original_text=text\r\n        )\r\n    \r\n    def _classify_general_command(self, text, original_text):\r\n        """Classify commands that don\'t match specific patterns"""\r\n        # Check similarity with known commands\r\n        if any(cmd in text for cmd in self.navigation_commands):\r\n            return VoiceCommand(\r\n                action="generic_navigation",\r\n                parameters={"command": text},\r\n                confidence=0.5,\r\n                original_text=original_text\r\n            )\r\n        elif any(cmd in text for cmd in self.object_commands):\r\n            return VoiceCommand(\r\n                action="manipulation_command",\r\n                parameters={"command": text},\r\n                confidence=0.5,\r\n                original_text=original_text\r\n            )\r\n        else:\r\n            return None  # Unknown command\r\n    \r\n    def _calculate_confidence(self, original_text, action):\r\n        """Calculate confidence based on various factors"""\r\n        # Base confidence from pattern match\r\n        base_conf = 0.7\r\n        \r\n        # Boost for clear directional words\r\n        directional_boost = sum(1 for word in [\'forward\', \'backward\', \'left\', \'right\', \'stop\'] \r\n                               if word in original_text.lower())\r\n        \r\n        # Penalize for ambiguous words\r\n        ambiguity_penalty = sum(0.1 for word in [\'maybe\', \'perhaps\', \'kind of\', \'i think\'] \r\n                               if word in original_text.lower())\r\n        \r\n        confidence = min(0.95, base_conf + (directional_boost * 0.05)  ambiguity_penalty)\r\n        return max(0.1, confidence)\r\n    \r\n    def _execute_parsed_command(self, command: VoiceCommand):\r\n        """Execute a parsed voice command"""\r\n        if command.confidence < 0.3:\r\n            rospy.logwarn(f"Command confidence too low ({command.confidence}), ignoring: {command.original_text}")\r\n            return\r\n        \r\n        rospy.loginfo(f"Executing command \'{command.action}\' with confidence {command.confidence:.2f}")\r\n        \r\n        # Handle different command types\r\n        if command.action == "move_forward":\r\n            self._execute_move(0.3, 0.0, command.parameters.get("duration", 2.0))\r\n        elif command.action == "move_backward":\r\n            self._execute_move(0.3, 0.0, command.parameters.get("duration", 2.0))\r\n        elif command.action == "turn_left":\r\n            self._execute_move(0.0, 0.5, command.parameters.get("duration", 1.5))\r\n        elif command.action == "turn_right":\r\n            self._execute_move(0.0, 0.5, command.parameters.get("duration", 1.5))\r\n        elif command.action == "stop":\r\n            self._execute_stop()\r\n        elif command.action == "move_distance":\r\n            # For distancebased movement, we\'d need to track odometry\r\n            # For simplicity, use timebased approximation\r\n            distance = command.parameters["distance"]\r\n            speed = command.parameters["speed"]\r\n            duration = distance / speed\r\n            self._execute_move(speed, 0.0, duration)\r\n        elif command.action == "turn_degrees":\r\n            # For degreebased turns, we\'d need to track robot heading\r\n            # For simplicity, use timebased approximation\r\n            angle = command.parameters["angle"]\r\n            duration = abs(angle) / 90.0 * 1.5  # Assuming 90 degree turn takes 1.5 seconds\r\n            angular_speed = np.sign(angle) * 0.5\r\n            self._execute_move(0.0, angular_speed, duration)\r\n        else:\r\n            rospy.logwarn(f"Unknown command action: {command.action}")\r\n    \r\n    def _execute_move(self, linear_x, angular_z, duration):\r\n        """Execute movement command"""\r\n        twist = Twist()\r\n        twist.linear.x = linear_x\r\n        twist.angular.z = angular_z\r\n        \r\n        # Publish for the duration\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        start_time = rospy.Time.now()\r\n        \r\n        while (rospy.Time.now()  start_time).to_sec() < duration and not rospy.is_shutdown():\r\n            self.cmd_vel_pub.publish(twist)\r\n            rate.sleep()\r\n        \r\n        # Stop robot after movement\r\n        self._execute_stop()\r\n    \r\n    def _execute_stop(self):\r\n        """Stop the robot"""\r\n        twist = Twist()\r\n        twist.linear.x = 0.0\r\n        twist.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(twist)\r\n    \r\n    def _publish_parsed_command(self, command: VoiceCommand):\r\n        """Publish parsed command as JSON"""\r\n        cmd_dict = {\r\n            "action": command.action,\r\n            "parameters": command.parameters,\r\n            "confidence": command.confidence,\r\n            "original_text": command.original_text\r\n        }\r\n        \r\n        self.voice_cmd_pub.publish(String(data=json.dumps(cmd_dict)))\r\n    \r\n    def _update_context(self, command: VoiceCommand):\r\n        """Update context memory with new command"""\r\n        self.context_memory.append({\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "command": command\r\n        })\r\n        \r\n        # Keep only recent items\r\n        if len(self.context_memory) > self.max_context_items:\r\n            self.context_memory = self.context_memory[self.max_context_items:]\r\n    \r\n    def get_context_summary(self):\r\n        """Get a summary of recent context"""\r\n        if not self.context_memory:\r\n            return "No recent commands in context"\r\n        \r\n        recent_commands = [item["command"].original_text for item in self.context_memory[5:]]\r\n        return f"Recent commands: {\', \'.join(recent_commands)}"\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"lab-3-integrating-with-ros-and-robot-control",children:"Lab 3: Integrating with ROS and Robot Control"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create ROS integration for voice control"})," (",(0,o.jsx)(n.code,{children:"ros_voice_integration.py"}),"):","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan\r\nimport json\r\nimport threading\r\nfrom typing import Dict, Any\r\nfrom advanced_voice_processor import AdvancedVoiceProcessor\r\n\r\nclass ROSVoiceController:\r\n    def __init__(self):\r\n        rospy.init_node(\'ros_voice_controller\', anonymous=True)\r\n        \r\n        # Initialize voice processor\r\n        self.voice_processor = AdvancedVoiceProcessor(model_size="base")\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/voice_control_status\', String, queue_size=10)\r\n        self.response_pub = rospy.Publisher(\'/voice_response\', String, queue_size=10)\r\n        \r\n        # Subscribers\r\n        rospy.Subscriber(\'/voice_transcript\', String, self.transcript_callback)\r\n        rospy.Subscriber(\'/parsed_voice_command\', String, self.parsed_command_callback)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\r\n        rospy.Subscriber(\'/toggle_voice_control\', Bool, self.toggle_callback)\r\n        \r\n        # Parameters\r\n        self.voice_control_enabled = rospy.get_param(\'~voice_control_enabled\', True)\r\n        self.safe_distance = rospy.get_param(\'~safe_distance\', 0.5)  # meters\r\n        \r\n        # State\r\n        self.laser_scan = None\r\n        self.obstacle_detected = False\r\n        self.voice_control_active = False\r\n        \r\n        rospy.loginfo("ROS Voice Controller initialized")\r\n    \r\n    def laser_callback(self, msg):\r\n        """Update laser scan and check for obstacles"""\r\n        self.laser_scan = msg\r\n        \r\n        # Check for frontfacing obstacles\r\n        if self.laser_scan:\r\n            # Get frontfacing ranges (\xb130 degrees)\r\n            front_ranges = self.laser_scan.ranges[\r\n                len(self.laser_scan.ranges)//2  30 :\r\n                len(self.laser_scan.ranges)//2 + 30\r\n            ]\r\n            \r\n            # Filter valid ranges\r\n            valid_ranges = [r for r in front_ranges if not (r == float(\'inf\') or r == float(\'NaN\'))]\r\n            \r\n            if valid_ranges:\r\n                min_range = min(valid_ranges)\r\n                self.obstacle_detected = min_range < self.safe_distance\r\n                \r\n                if self.obstacle_detected and self.voice_control_active:\r\n                    # Issue warning if robot is trying to move forward into obstacle\r\n                    self.announce_obstacle(min_range)\r\n    \r\n    def transcript_callback(self, msg):\r\n        """Handle new transcript from voice recognition"""\r\n        rospy.loginfo(f"New transcript: {msg.data}")\r\n        \r\n        # Check if command is related to movement when obstacle is detected\r\n        if self.obstacle_detected:\r\n            text = msg.data.lower()\r\n            if any(word in text for word in [\'forward\', \'ahead\', \'straight\', \'go\']):\r\n                self.warn_about_obstacle(msg.data)\r\n    \r\n    def parsed_command_callback(self, msg):\r\n        """Handle parsed voice command"""\r\n        try:\r\n            cmd_data = json.loads(msg.data)\r\n            action = cmd_data["action"]\r\n            confidence = cmd_data["confidence"]\r\n            original_text = cmd_data["original_text"]\r\n            \r\n            rospy.loginfo(f"Parsed command: {action} (conf: {confidence:.2f})")\r\n            \r\n            if confidence > 0.3:\r\n                if self.voice_control_enabled and self._is_command_safe(action, cmd_data):\r\n                    self.execute_command(action, cmd_data)\r\n                    self.acknowledge_command(original_text)\r\n                else:\r\n                    self.reject_command(original_text, "Command deemed unsafe")\r\n            else:\r\n                self.reject_command(original_text, "Low confidence")\r\n        \r\n        except json.JSONDecodeError as e:\r\n            rospy.logerr(f"JSON decode error: {e}")\r\n    \r\n    def toggle_callback(self, msg):\r\n        """Toggle voice control on/off"""\r\n        if msg.data:\r\n            if not self.voice_control_active:\r\n                self.voice_control_active = True\r\n                self.voice_processor.start_listening()\r\n                self.status_pub.publish(String(data="Voice control activated"))\r\n                self.announce("Voice control activated")\r\n                rospy.loginfo("Voice control activated")\r\n        else:\r\n            if self.voice_control_active:\r\n                self.voice_control_active = False\r\n                self.voice_processor.stop_listening()\r\n                self.status_pub.publish(String(data="Voice control deactivated"))\r\n                self.announce("Voice control deactivated")\r\n                rospy.loginfo("Voice control deactivated")\r\n    \r\n    def _is_command_safe(self, action: str, cmd_data: Dict[str, Any]) > bool:\r\n        """Check if command is safe to execute given current state"""\r\n        # Check for obstacles if command involves forward movement\r\n        if action in ["move_forward", "move_distance"] and self.obstacle_detected:\r\n            rospy.logwarn("Obstacle detected, cancelling forward movement")\r\n            return False\r\n        \r\n        # Check other safety conditions\r\n        # (Add more safety checks as needed)\r\n        \r\n        return True\r\n    \r\n    def execute_command(self, action: str, cmd_data: Dict[str, Any]):\r\n        """Execute the parsed command"""\r\n        twist = Twist()\r\n        \r\n        if action == "move_forward":\r\n            twist.linear.x = 0.3\r\n        elif action == "move_backward":\r\n            twist.linear.x = 0.3\r\n        elif action == "turn_left":\r\n            twist.angular.z = 0.4\r\n        elif action == "turn_right":\r\n            twist.angular.z = 0.4\r\n        elif action == "stop":\r\n            pass  # Already zero\r\n        elif action == "move_distance":\r\n            # This would need odometry feedback for precision\r\n            # For now, just move forward\r\n            twist.linear.x = 0.3\r\n        elif action == "turn_degrees":\r\n            # This would need orientation feedback for precision\r\n            # For now, just turn\r\n            angle = cmd_data["parameters"].get("angle", 90.0)\r\n            twist.angular.z = 0.4 if angle > 0 else 0.4\r\n        \r\n        # Publish command\r\n        self.cmd_vel_pub.publish(twist)\r\n        \r\n        # For commands that need to stop after execution, add a timer\r\n        if action in ["move_distance", "turn_degrees"]:\r\n            duration = cmd_data["parameters"].get("duration", 1.0)\r\n            threading.Timer(duration, self._stop_after_movement).start()\r\n    \r\n    def _stop_after_movement(self):\r\n        """Stop robot after timed movement command"""\r\n        twist = Twist()\r\n        self.cmd_vel_pub.publish(twist)\r\n    \r\n    def announce(self, message: str):\r\n        """Announce a message (placeholder  would use texttospeech in real implementation)"""\r\n        rospy.loginfo(f"Announcement: {message}")\r\n        # In a real implementation, this would use a texttospeech system\r\n    \r\n    def acknowledge_command(self, command_text: str):\r\n        """Acknowledge successful command execution"""\r\n        self.announce(f"Executing: {command_text}")\r\n    \r\n    def reject_command(self, command_text: str, reason: str):\r\n        """Reject command with reason"""\r\n        response = f"Unable to execute \'{command_text}\'. Reason: {reason}."\r\n        self.response_pub.publish(String(data=response))\r\n        self.announce(response)\r\n    \r\n    def announce_obstacle(self, distance: float):\r\n        """Announce obstacle detection"""\r\n        if distance < 0.2:\r\n            alert = "Critical obstacle very close!"\r\n        elif distance < 0.5:\r\n            alert = f"Obstacle detected at {distance:.2f} meters."\r\n        else:\r\n            alert = f"Obstacle ahead at {distance:.2f} meters."\r\n        \r\n        self.announce(alert)\r\n    \r\n    def warn_about_obstacle(self, command_text: str):\r\n        """Warn about obstacle when forward command is issued"""\r\n        warning = f"Obstacle detected. Cannot execute \'{command_text}\' safely."\r\n        self.response_pub.publish(String(data=warning))\r\n        self.announce(warning)\r\n    \r\n    def run(self):\r\n        """Run the voice controller"""\r\n        rospy.loginfo("Starting ROS Voice Controller")\r\n        \r\n        # Start voice processor if initially enabled\r\n        if self.voice_control_enabled:\r\n            self.voice_control_active = True\r\n            self.voice_processor.start_listening()\r\n            self.announce("Voice control system online")\r\n        \r\n        try:\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down...")\r\n            self.voice_processor.stop_listening()\r\n\r\ndef main():\r\n    controller = ROSVoiceController()\r\n    controller.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,o.jsx)(n.p,{children:"Here's a complete example that demonstrates the voicetoaction system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n# complete_voice_to_action_system.py\r\n\r\nimport rospy\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom complete_whisper_integration import WhisperRobotInterface\r\nimport threading\r\nimport time\r\n\r\nclass CompleteVoiceToActionSystem:\r\n    \"\"\"Complete voicetoaction system using Whisper for robotics\"\"\"\r\n    \r\n    def __init__(self):\r\n        rospy.init_node('complete_voice_to_action_system', anonymous=True)\r\n        \r\n        # Parameters\r\n        self.whisper_model_size = rospy.get_param('~whisper_model', 'base')\r\n        self.voice_control_enabled = rospy.get_param('~voice_control_enabled', True)\r\n        self.safe_distance = rospy.get_param('~safe_distance', 0.5)\r\n        \r\n        # Initialize Whisper interface\r\n        self.whisper_interface = WhisperRobotInterface(\r\n            model_size=self.whisper_model_size\r\n        )\r\n        \r\n        # Publishers and Subscribers\r\n        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\r\n        self.status_pub = rospy.Publisher('/voice_system_status', String, queue_size=10)\r\n        \r\n        rospy.Subscriber('/voice_transcript', String, self.transcript_callback)\r\n        rospy.Subscriber('/voice_command', String, self.command_callback)\r\n        rospy.Subscriber('/scan', LaserScan, self.scan_callback)\r\n        \r\n        # System state\r\n        self.laser_scan = None\r\n        self.obstacle_detected = False\r\n        self.system_active = False\r\n        self.last_command_time = 0\r\n        \r\n        rospy.loginfo(\"Complete VoicetoAction System initialized\")\r\n    \r\n    def scan_callback(self, msg):\r\n        \"\"\"Update laser scan data\"\"\"\r\n        self.laser_scan = msg\r\n        \r\n        # Check for obstacles in front\r\n        if self.laser_scan:\r\n            # Check front arc (\xb130 degrees)\r\n            front_idx = len(self.laser_scan.ranges) // 2\r\n            front_start = max(0, front_idx  30)\r\n            front_end = min(len(self.laser_scan.ranges), front_idx + 30)\r\n            front_ranges = self.laser_scan.ranges[front_start:front_end]\r\n            \r\n            # Filter valid ranges\r\n            valid_ranges = [r for r in front_ranges if 0.1 < r < 10.0]\r\n            \r\n            if valid_ranges:\r\n                min_range = min(valid_ranges)\r\n                self.obstacle_detected = min_range < self.safe_distance\r\n            else:\r\n                self.obstacle_detected = False\r\n    \r\n    def transcript_callback(self, msg):\r\n        \"\"\"Handle new voice transcript\"\"\"\r\n        transcript = msg.data\r\n        rospy.loginfo(f\"Heard: {transcript}\")\r\n        \r\n        # Check if transcript contains a command\r\n        if self._contains_command(transcript):\r\n            if self.obstacle_detected and self._is_movement_command(transcript):\r\n                self._warn_about_obstacle()\r\n            else:\r\n                # Process command\r\n                command = self._interpret_command(transcript)\r\n                if command:\r\n                    self._execute_command(command)\r\n    \r\n    def command_callback(self, msg):\r\n        \"\"\"Handle processed command\"\"\"\r\n        command = msg.data\r\n        rospy.loginfo(f\"Processed command: {command}\")\r\n        # Additional command handling can go here\r\n    \r\n    def _contains_command(self, text):\r\n        \"\"\"Check if text contains actionable commands\"\"\"\r\n        command_keywords = [\r\n            'move', 'go', 'stop', 'turn', 'left', 'right', \r\n            'forward', 'backward', 'straight', 'halt'\r\n        ]\r\n        text_lower = text.lower()\r\n        return any(keyword in text_lower for keyword in command_keywords)\r\n    \r\n    def _is_movement_command(self, text):\r\n        \"\"\"Check if command involves movement\"\"\"\r\n        movement_keywords = ['forward', 'backward', 'go', 'move', 'straight']\r\n        text_lower = text.lower()\r\n        return any(keyword in text_lower for keyword in movement_keywords)\r\n    \r\n    def _interpret_command(self, text):\r\n        \"\"\"Interpret voice command\"\"\"\r\n        text_lower = text.lower()\r\n        \r\n        # Simple command mapping\r\n        if 'forward' in text_lower or 'ahead' in text_lower or 'straight' in text_lower:\r\n            return {'action': 'move', 'direction': 'forward', 'speed': 0.3}\r\n        elif 'backward' in text_lower or 'back' in text_lower:\r\n            return {'action': 'move', 'direction': 'backward', 'speed': 0.3}\r\n        elif 'left' in text_lower:\r\n            return {'action': 'turn', 'direction': 'left', 'speed': 0.4}\r\n        elif 'right' in text_lower:\r\n            return {'action': 'turn', 'direction': 'right', 'speed': 0.4}\r\n        elif 'stop' in text_lower or 'halt' in text_lower:\r\n            return {'action': 'stop'}\r\n        else:\r\n            return None\r\n    \r\n    def _execute_command(self, command):\r\n        \"\"\"Execute parsed command\"\"\"\r\n        twist = Twist()\r\n        \r\n        if command['action'] == 'move':\r\n            if command['direction'] == 'forward':\r\n                twist.linear.x = command['speed']\r\n            elif command['direction'] == 'backward':\r\n                twist.linear.x = command['speed']\r\n        elif command['action'] == 'turn':\r\n            if command['direction'] == 'left':\r\n                twist.angular.z = command['speed']\r\n            elif command['direction'] == 'right':\r\n                twist.angular.z = command['speed']\r\n        elif command['action'] == 'stop':\r\n            # Twist is already zeroed\r\n            pass\r\n        \r\n        # Publish command\r\n        self.cmd_vel_pub.publish(twist)\r\n        rospy.loginfo(f\"Executed command: {command}\")\r\n        \r\n        # Update last command time\r\n        self.last_command_time = time.time()\r\n    \r\n    def _warn_about_obstacle(self):\r\n        \"\"\"Warn about obstacle when movement command is issued\"\"\"\r\n        rospy.logwarn(\"Obstacle detected in front! Movement command aborted.\")\r\n        status_msg = String()\r\n        status_msg.data = \"WARNING: Obstacle detected, movement halted\"\r\n        self.status_pub.publish(status_msg)\r\n    \r\n    def start_system(self):\r\n        \"\"\"Start the voicetoaction system\"\"\"\r\n        rospy.loginfo(\"Starting voicetoaction system...\")\r\n        \r\n        if self.voice_control_enabled:\r\n            self.whisper_interface.start_listening()\r\n            self.system_active = True\r\n            rospy.loginfo(\"Voice control system activated\")\r\n        else:\r\n            rospy.loginfo(\"Voice control disabled by parameter\")\r\n    \r\n    def stop_system(self):\r\n        \"\"\"Stop the system\"\"\"\r\n        self.whisper_interface.stop_listening()\r\n        self.system_active = False\r\n        \r\n        # Stop robot\r\n        stop_twist = Twist()\r\n        self.cmd_vel_pub.publish(stop_twist)\r\n        \r\n        rospy.loginfo(\"Voicetoaction system stopped\")\r\n    \r\n    def run(self):\r\n        \"\"\"Main run loop\"\"\"\r\n        self.start_system()\r\n        \r\n        try:\r\n            # Keep the node running\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo(\"Interrupted, shutting down...\")\r\n        finally:\r\n            self.stop_system()\r\n\r\ndef main():\r\n    \"\"\"Main function to run the complete system\"\"\"\r\n    system = CompleteVoiceToActionSystem()\r\n    \r\n    try:\r\n        system.run()\r\n    except Exception as e:\r\n        rospy.logerr(f\"Error running voicetoaction system: {e}\")\r\n        system.stop_system()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsxs)(n.h3,{id:"launch-file-for-the-system-voice_control_systemlaunch",children:["Launch file for the system (",(0,o.jsx)(n.code,{children:"voice_control_system.launch"}),"):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<launch>\r\n  <! VoicetoAction System >\r\n  <node name="complete_voice_to_action_system" pkg="robot_voice_control" type="complete_voice_to_action_system.py" output="screen">\r\n    <param name="whisper_model" value="base"/>\r\n    <param name="voice_control_enabled" value="true"/>\r\n    <param name="safe_distance" value="0.5"/>\r\n  </node>\r\n  \r\n  <! Optional: Add voice activity detection node >\r\n  <node name="voice_activity_detector" pkg="audio_common" type="voice_activity_detector.py" output="screen">\r\n    <param name="threshold" value="0.01"/>\r\n  </node>\r\n  \r\n  <! Optional: Add texttospeech for feedback >\r\n  <node name="text_to_speech" pkg="sound_play" type="say.py" output="screen"/>\r\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete voicecontrolled robot system that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implements Whisperbased speech recognition for robot commands"}),"\n",(0,o.jsx)(n.li,{children:"Integrates with ROS for seamless robot control"}),"\n",(0,o.jsx)(n.li,{children:"Handles noise reduction and voice activity detection"}),"\n",(0,o.jsx)(n.li,{children:"Implements contextaware command processing"}),"\n",(0,o.jsx)(n.li,{children:"Creates a multimodal feedback system (audio + visual)"}),"\n",(0,o.jsx)(n.li,{children:"Evaluates performance with various accents and environments"}),"\n",(0,o.jsx)(n.li,{children:"Implements safety mechanisms for voice command execution"}),"\n",(0,o.jsx)(n.li,{children:"Provides error recovery for misunderstood commands"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Your project should include:\r\nComplete Whisper integration with audio preprocessing\r\nROS node for voice command processing\r\nContextaware command interpretation\r\nSafety validation for voice commands\r\nPerformance evaluation metrics\r\nMultimodal feedback system\r\nError recovery mechanisms"}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter covered voicetoaction processing using OpenAI Whisper for robotics:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Whisper Integration"}),": Using OpenAI's Whisper model for speech recognition in robotics\r\n",(0,o.jsx)(n.strong,{children:"Audio Preprocessing"}),": Techniques for handling roboticsspecific audio challenges\r\n",(0,o.jsx)(n.strong,{children:"Command Interpretation"}),": Mapping voice commands to robot actions\r\n",(0,o.jsx)(n.strong,{children:"RealTime Processing"}),": Continuous audio processing for immediate response\r\n",(0,o.jsx)(n.strong,{children:"Safety Mechanisms"}),": Validation and safety checks for voicecontrolled robots\r\n",(0,o.jsx)(n.strong,{children:"Context Awareness"}),": Using environmental context to disambiguate commands\r\n",(0,o.jsx)(n.strong,{children:"ROS Integration"}),": Seamless integration with the Robot Operating System"]}),"\n",(0,o.jsx)(n.p,{children:"Voice control provides a natural interface for humanrobot interaction, allowing users to command robots using everyday language. However, special attention must be paid to the unique challenges of audio processing in robotic environments and ensuring safe execution of voice commands."})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}}}]);