"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[2111],{1769:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>d,contentTitle:()=>l,default:()=>c,frontMatter:()=>t,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"module-4-vision-language-action/ch19-multimodal-perception-fusion","title":"ch19-multimodal-perception-fusion","description":"-----","source":"@site/docs/module-4-vision-language-action/ch19-multimodal-perception-fusion.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch19-multimodal-perception-fusion","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch19-multimodal-perception-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch19-multimodal-perception-fusion.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch18-cognitive-task-planning-gpt4o","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch18-cognitive-task-planning-gpt4o"},"next":{"title":"ch20-capstone-autonomous-humanoid","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch20-capstone-autonomous-humanoid"}}');var s=e(4848),a=e(8453),o=e(7242);const t={},l=void 0,d={},m=[{value:"title: Ch19  Multimodal Perception Fusion\r\nmodule: 4\r\nchapter: 19\r\nsidebar_label: Ch19: Multimodal Perception Fusion\r\ndescription: Fusing multiple sensor modalities using advanced AI for enhanced robotic perception\r\ntags: [multimodal, perception, fusion, vision, lidar, radar, sensors, deeplearning, transformers]\r\ndifficulty: advanced\r\nestimated_duration: 150",id:"title-ch19--multimodal-perception-fusionmodule-4chapter-19sidebar_label-ch19-multimodal-perception-fusiondescription-fusing-multiple-sensor-modalities-using-advanced-ai-for-enhanced-robotic-perceptiontags-multimodal-perception-fusion-vision-lidar-radar-sensors-deeplearning-transformersdifficulty-advancedestimated_duration-150",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"Multimodal Perception Fundamentals",id:"multimodal-perception-fundamentals",level:3},{value:"Sensor Characteristics",id:"sensor-characteristics",level:3},{value:"Fusion Architectures",id:"fusion-architectures",level:3},{value:"TransformerBased Fusion",id:"transformerbased-fusion",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up Multimodal Sensor Data Pipeline",id:"lab-1-setting-up-multimodal-sensor-data-pipeline",level:3},{value:"Lab 2: Creating a CrossModal Attention Fusion Network",id:"lab-2-creating-a-crossmodal-attention-fusion-network",level:3},{value:"Lab 3: Implementing Sensor Calibration and Data Association",id:"lab-3-implementing-sensor-calibration-and-data-association",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Launch file for the multimodal fusion system:",id:"launch-file-for-the-multimodal-fusion-system",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function u(n){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"title-ch19--multimodal-perception-fusionmodule-4chapter-19sidebar_label-ch19-multimodal-perception-fusiondescription-fusing-multiple-sensor-modalities-using-advanced-ai-for-enhanced-robotic-perceptiontags-multimodal-perception-fusion-vision-lidar-radar-sensors-deeplearning-transformersdifficulty-advancedestimated_duration-150",children:"title: Ch19  Multimodal Perception Fusion\r\nmodule: 4\r\nchapter: 19\r\nsidebar_label: Ch19: Multimodal Perception Fusion\r\ndescription: Fusing multiple sensor modalities using advanced AI for enhanced robotic perception\r\ntags: [multimodal, perception, fusion, vision, lidar, radar, sensors, deeplearning, transformers]\r\ndifficulty: advanced\r\nestimated_duration: 150"}),"\n","\n",(0,s.jsx)(r.h1,{id:"multimodal-perception-fusion",children:"Multimodal Perception Fusion"}),"\n",(0,s.jsx)(r.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(r.p,{children:"Understand the principles of multimodal sensor fusion for robotics\r\nImplement fusion of vision, LiDAR, radar, IMU, and other sensor modalities\r\nDesign neural architectures for multimodal perception\r\nApply transformerbased fusion techniques\r\nEvaluate the performance benefits of multimodal fusion\r\nHandle sensor calibration and synchronization\r\nImplement robust perception systems using multiple sensors\r\nCreate confidenceaware perception fusion systems"}),"\n",(0,s.jsx)(r.h2,{id:"theory",children:"Theory"}),"\n",(0,s.jsx)(r.h3,{id:"multimodal-perception-fundamentals",children:"Multimodal Perception Fundamentals"}),"\n",(0,s.jsx)(r.p,{children:"Multimodal perception in robotics involves combining information from different sensory modalities to create a more comprehensive understanding of the environment than any single sensor could provide."}),"\n",(0,s.jsx)(o.A,{chart:"\ngraph TD;\n  A[Multimodal Sensor Fusion] > B[Vision];\n  A > C[LiDAR];\n  A > D[Radar];\n  A > E[IMU];\n  A > F[Other Sensors];\n  \n  B > G[Semantic Segmentation];\n  B > H[Object Detection];\n  B > I[Depth Estimation];\n  \n  C > J[Point Cloud Processing];\n  C > K[3D Object Detection];\n  C > L[Environment Mapping];\n  \n  D > M[Long Range Detection];\n  D > N[Weather Robustness];\n  D > O[Doppler Information];\n  \n  E > P[Inertial Navigation];\n  E > Q[Orientation Tracking];\n  E > R[Motion Compensation];\n  \n  F > S[Touch Feedback];\n  F > T[Auditory Input];\n  F > U[Tactile Sensing];\n  \n  G > V[SpatioTemporal Fusion];\n  H > V;\n  J > V;\n  M > V;\n  P > V;\n  \n  V > W[Fused Perception Output];\n  W > X[Robotic Action];\n  W > Y[Navigation Planning];\n  W > Z[Interaction Decision];\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style V fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style W fill:#FF9800,stroke:#E65100,color:#fff;\n"}),"\n",(0,s.jsx)(r.h3,{id:"sensor-characteristics",children:"Sensor Characteristics"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Vision"}),": Rich semantic information, but sensitive to lighting conditions and occlusions. Excels at texture and color recognition."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"LiDAR"}),": Precise distance measurements, robust to lighting conditions, but sparse data and no texture information."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Radar"}),": Longrange detection, works in adverse weather, provides velocity information, but lower resolution."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"IMU"}),": Highfrequency motion data, drift over time, excellent for shortterm motion tracking."]}),"\n",(0,s.jsx)(r.h3,{id:"fusion-architectures",children:"Fusion Architectures"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Early Fusion"}),": Raw sensor data is combined before feature extraction. Allows for crossmodality learning but requires sensor synchronization."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Late Fusion"}),": Features from each sensor modality are extracted separately and then combined. More robust to sensor failures but may miss crossmodal patterns."]}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Deep Fusion"}),": Learnable fusion layers within deep neural networks. Combines benefits of early and late fusion."]}),"\n",(0,s.jsx)(r.h3,{id:"transformerbased-fusion",children:"TransformerBased Fusion"}),"\n",(0,s.jsx)(r.p,{children:"Attention mechanisms allow the model to focus on the most relevant information from each modality at different spatial and temporal locations."}),"\n",(0,s.jsx)(r.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,s.jsx)(r.h3,{id:"lab-1-setting-up-multimodal-sensor-data-pipeline",children:"Lab 1: Setting up Multimodal Sensor Data Pipeline"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Create a multimodal data loader"})," (",(0,s.jsx)(r.code,{children:"multimodal_dataloader.py"}),"):","\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\nimport cv2\r\nfrom sensor_msgs.msg import Image, PointCloud2, Imu\r\nfrom geometry_msgs.msg import Vector3, Quaternion\r\nfrom cv_bridge import CvBridge\r\nimport sensor_msgs.point_cloud2 as pc2\r\nfrom std_msgs.msg import Header\r\nfrom typing import Dict, List, Tuple, Optional\r\nimport rospy\r\n\r\nclass MultiModalDataLoader:\r\n    def __init__(self):\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Sensor buffers\r\n        self.rgb_buffer = []\r\n        self.depth_buffer = []\r\n        self.lidar_buffer = []\r\n        self.imu_buffer = []\r\n        \r\n        # Synchronization window size\r\n        self.sync_window = 0.1  # 100ms sync window\r\n        \r\n        # ROS subscribers\r\n        self.rgb_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.rgb_callback)\r\n        self.depth_sub = rospy.Subscriber('/camera/depth/image_raw', Image, self.depth_callback)\r\n        self.lidar_sub = rospy.Subscriber('/lidar/points', PointCloud2, self.lidar_callback)\r\n        self.imu_sub = rospy.Subscriber('/imu/data', Imu, self.imu_callback)\r\n        \r\n        # Data transformations\r\n        self.image_transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((224, 224)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n    \r\n    def rgb_callback(self, msg: Image):\r\n        \"\"\"Process RGB image message\"\"\"\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            timestamp = msg.header.stamp.to_sec()\r\n            self.rgb_buffer.append((cv_image, timestamp, msg.header))\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error processing RGB image: {e}\")\r\n    \r\n    def depth_callback(self, msg: Image):\r\n        \"\"\"Process depth image message\"\"\"\r\n        try:\r\n            cv_depth = self.cv_bridge.imgmsg_to_cv2(msg, \"32FC1\")\r\n            timestamp = msg.header.stamp.to_sec()\r\n            self.depth_buffer.append((cv_depth, timestamp, msg.header))\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error processing depth image: {e}\")\r\n    \r\n    def lidar_callback(self, msg: PointCloud2):\r\n        \"\"\"Process LiDAR point cloud message\"\"\"\r\n        try:\r\n            points = np.array(list(pc2.read_points(msg, \r\n                                                  field_names=[\"x\", \"y\", \"z\"], \r\n                                                  skip_nans=True)))\r\n            timestamp = msg.header.stamp.to_sec()\r\n            self.lidar_buffer.append((points, timestamp, msg.header))\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error processing LiDAR data: {e}\")\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        \"\"\"Process IMU message\"\"\"\r\n        try:\r\n            accel = np.array([msg.linear_acceleration.x, \r\n                            msg.linear_acceleration.y, \r\n                            msg.linear_acceleration.z])\r\n            gyro = np.array([msg.angular_velocity.x, \r\n                           msg.angular_velocity.y, \r\n                           msg.angular_velocity.z])\r\n            quat = np.array([msg.orientation.x, \r\n                           msg.orientation.y, \r\n                           msg.orientation.z, \r\n                           msg.orientation.w])\r\n            \r\n            timestamp = msg.header.stamp.to_sec()\r\n            self.imu_buffer.append((accel, gyro, quat, timestamp, msg.header))\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error processing IMU data: {e}\")\r\n    \r\n    def synchronize_modalities(self) > Optional[Dict]:\r\n        \"\"\"Synchronize data from different sensors within time window\"\"\"\r\n        # Remove old data outside sync window\r\n        current_time = rospy.get_rostime().to_sec()\r\n        window_start = current_time  self.sync_window\r\n        \r\n        # Filter buffers\r\n        self.rgb_buffer = [(data, ts, header) for data, ts, header in self.rgb_buffer if ts >= window_start]\r\n        self.depth_buffer = [(data, ts, header) for data, ts, header in self.depth_buffer if ts >= window_start]\r\n        self.lidar_buffer = [(data, ts, header) for data, ts, header in self.lidar_buffer if ts >= window_start]\r\n        self.imu_buffer = [(acc, gyr, qua, ts, header) for acc, gyr, qua, ts, header in self.imu_buffer if ts >= window_start]\r\n        \r\n        if not (self.rgb_buffer and self.lidar_buffer and self.imu_buffer):\r\n            return None\r\n        \r\n        # Find closest timestamps\r\n        rgb_ts = [(abs(ts  current_time), i) for i, (_, ts, _) in enumerate(self.rgb_buffer)]\r\n        lidar_ts = [(abs(ts  current_time), i) for i, (_, ts, _) in enumerate(self.lidar_buffer)]\r\n        imu_ts = [(abs(ts  current_time), i) for i, (_, _, _, ts, _) in enumerate(self.imu_buffer)]\r\n        \r\n        if not (rgb_ts and lidar_ts and imu_ts):\r\n            return None\r\n        \r\n        # Get closest data points\r\n        rgb_idx = min(rgb_ts)[1]\r\n        lidar_idx = min(lidar_ts)[1]\r\n        imu_idx = min(imu_ts)[1]\r\n        \r\n        # Pack synchronized data\r\n        synchronized_data = {\r\n            'rgb': self.rgb_buffer[rgb_idx][0],\r\n            'rgb_timestamp': self.rgb_buffer[rgb_idx][1],\r\n            'lidar': self.lidar_buffer[lidar_idx][0],\r\n            'lidar_timestamp': self.lidar_buffer[lidar_idx][1],\r\n            'imu_accel': self.imu_buffer[imu_idx][0],\r\n            'imu_gyro': self.imu_buffer[imu_idx][1],\r\n            'imu_quat': self.imu_buffer[imu_idx][2],\r\n            'imu_timestamp': self.imu_buffer[imu_idx][3]\r\n        }\r\n        \r\n        return synchronized_data\r\n    \r\n    def preprocess_multimodal_data(self, data: Dict) > Dict:\r\n        \"\"\"Preprocess multimodal data for neural networks\"\"\"\r\n        processed_data = {}\r\n        \r\n        # Process RGB image\r\n        if 'rgb' in data:\r\n            rgb_tensor = torch.from_numpy(data['rgb']).permute(2, 0, 1).float() / 255.0\r\n            processed_data['rgb'] = self.image_transform(data['rgb']).unsqueeze(0)\r\n        \r\n        # Process LiDAR points\r\n        if 'lidar' in data:\r\n            lidar_tensor = torch.from_numpy(data['lidar']).float()\r\n            # Normalize point cloud\r\n            lidar_mean = lidar_tensor.mean(dim=0, keepdim=True)\r\n            lidar_std = lidar_tensor.std(dim=0, keepdim=True) + 1e8\r\n            normalized_lidar = (lidar_tensor  lidar_mean) / lidar_std\r\n            processed_data['lidar'] = normalized_lidar.unsqueeze(0)\r\n        \r\n        # Process IMU data\r\n        if 'imu_accel' in data:\r\n            accel_tensor = torch.from_numpy(data['imu_accel']).float().unsqueeze(0)\r\n            gyro_tensor = torch.from_numpy(data['imu_gyro']).float().unsqueeze(0)\r\n            quat_tensor = torch.from_numpy(data['imu_quat']).float().unsqueeze(0)\r\n            \r\n            processed_data['imu'] = {\r\n                'accel': accel_tensor,\r\n                'gyro': gyro_tensor,\r\n                'quat': quat_tensor\r\n            }\r\n        \r\n        return processed_data\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"lab-2-creating-a-crossmodal-attention-fusion-network",children:"Lab 2: Creating a CrossModal Attention Fusion Network"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Implement a crossmodal fusion network"})," (",(0,s.jsx)(r.code,{children:"crossmodal_fusion_network.py"}),"):","\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision.models as models\r\nfrom typing import Dict, List, Optional\r\nimport numpy as np\r\n\r\nclass VisionTransformerEncoder(nn.Module):\r\n    """Vision Transformer for image encoding"""\r\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, num_heads=12):\r\n        super().__init__()\r\n        self.embed_dim = embed_dim\r\n        \r\n        # Patch embedding\r\n        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\r\n        \r\n        # Positional embedding\r\n        self.num_patches = (img_size // patch_size) ** 2\r\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\r\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\r\n        \r\n        # Transformer blocks\r\n        self.blocks = nn.ModuleList([\r\n            Block(embed_dim, num_heads) for _ in range(depth)\r\n        ])\r\n        \r\n        self.norm = nn.LayerNorm(embed_dim)\r\n        \r\n    def forward(self, x):\r\n        B, C, H, W = x.shape\r\n        \r\n        # Convert image to patches and embed\r\n        x = self.patch_embed(x)  # [B, embed_dim, h, w]\r\n        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\r\n        \r\n        # Add class token and positional embeddings\r\n        cls_tokens = self.cls_token.expand(B, 1, 1)\r\n        x = torch.cat([cls_tokens, x], dim=1)\r\n        x = x + self.pos_embed\r\n        \r\n        # Apply transformer blocks\r\n        for blk in self.blocks:\r\n            x = blk(x)\r\n        \r\n        x = self.norm(x)\r\n        return x\r\n\r\nclass Block(nn.Module):\r\n    """Transformer block with crossmodal attention"""\r\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\r\n        super().__init__()\r\n        self.norm1 = nn.LayerNorm(dim)\r\n        self.attn = Attention(dim, num_heads, qkv_bias, attn_drop, drop)\r\n        self.norm2 = nn.LayerNorm(dim)\r\n        mlp_hidden_dim = int(dim * mlp_ratio)\r\n        self.mlp = Mlp(dim, mlp_hidden_dim, drop=drop)\r\n    \r\n    def forward(self, x):\r\n        x = x + self.attn(self.norm1(x))\r\n        x = x + self.mlp(self.norm2(x))\r\n        return x\r\n\r\nclass Attention(nn.Module):\r\n    """Multihead attention with potential crossmodal capabilities"""\r\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\r\n        super().__init__()\r\n        self.num_heads = num_heads\r\n        head_dim = dim // num_heads\r\n        self.scale = head_dim ** 0.5\r\n\r\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\r\n        self.attn_drop = nn.Dropout(attn_drop)\r\n        self.proj = nn.Linear(dim, dim)\r\n        self.proj_drop = nn.Dropout(proj_drop)\r\n\r\n    def forward(self, x):\r\n        B, N, C = x.shape\r\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\r\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\r\n\r\n        attn = (q @ k.transpose(2, 1)) * self.scale\r\n        attn = attn.softmax(dim=1)\r\n        attn = self.attn_drop(attn)\r\n\r\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\r\n        x = self.proj(x)\r\n        x = self.proj_drop(x)\r\n        return x\r\n\r\nclass Mlp(nn.Module):\r\n    """MLP as used in Vision Transformer, MLPMixer and related networks"""\r\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\r\n        super().__init__()\r\n        out_features = out_features or in_features\r\n        hidden_features = hidden_features or in_features\r\n        self.fc1 = nn.Linear(in_features, hidden_features)\r\n        self.act = act_layer()\r\n        self.fc2 = nn.Linear(hidden_features, out_features)\r\n        self.drop = nn.Dropout(drop)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        x = self.act(x)\r\n        x = self.drop(x)\r\n        x = self.fc2(x)\r\n        x = self.drop(x)\r\n        return x\r\n\r\nclass PointNetEncoder(nn.Module):\r\n    """PointNetstyle encoder for LiDAR points"""\r\n    def __init__(self, in_features=3, embed_dim=512):\r\n        super().__init__()\r\n        \r\n        # Shared MLP layers\r\n        self.conv1 = nn.Conv1d(in_features, 64, 1)\r\n        self.conv2 = nn.Conv1d(64, 128, 1)\r\n        self.conv3 = nn.Conv1d(128, embed_dim, 1)\r\n        \r\n        # Global feature aggregation\r\n        self.fc_global = nn.Linear(embed_dim, embed_dim)\r\n        \r\n        self.bn1 = nn.BatchNorm1d(64)\r\n        self.bn2 = nn.BatchNorm1d(128)\r\n        self.bn3 = nn.BatchNorm1d(embed_dim)\r\n        self.bn_global = nn.BatchNorm1d(embed_dim)\r\n        \r\n    def forward(self, x):\r\n        # x shape: [batch_size, num_points, in_features]\r\n        x = x.transpose(1, 2)  # [batch_size, in_features, num_points]\r\n        \r\n        # Apply convolutions\r\n        x = F.relu(self.bn1(self.conv1(x)))  # [batch_size, 64, num_points]\r\n        x = F.relu(self.bn2(self.conv2(x)))  # [batch_size, 128, num_points]\r\n        x = F.relu(self.bn3(self.conv3(x)))  # [batch_size, embed_dim, num_points]\r\n        \r\n        # Global max pooling to get global features\r\n        global_feat = torch.max(x, dim=2)[0]  # [batch_size, embed_dim]\r\n        \r\n        global_feat = F.relu(self.bn_global(self.fc_global(global_feat)))\r\n        \r\n        return global_feat\r\n\r\nclass IMUEncoder(nn.Module):\r\n    """Encoder for IMU data"""\r\n    def __init__(self, input_dim=10, hidden_dim=256, output_dim=128):\r\n        super().__init__()\r\n        \r\n        # IMU data has: 3D acceleration + 3D gyro + 4D quaternion = 10D\r\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.1)\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n        self.norm = nn.LayerNorm(output_dim)\r\n    \r\n    def forward(self, accel, gyro, quat):\r\n        # Concatenate IMU data\r\n        imu_data = torch.cat([accel, gyro, quat], dim=1)  # [batch, seq_len, 10]\r\n        \r\n        # LSTM processing\r\n        lstm_out, _ = self.lstm(imu_data)\r\n        \r\n        # Take the last output\r\n        last_output = lstm_out[:, 1, :]  # [batch, hidden_dim]\r\n        \r\n        # Final projection\r\n        output = self.fc(last_output)  # [batch, output_dim]\r\n        output = self.norm(output)\r\n        \r\n        return output\r\n\r\nclass CrossModalFusion(nn.Module):\r\n    """Crossmodal attention fusion module"""\r\n    def __init__(self, vision_dim=768, lidar_dim=512, imu_dim=128, fused_dim=1024):\r\n        super().__init__()\r\n        \r\n        self.vision_dim = vision_dim\r\n        self.lidar_dim = lidar_dim\r\n        self.imu_dim = imu_dim\r\n        self.fused_dim = fused_dim\r\n        \r\n        # Projection layers to common dimension\r\n        self.vision_proj = nn.Linear(vision_dim, fused_dim)\r\n        self.lidar_proj = nn.Linear(lidar_dim, fused_dim)\r\n        self.imu_proj = nn.Linear(imu_dim, fused_dim)\r\n        \r\n        # Crossattention layers\r\n        self.vision_lidar_attn = nn.MultiheadAttention(fused_dim, num_heads=8, batch_first=True)\r\n        self.lidar_imu_attn = nn.MultiheadAttention(fused_dim, num_heads=8, batch_first=True)\r\n        self.global_fusion_attn = nn.MultiheadAttention(fused_dim, num_heads=8, batch_first=True)\r\n        \r\n        # Layer normalization\r\n        self.norm_vision = nn.LayerNorm(fused_dim)\r\n        self.norm_lidar = nn.LayerNorm(fused_dim)\r\n        self.norm_imu = nn.LayerNorm(fused_dim)\r\n        self.norm_fused = nn.LayerNorm(fused_dim)\r\n        \r\n        # Output layers\r\n        self.fusion_mlp = nn.Sequential(\r\n            nn.Linear(fused_dim, fused_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(fused_dim // 2, fused_dim // 4),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(fused_dim // 4, fused_dim)\r\n        )\r\n    \r\n    def forward(self, vision_feat, lidar_feat, imu_feat):\r\n        """\r\n        Fusion of vision, lidar, and IMU features\r\n        vision_feat: [batch, vision_dim]\r\n        lidar_feat: [batch, lidar_dim] \r\n        imu_feat: [batch, imu_dim]\r\n        """\r\n        batch_size = vision_feat.size(0)\r\n        \r\n        # Project to common dimension\r\n        vision_proj = self.norm_vision(self.vision_proj(vision_feat))\r\n        lidar_proj = self.norm_lidar(self.lidar_proj(lidar_feat))\r\n        imu_proj = self.norm_imu(self.imu_proj(imu_feat))\r\n        \r\n        # Reshape for attention: [batch, seq_len, feat_dim]\r\n        vision_seq = vision_proj.unsqueeze(1)  # [batch, 1, fused_dim]\r\n        lidar_seq = lidar_proj.unsqueeze(1)    # [batch, 1, fused_dim]\r\n        imu_seq = imu_proj.unsqueeze(1)        # [batch, 1, fused_dim]\r\n        \r\n        # Crossattention between vision and lidar\r\n        vis_lid_query = vision_seq\r\n        vis_lid_key_value = lidar_seq\r\n        vis_lid_attn_out, _ = self.vision_lidar_attn(vis_lid_query, vis_lid_key_value, vis_lid_key_value)\r\n        vis_lid_fused = self.norm_fused(vision_seq + vis_lid_attn_out)  # Residual connection\r\n        \r\n        # Crossattention between lidar and IMU\r\n        lid_imu_query = lidar_seq\r\n        lid_imu_key_value = imu_seq\r\n        lid_imu_attn_out, _ = self.lidar_imu_attn(lid_imu_query, lid_imu_key_value, lid_imu_key_value)\r\n        lid_imu_fused = self.norm_fused(lidar_seq + lid_imu_attn_out)\r\n        \r\n        # Global fusion of all modalities\r\n        all_features = torch.cat([\r\n            vis_lid_fused,  # VisionLidar fused\r\n            lid_imu_fused,  # LiDARIMU fused  \r\n            imu_seq         # Original IMU (for stability)\r\n        ], dim=1)  # [batch, 3, fused_dim]\r\n        \r\n        # Selfattention across modalities\r\n        global_fused, _ = self.global_fusion_attn(all_features, all_features, all_features)\r\n        \r\n        # Global pooling (average)\r\n        global_pooled = global_fused.mean(dim=1)  # [batch, fused_dim]\r\n        \r\n        # Final fusion MLP\r\n        fused_output = self.fusion_mlp(global_pooled)\r\n        \r\n        return fused_output\r\n\r\nclass MultiModalPerceptionFusion(nn.Module):\r\n    """Complete multimodal perception fusion network"""\r\n    def __init__(self, num_classes=10):\r\n        super().__init__()\r\n        \r\n        # Encoder networks\r\n        self.vision_encoder = VisionTransformerEncoder()\r\n        self.lidar_encoder = PointNetEncoder()\r\n        self.imu_encoder = IMUEncoder()\r\n        \r\n        # Crossmodal fusion\r\n        self.cross_fusion = CrossModalFusion()\r\n        \r\n        # Taskspecific heads\r\n        self.classification_head = nn.Linear(1024, num_classes)\r\n        self.detection_head = nn.Linear(1024, 4)  # bbox coordinates\r\n        self.segmentation_head = nn.Linear(1024, 21)  # 21 semantic classes\r\n        \r\n        # Confidence estimation\r\n        self.confidence_head = nn.Linear(1024, 1)\r\n    \r\n    def forward(self, rgb, lidar_points, imu_accel, imu_gyro, imu_quat):\r\n        # Encode each modality\r\n        vision_feat = self.vision_encoder(rgb)  # Vision features\r\n        vision_cls_feat = vision_feat[:, 0, :]  # Take CLS token\r\n        \r\n        lidar_feat = self.lidar_encoder(lidar_points)\r\n        imu_feat = self.imu_encoder(imu_accel, imu_gyro, imu_quat)\r\n        \r\n        # Fuse modalities\r\n        fused_feat = self.cross_fusion(vision_cls_feat, lidar_feat, imu_feat)\r\n        \r\n        # Taskspecific outputs\r\n        classification_out = self.classification_head(fused_feat)\r\n        detection_out = self.detection_head(fused_feat)\r\n        segmentation_out = self.segmentation_head(fused_feat)\r\n        confidence_out = torch.sigmoid(self.confidence_head(fused_feat))\r\n        \r\n        return {\r\n            \'classification\': classification_out,\r\n            \'detection\': detection_out,\r\n            \'segmentation\': segmentation_out,\r\n            \'confidence\': confidence_out,\r\n            \'fused_features\': fused_feat\r\n        }\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"lab-3-implementing-sensor-calibration-and-data-association",children:"Lab 3: Implementing Sensor Calibration and Data Association"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Create calibration and data association module"})," (",(0,s.jsx)(r.code,{children:"calibration_module.py"}),"):","\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport numpy as np\r\nimport cv2\r\nimport rospy\r\nfrom sensor_msgs.msg import CameraInfo\r\nfrom geometry_msgs.msg import Transform, TransformStamped\r\nimport tf2_ros\r\nimport tf2_geometry_msgs\r\nfrom typing import Dict, Tuple, Optional\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass SensorCalibrationModule:\r\n    def __init__(self):\r\n        # Initialize TF buffer and listener\r\n        self.tf_buffer = tf2_ros.Buffer()\r\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer)\r\n        \r\n        # Camera intrinsic parameters (will be populated from camera_info topic)\r\n        self.camera_matrix = None\r\n        self.dist_coeffs = None\r\n        \r\n        # Transformation matrices between sensors\r\n        self.transforms = {\r\n            'lidar_to_camera': None,  # LiDAR to camera\r\n            'imu_to_camera': None,    # IMU to camera\r\n            'gps_to_lidar': None      # GPS to LiDAR\r\n        }\r\n        \r\n        # Calibration state\r\n        self.is_calibrated = False\r\n        \r\n        # Subscribe to camera info for intrinsic parameters\r\n        rospy.Subscriber('/camera/rgb/camera_info', CameraInfo, self.camera_info_callback)\r\n        \r\n        rospy.loginfo(\"Sensor Calibration Module initialized\")\r\n    \r\n    def camera_info_callback(self, msg: CameraInfo):\r\n        \"\"\"Update camera intrinsic parameters\"\"\"\r\n        self.camera_matrix = np.array(msg.K).reshape(3, 3)\r\n        self.dist_coeffs = np.array(msg.D)\r\n    \r\n    def get_transform(self, source_frame: str, target_frame: str) > Optional[np.ndarray]:\r\n        \"\"\"Get transformation matrix between two frames\"\"\"\r\n        try:\r\n            transform_stamped = self.tf_buffer.lookup_transform(\r\n                target_frame, source_frame, rospy.Time(0), rospy.Duration(1.0))\r\n            \r\n            # Extract translation and rotation\r\n            translation = np.array([\r\n                transform_stamped.transform.translation.x,\r\n                transform_stamped.transform.translation.y,\r\n                transform_stamped.transform.translation.z\r\n            ])\r\n            \r\n            rotation_quat = np.array([\r\n                transform_stamped.transform.rotation.x,\r\n                transform_stamped.transform.rotation.y,\r\n                transform_stamped.transform.rotation.z,\r\n                transform_stamped.transform.rotation.w\r\n            ])\r\n            \r\n            # Convert quaternion to rotation matrix\r\n            rotation = R.from_quat(rotation_quat).as_matrix()\r\n            \r\n            # Create 4x4 transformation matrix\r\n            transform_matrix = np.eye(4)\r\n            transform_matrix[:3, :3] = rotation\r\n            transform_matrix[:3, 3] = translation\r\n            \r\n            return transform_matrix\r\n            \r\n        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\r\n            rospy.logerr(f\"Transform lookup failed: {e}\")\r\n            return None\r\n    \r\n    def project_lidar_to_camera(self, lidar_points: np.ndarray, \r\n                              lidar_to_camera_transform: np.ndarray) > Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"Project LiDAR points to camera image coordinates\"\"\"\r\n        if self.camera_matrix is None:\r\n            rospy.logwarn(\"Camera matrix not available\")\r\n            return np.array([]), np.array([])\r\n        \r\n        # Transform LiDAR points to camera frame\r\n        # Add homogeneous coordinate\r\n        lidar_points_homo = np.hstack([lidar_points, np.ones((lidar_points.shape[0], 1))])\r\n        camera_frame_points = (lidar_to_camera_transform @ lidar_points_homo.T).T\r\n        \r\n        # Remove homogeneous coordinate\r\n        camera_frame_points = camera_frame_points[:, :3]\r\n        \r\n        # Project to image coordinates\r\n        projected_points = cv2.projectPoints(\r\n            camera_frame_points.astype(np.float32),\r\n            np.zeros(3),  # rvec (rotation vector)\r\n            np.zeros(3),  # tvec (translation vector, already applied)\r\n            self.camera_matrix,\r\n            self.dist_coeffs if self.dist_coeffs is not None else np.zeros(5)\r\n        )[0].squeeze()\r\n        \r\n        # Get depths for filtering\r\n        depths = camera_frame_points[:, 2]  # zcoordinate in camera frame\r\n        \r\n        # Filter points in front of camera\r\n        valid_points = depths > 0\r\n        projected_points = projected_points[valid_points]\r\n        depths = depths[valid_points]\r\n        \r\n        return projected_points, depths\r\n    \r\n    def associate_sensor_data(self, rgb_image_shape: Tuple[int, int], \r\n                            lidar_points: np.ndarray, \r\n                            imu_data: Dict) > Dict:\r\n        \"\"\"Associate data from different sensors\"\"\"\r\n        # Get transforms\r\n        lidar_to_camera = self.get_transform('lidar_frame', 'camera_frame')\r\n        imu_to_camera = self.get_transform('imu_frame', 'camera_frame')\r\n        \r\n        if lidar_to_camera is None or imu_to_camera is None:\r\n            rospy.logwarn(\"Cannot get necessary transforms for data association\")\r\n            return {}\r\n        \r\n        # Project LiDAR points to camera coordinates\r\n        projected_points, depths = self.project_lidar_to_camera(lidar_points, lidar_to_camera)\r\n        \r\n        # Associate LiDAR points with image pixels\r\n        associations = []\r\n        img_height, img_width = rgb_image_shape\r\n        \r\n        for i, (proj_point, depth) in enumerate(zip(projected_points, depths)):\r\n            u, v = int(proj_point[0]), int(proj_point[1])\r\n            \r\n            # Check if projection is within image bounds\r\n            if 0 <= u < img_width and 0 <= v < img_height:\r\n                associations.append({\r\n                    'lidar_idx': i,\r\n                    'image_u': u,\r\n                    'image_v': v,\r\n                    'depth': depth\r\n                })\r\n        \r\n        return {\r\n            'lidar_projection': projected_points,\r\n            'lidar_depths': depths,\r\n            'associations': associations,\r\n            'transforms': {\r\n                'lidar_to_camera': lidar_to_camera,\r\n                'imu_to_camera': imu_to_camera\r\n            }\r\n        }\r\n    \r\n    def calibrate_sensors(self) > bool:\r\n        \"\"\"Perform sensor calibration routine\"\"\"\r\n        rospy.loginfo(\"Starting sensor calibration...\")\r\n        \r\n        # This would typically involve:\r\n        # 1. Collecting synchronized calibration data\r\n        # 2. Using calibration patterns (chessboards, etc.)\r\n        # 3. Computing extrinsic parameters\r\n        # 4. Validating calibration quality\r\n        \r\n        # For this example, we'll use the TF transforms directly\r\n        # as they represent the calibrated relationships\r\n        \r\n        try:\r\n            # Check if all necessary transforms are available\r\n            transforms_available = True\r\n            required_pairs = [\r\n                ('lidar_frame', 'camera_frame'),\r\n                ('imu_frame', 'camera_frame'),\r\n                ('camera_frame', 'base_link')\r\n            ]\r\n            \r\n            for source, target in required_pairs:\r\n                try:\r\n                    self.get_transform(source, target)\r\n                except:\r\n                    rospy.logwarn(f\"Transform {source} to {target} not available\")\r\n                    transforms_available = False\r\n            \r\n            if transforms_available:\r\n                rospy.loginfo(\"Sensor calibration completed using TF transforms\")\r\n                self.is_calibrated = True\r\n                return True\r\n            else:\r\n                rospy.logerr(\"Not all required transforms are available\")\r\n                return False\r\n                \r\n        except Exception as e:\r\n            rospy.logerr(f\"Calibration failed: {e}\")\r\n            return False\r\n    \r\n    def validate_calibration(self, data_association: Dict) > Dict:\r\n        \"\"\"Validate the quality of calibration\"\"\"\r\n        if 'associations' not in data_association:\r\n            return {'valid': False, 'error': 'No associations to validate'}\r\n        \r\n        associations = data_association['associations']\r\n        \r\n        if len(associations) == 0:\r\n            return {'valid': False, 'error': 'No valid associations found'}\r\n        \r\n        # Calculate reprojection errors\r\n        errors = []\r\n        for assoc in associations:\r\n            # In a real system, we'd have ground truth correspondences\r\n            # For now, we'll just validate the association is reasonable\r\n            if 0 <= assoc['image_u'] < 1920 and 0 <= assoc['image_v'] < 1080:  # common camera resolution\r\n                errors.append(abs(assoc['depth']))  # Depth should be positive for valid projections\r\n            else:\r\n                errors.append(float('inf'))\r\n        \r\n        avg_error = np.mean(errors) if errors else float('inf')\r\n        max_error = np.max(errors) if errors else float('inf')\r\n        \r\n        # Acceptable thresholds (adjust based on sensor specs)\r\n        valid_associations = [e for e in errors if e < 100.0]  # arbitrary threshold\r\n        success_rate = len(valid_associations) / len(errors) if errors else 0.0\r\n        \r\n        return {\r\n            'valid': success_rate > 0.1,  # At least 10% of points should be valid\r\n            'avg_error': avg_error,\r\n            'max_error': max_error,\r\n            'success_rate': success_rate,\r\n            'total_associations': len(associations)\r\n        }\r\n\r\nclass MultiModalFusionPipeline:\r\n    \"\"\"Complete pipeline for multimodal fusion with calibration\"\"\"\r\n    def __init__(self):\r\n        self.calibration_module = SensorCalibrationModule()\r\n        self.fusion_network = MultiModalPerceptionFusion()\r\n        \r\n        # Initialize ROS node and subscribers\r\n        rospy.init_node('multimodal_fusion_pipeline', anonymous=True)\r\n        \r\n        # Data synchronization\r\n        self.latest_data = {\r\n            'rgb': None,\r\n            'lidar': None,\r\n            'imu': None,\r\n            'timestamp': None\r\n        }\r\n        \r\n        # Data association results\r\n        self.associations = None\r\n        \r\n        rospy.loginfo(\"Multimodal Fusion Pipeline initialized\")\r\n    \r\n    def process_multimodal_data(self, rgb_img, lidar_points, imu_data, timestamp):\r\n        \"\"\"Process synchronized multimodal data\"\"\"\r\n        # Validate calibration\r\n        if not self.calibration_module.is_calibrated:\r\n            if not self.calibration_module.calibrate_sensors():\r\n                rospy.logerr(\"Failed to calibrate sensors, skipping fusion\")\r\n                return None\r\n        \r\n        # Associate sensor data\r\n        association_result = self.calibration_module.associate_sensor_data(\r\n            rgb_img.shape[:2], lidar_points, imu_data\r\n        )\r\n        \r\n        if not association_result:\r\n            rospy.logwarn(\"Failed to associate sensor data\")\r\n            return None\r\n        \r\n        # Validate associations\r\n        validation_result = self.calibration_module.validate_calibration(association_result)\r\n        if not validation_result['valid']:\r\n            rospy.logwarn(f\"Association validation failed: {validation_result.get('error', 'Unknown error')}\")\r\n            return None\r\n        \r\n        self.associations = association_result\r\n        \r\n        # Prepare data for neural network\r\n        try:\r\n            # Convert data to tensors\r\n            rgb_tensor = torch.from_numpy(rgb_img).permute(2, 0, 1).float().unsqueeze(0) / 255.0\r\n            lidar_tensor = torch.from_numpy(lidar_points).float().unsqueeze(0)\r\n            \r\n            # IMU data\r\n            imu_accel = torch.from_numpy(imu_data['accel']).float().unsqueeze(0).unsqueeze(0)  # Add sequence dim\r\n            imu_gyro = torch.from_numpy(imu_data['gyro']).float().unsqueeze(0).unsqueeze(0)\r\n            imu_quat = torch.from_numpy(imu_data['quat']).float().unsqueeze(0).unsqueeze(0)\r\n            \r\n            # Run through fusion network\r\n            fusion_output = self.fusion_network(\r\n                rgb_tensor, lidar_tensor, \r\n                imu_accel, imu_gyro, imu_quat\r\n            )\r\n            \r\n            return fusion_output\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f\"Error in fusion network: {e}\")\r\n            return None\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,s.jsx)(r.p,{children:"Here's a complete multimodal fusion system that demonstrates the integration:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_multimodal_fusion_system.py\r\n\r\nimport torch\r\nimport numpy as np\r\nimport rospy\r\nimport cv2\r\nfrom sensor_msgs.msg import Image, PointCloud2, Imu\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Vector3, Quaternion\r\nfrom cv_bridge import CvBridge\r\nimport sensor_msgs.point_cloud2 as pc2\r\nfrom typing import Dict, Tuple\r\nimport time\r\n\r\n# Import the fusion modules we created\r\nfrom multimodal_dataloader import MultiModalDataLoader\r\nfrom crossmodal_fusion_network import MultiModalPerceptionFusion\r\nfrom calibration_module import SensorCalibrationModule, MultiModalFusionPipeline\r\n\r\nclass CompleteMultiModalFusionSystem:\r\n    """Complete multimodal fusion system for robotic perception"""\r\n    \r\n    def __init__(self):\r\n        rospy.init_node(\'complete_multimodal_fusion_system\', anonymous=True)\r\n        \r\n        # Initialize components\r\n        self.data_loader = MultiModalDataLoader()\r\n        self.calibration_module = SensorCalibrationModule()\r\n        self.fusion_network = MultiModalPerceptionFusion()\r\n        self.pipeline = MultiModalFusionPipeline()\r\n        \r\n        # Publishers\r\n        self.perception_pub = rospy.Publisher(\'/multimodal_perception\', String, queue_size=10)\r\n        self.fusion_status_pub = rospy.Publisher(\'/fusion_status\', String, queue_size=10)\r\n        \r\n        # CV Bridge for image conversions\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # System state\r\n        self.system_ready = False\r\n        self.last_fusion_time = 0\r\n        self.fusion_interval = 0.5  # seconds between fusion\r\n        self.confidence_threshold = 0.5  # minimum confidence for valid perception\r\n        \r\n        rospy.loginfo("Complete Multimodal Fusion System initialized")\r\n    \r\n    def run_fusion_cycle(self):\r\n        """Run one cycle of multimodal fusion"""\r\n        # Synchronize data from all modalities\r\n        synchronized_data = self.data_loader.synchronize_modalities()\r\n        \r\n        if synchronized_data is None:\r\n            rospy.logdebug("No synchronized data available")\r\n            return\r\n        \r\n        # Check if it\'s time for fusion (rate limiting)\r\n        current_time = time.time()\r\n        if current_time  self.last_fusion_time < self.fusion_interval:\r\n            return\r\n        \r\n        # Preprocess multimodal data\r\n        try:\r\n            processed_data = self.data_loader.preprocess_multimodal_data(synchronized_data)\r\n        except Exception as e:\r\n            rospy.logerr(f"Error preprocessing data: {e}")\r\n            return\r\n        \r\n        # Convert to tensors for neural network\r\n        try:\r\n            # Extract data\r\n            rgb_tensor = processed_data[\'rgb\']\r\n            lidar_tensor = processed_data[\'lidar\']\r\n            \r\n            # Extract IMU data\r\n            imu_data = processed_data[\'imu\']\r\n            imu_accel = imu_data[\'accel\']\r\n            imu_gyro = imu_data[\'gyro\']\r\n            imu_quat = imu_data[\'quat\']\r\n            \r\n            # Run fusion network\r\n            with torch.no_grad():\r\n                fusion_output = self.fusion_network(\r\n                    rgb_tensor, lidar_tensor,\r\n                    imu_accel, imu_gyro, imu_quat\r\n                )\r\n            \r\n            # Check confidence threshold\r\n            confidence = fusion_output[\'confidence\'].item()\r\n            if confidence < self.confidence_threshold:\r\n                rospy.logwarn(f"Fusion confidence below threshold: {confidence:.3f}")\r\n                return\r\n            \r\n            # Process fusion results\r\n            self.process_fusion_results(fusion_output, synchronized_data)\r\n            self.last_fusion_time = current_time\r\n            \r\n            rospy.loginfo(f"Fusion completed with confidence: {confidence:.3f}")\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error in fusion cycle: {e}")\r\n    \r\n    def process_fusion_results(self, fusion_output: Dict, raw_data: Dict):\r\n        """Process the fusion results and publish perception data"""\r\n        # Extract outputs\r\n        classification = torch.softmax(fusion_output[\'classification\'], dim=1)\r\n        detection = fusion_output[\'detection\']\r\n        segmentation = torch.softmax(fusion_output[\'segmentation\'], dim=1)\r\n        confidence = fusion_output[\'confidence\']\r\n        fused_features = fusion_output[\'fused_features\']\r\n        \r\n        # Prepare perception result\r\n        perception_result = {\r\n            \'timestamp\': rospy.get_rostime().to_sec(),\r\n            \'classification\': classification.cpu().numpy().tolist(),\r\n            \'detection\': detection.cpu().numpy().tolist(),\r\n            \'segmentation\': segmentation.cpu().numpy().tolist(),\r\n            \'confidence\': confidence.item(),\r\n            \'fused_features\': fused_features.cpu().numpy().tolist(),\r\n            \'raw_data_timestamps\': {\r\n                \'rgb\': raw_data[\'rgb_timestamp\'],\r\n                \'lidar\': raw_data[\'lidar_timestamp\'],\r\n                \'imu\': raw_data[\'imu_timestamp\']\r\n            }\r\n        }\r\n        \r\n        # Publish perception result\r\n        result_msg = String()\r\n        result_msg.data = json.dumps(perception_result, indent=2)\r\n        self.perception_pub.publish(result_msg)\r\n        \r\n        # Log important detections\r\n        top_class_idx = torch.argmax(classification, dim=1).item()\r\n        top_class_prob = torch.max(classification, dim=1)[0].item()\r\n        \r\n        rospy.loginfo(f"Perception: Class {top_class_idx} with probability {top_class_prob:.3f}")\r\n    \r\n    def run(self):\r\n        """Main loop for multimodal fusion system"""\r\n        rospy.loginfo("Starting multimodal fusion system...")\r\n        \r\n        # Calibrate sensors if needed\r\n        if not self.calibration_module.is_calibrated:\r\n            rospy.loginfo("Calibrating sensors...")\r\n            if self.calibration_module.calibrate_sensors():\r\n                rospy.loginfo("Sensor calibration completed successfully")\r\n            else:\r\n                rospy.logerr("Sensor calibration failed")\r\n                return\r\n        \r\n        rate = rospy.Rate(10)  # 10 Hz fusion rate (adjustable based on compute power)\r\n        \r\n        while not rospy.is_shutdown():\r\n            try:\r\n                # Run fusion cycle\r\n                self.run_fusion_cycle()\r\n                \r\n                # Publish system status\r\n                status_msg = String()\r\n                status_msg.data = f"Fusion system active. Last fusion: {time.time()  self.last_fusion_time:.2f}s ago"\r\n                self.fusion_status_pub.publish(status_msg)\r\n                \r\n                rate.sleep()\r\n                \r\n            except KeyboardInterrupt:\r\n                rospy.loginfo("Shutting down multimodal fusion system...")\r\n                break\r\n            except Exception as e:\r\n                rospy.logerr(f"Error in main loop: {e}")\r\n                continue\r\n\r\ndef main():\r\n    """Main function to run the complete system"""\r\n    system = CompleteMultiModalFusionSystem()\r\n    \r\n    try:\r\n        system.run()\r\n    except rospy.ROSInterruptException:\r\n        rospy.loginfo("Multimodal fusion system interrupted")\r\n    except Exception as e:\r\n        rospy.logerr(f"Fatal error in fusion system: {e}")\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"launch-file-for-the-multimodal-fusion-system",children:"Launch file for the multimodal fusion system:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<launch>\r\n  <! Multimodal Fusion System >\r\n  <node name="multimodal_fusion_system" pkg="robot_perception" type="complete_multimodal_fusion_system.py" output="screen">\r\n    <! Parameters for fusion >\r\n    <param name="fusion_interval" value="0.5"/>\r\n    <param name="confidence_threshold" value="0.5"/>\r\n  </node>\r\n  \r\n  <! Example sensor nodes (these should be running) >\r\n  <group ns="sensors">\r\n    <node name="camera_driver" pkg="usb_cam" type="usb_cam_node" output="screen">\r\n      <param name="video_device" value="/dev/video0"/>\r\n      <param name="image_width" value="640"/>\r\n      <param name="image_height" value="480"/>\r\n      <param name="pixel_format" value="yuyv"/>\r\n    </node>\r\n    \r\n    <node name="lidar_driver" pkg="velodyne_driver" type="velodyne_node" output="screen">\r\n      <param name="device_ip" value="192.168.1.201"/>\r\n      <param name="port" value="2368"/>\r\n    </node>\r\n    \r\n    <node name="imu_driver" pkg="razor_imu_9dof" type="imu_node" output="screen"/>\r\n  </group>\r\n  \r\n  <! TF transforms for sensors >\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.1 0.0 0.3 0.0 0.0 0.0 base_link camera_link" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.1 0.0 0.1 0.0 0.0 0.0 base_link lidar_link" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.0 0.0 0.2 0.0 0.0 0.0 base_link imu_link" />\r\n</launch>\n'})}),"\n",(0,s.jsx)(r.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,s.jsx)(r.p,{children:"Create a complete multimodal perception fusion system that:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Implements sensor data synchronization and calibration"}),"\n",(0,s.jsx)(r.li,{children:"Fuses RGB camera, LiDAR, and IMU data using neural networks"}),"\n",(0,s.jsx)(r.li,{children:"Implements transformerbased crossmodal attention mechanisms"}),"\n",(0,s.jsx)(r.li,{children:"Creates taskspecific heads for classification, detection, and segmentation"}),"\n",(0,s.jsx)(r.li,{children:"Designs confidence estimation for fusion outputs"}),"\n",(0,s.jsx)(r.li,{children:"Implements robustness mechanisms for sensor failures"}),"\n",(0,s.jsx)(r.li,{children:"Evaluates fusion performance against singlemodal baselines"}),"\n",(0,s.jsx)(r.li,{children:"Demonstrates the system in a robotic navigation scenario"}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"Your project should include:\r\nComplete multimodal data pipeline with synchronization\r\nCrossmodal attention fusion network\r\nSensor calibration and data association module\r\nRobust perception system with failure recovery\r\nPerformance evaluation metrics\r\nComparative analysis with singlemodal approaches\r\nNavigation demonstration using fused perception"}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(r.p,{children:"This chapter covered multimodal perception fusion for robotics:"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Sensor Synchronization"}),": Techniques for synchronizing data from different sensors\r\n",(0,s.jsx)(r.strong,{children:"CrossModal Attention"}),": Transformerbased mechanisms for fusing information across modalities\r\n",(0,s.jsx)(r.strong,{children:"Calibration"}),": Procedures for determining spatial relationships between sensors\r\n",(0,s.jsx)(r.strong,{children:"Deep Fusion"}),": Neural architectures that learn to combine multimodal information\r\n",(0,s.jsx)(r.strong,{children:"Confidence Estimation"}),": Mechanisms to assess the quality of fused perceptions\r\n",(0,s.jsx)(r.strong,{children:"Robustness"}),": Handling sensor failures and degraded conditions\r\n",(0,s.jsx)(r.strong,{children:"Evaluation"}),": Metrics for assessing the benefits of multimodal fusion"]}),"\n",(0,s.jsx)(r.p,{children:"Multimodal perception fusion enables robots to gain a more comprehensive understanding of their environment, leading to improved robustness and performance in challenging conditions where single sensors may fail or provide inadequate information."})]})}function c(n={}){const{wrapper:r}={...(0,a.R)(),...n.components};return r?(0,s.jsx)(r,{...n,children:(0,s.jsx)(u,{...n})}):u(n)}}}]);