"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[9833],{8966:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>_,frontMatter:()=>i,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"module-4-vision-language-action/ch20-voice-plan-navigate-manipulate","title":"ch20-voice-plan-navigate-manipulate","description":"-----","source":"@site/docs/module-4-vision-language-action/ch20-voice-plan-navigate-manipulate.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch20-voice-plan-navigate-manipulate","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch20-voice-plan-navigate-manipulate","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch20-voice-plan-navigate-manipulate.md","tags":[],"version":"current","frontMatter":{}}');var a=r(4848),o=r(8453),s=r(7242);const i={},c=void 0,l={},p=[{value:"title: Ch20  Capstone  Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate\r\nmodule: 4\r\nchapter: 20\r\nsidebar_label: Ch20: Capstone  Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate\r\ndescription: Complete capstone project integrating voice control, cognitive planning, navigation, and manipulation\r\ntags: [capstone, integration, voicecontrol, planning, navigation, manipulation, airobotics]\r\ndifficulty: advanced\r\nestimated_duration: 240",id:"title-ch20--capstone--voice--plan--navigate--manipulatemodule-4chapter-20sidebar_label-ch20-capstone--voice--plan--navigate--manipulatedescription-complete-capstone-project-integrating-voice-control-cognitive-planning-navigation-and-manipulationtags-capstone-integration-voicecontrol-planning-navigation-manipulation-airoboticsdifficulty-advancedestimated_duration-240",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"Integrated VoicetoAction Pipeline",id:"integrated-voicetoaction-pipeline",level:3},{value:"Integration Challenges",id:"integration-challenges",level:3},{value:"Architecture Patterns",id:"architecture-patterns",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Building the VoicetoAction Pipeline",id:"lab-1-building-the-voicetoaction-pipeline",level:3},{value:"Lab 2: Creating a Safety Validation Layer",id:"lab-2-creating-a-safety-validation-layer",level:3},{value:"Lab 3: Implementing the Complete Integration System",id:"lab-3-implementing-the-complete-integration-system",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Running the Complete System",id:"running-the-complete-system",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"title-ch20--capstone--voice--plan--navigate--manipulatemodule-4chapter-20sidebar_label-ch20-capstone--voice--plan--navigate--manipulatedescription-complete-capstone-project-integrating-voice-control-cognitive-planning-navigation-and-manipulationtags-capstone-integration-voicecontrol-planning-navigation-manipulation-airoboticsdifficulty-advancedestimated_duration-240",children:"title: Ch20  Capstone  Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate\r\nmodule: 4\r\nchapter: 20\r\nsidebar_label: Ch20: Capstone  Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate\r\ndescription: Complete capstone project integrating voice control, cognitive planning, navigation, and manipulation\r\ntags: [capstone, integration, voicecontrol, planning, navigation, manipulation, airobotics]\r\ndifficulty: advanced\r\nestimated_duration: 240"}),"\n","\n",(0,a.jsx)(n.h1,{id:"capstone-voice--plan--navigate--manipulate",children:"Capstone: Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate"}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"Integrate voice recognition, cognitive planning, navigation, and manipulation systems\r\nCreate endtoend robotic systems from voice command to physical action\r\nImplement safety validation throughout the pipeline\r\nDesign multimodal interfaces connecting language, vision, and action\r\nEvaluate system performance for complex voicecontrolled tasks\r\nHandle error recovery and graceful degradation in multicomponent systems\r\nCreate comprehensive logging and debugging systems\r\nDesign userfriendly interfaces for complex robotic systems"}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.h3,{id:"integrated-voicetoaction-pipeline",children:"Integrated VoicetoAction Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"The complete voicetoaction system involves multiple interconnected components that need to work seamlessly together. Each component affects the others, requiring careful integration and validation."}),"\n",(0,a.jsx)(s.A,{chart:"\ngraph TD;\n  A[User Voice Command] > B[Whisper Transcription];\n  B > C[Command Parsing];\n  C > D[GPT4o Task Planning];\n  D > E[Hierarchical Plan];\n  \n  F[Visual Input] > G[Object Detection];\n  G > H[Environment Mapping];\n  H > I[Context Integration];\n  \n  J[Navigation System] > K[Path Planning];\n  K > L[Obstacle Avoidance];\n  L > M[Safe Navigation];\n  \n  N[Manipulation System] > O[Grasp Planning];\n  O > P[Collision Checking];\n  P > Q[Safe Manipulation];\n  \n  E > R[Plan Validation];\n  I > R;\n  R > S[Execution Monitoring];\n  \n  S > T[Navigation Execution];\n  S > U[Manipulation Execution];\n  \n  T > V[Safety Validation];\n  U > V;\n  V > W[Physical Robot Action];\n  \n  W > X[Feedback Generation];\n  X > Y[Voice Response];\n  X > Z[Visual Feedback];\n  \n  E > AA[Recovery Planning];\n  AA > BB[Alternative Actions];\n  BB > W;\n  \n  style A fill:#E91E63,stroke:#AD1457,color:#fff;\n  style W fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style Y fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style Z fill:#FF9800,stroke:#E65100,color:#fff;\n  style D fill:#9C27B0,stroke:#4A148C,color:#fff;\n  style V fill:#FF5722,stroke:#BF360C,color:#fff;\n"}),"\n",(0,a.jsx)(n.h3,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Latency Management"}),": Each component adds processing time; the overall system must maintain responsiveness."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"State Consistency"}),": Different components may have different views of the world state; synchronization is critical."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Error Propagation"}),": Errors in early stages (voice recognition) cascade to later stages (action execution)."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Safety Validation"}),": Each planned action must be validated for safety considering all available context."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Context Awareness"}),": The system must maintain and update context across all processing stages."]}),"\n",(0,a.jsx)(n.h3,{id:"architecture-patterns",children:"Architecture Patterns"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"EventDriven Architecture"}),": Components communicate through event messages, allowing loose coupling and independent failure handling."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"State Machine Architecture"}),": Explicit state management for complex multistep processes."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Layered Architecture"}),": Clear separation between sensory input, cognitive processing, and action execution."]}),"\n",(0,a.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,a.jsx)(n.h3,{id:"lab-1-building-the-voicetoaction-pipeline",children:"Lab 1: Building the VoicetoAction Pipeline"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create the main integration node"})," (",(0,a.jsx)(n.code,{children:"voice_action_integration.py"}),"):","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport openai\r\nimport whisper\r\nimport pyaudio\r\nimport numpy as np\r\nimport json\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist, Pose, Point\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom geometry_msgs.msg import PoseStamped, PointStamped\r\nfrom actionlib_msgs.msg import GoalStatusArray\r\nfrom typing import Dict, List, Optional, Any\r\nimport torch\r\n\r\nclass VoiceActionIntegration:\r\n    def __init__(self, api_key: str, whisper_model_size: str = "base"):\r\n        # Initialize ROS\r\n        rospy.init_node(\'voice_action_integration\', anonymous=True)\r\n        \r\n        # Initialize Whisper\r\n        self.whisper_model = whisper.load_model(whisper_model_size)\r\n        \r\n        # Initialize OpenAI client\r\n        self.openai_client = openai.OpenAI(api_key=api_key)\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.goal_pub = rospy.Publisher(\'/move_base_simple/goal\', PoseStamped, queue_size=10)\r\n        self.system_status_pub = rospy.Publisher(\'/system_status\', String, queue_size=10)\r\n        self.voice_response_pub = rospy.Publisher(\'/voice_response\', String, queue_size=10)\r\n        self.action_feedback_pub = rospy.Publisher(\'/action_feedback\', String, queue_size=10)\r\n        \r\n        # Subscribers\r\n        rospy.Subscriber(\'/voice_command\', String, self.voice_command_callback)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.scan_callback)\r\n        rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\r\n        rospy.Subscriber(\'/move_base/status\', GoalStatusArray, self.navigation_status_callback)\r\n        rospy.Subscriber(\'/action_execution_status\', String, self.action_status_callback)\r\n        \r\n        # Internal state\r\n        self.laser_data = None\r\n        self.image_data = None\r\n        self.robot_position = Point(x=0.0, y=0.0, z=0.0)\r\n        self.navigation_active = False\r\n        self.action_queue = queue.Queue()\r\n        self.state_lock = threading.Lock()\r\n        \r\n        # System configuration\r\n        self.safe_distance = rospy.get_param(\'~safe_distance\', 0.5)\r\n        self.max_linear_speed = rospy.get_param(\'~max_linear_speed\', 0.3)\r\n        self.max_angular_speed = rospy.get_param(\'~max_angular_speed\', 0.5)\r\n        self.command_timeout = rospy.get_param(\'~command_timeout\', 30.0)\r\n        \r\n        # Command processing\r\n        self.command_processing_thread = threading.Thread(target=self.process_commands)\r\n        self.command_processing_thread.daemon = True\r\n        self.command_processing_thread.start()\r\n        \r\n        rospy.loginfo("VoiceAction Integration System initialized")\r\n    \r\n    def scan_callback(self, msg: LaserScan):\r\n        """Update laser scan data"""\r\n        with self.state_lock:\r\n            self.laser_data = msg\r\n    \r\n    def image_callback(self, msg: Image):\r\n        """Update image data"""\r\n        with self.state_lock:\r\n            self.image_data = msg\r\n    \r\n    def navigation_status_callback(self, msg: GoalStatusArray):\r\n        """Update navigation status"""\r\n        with self.state_lock:\r\n            if msg.status_list:\r\n                # Check the status of the most recent goal (last in list)\r\n                last_status = msg.status_list[1]\r\n                if last_status.status == 3:  # Succeeded\r\n                    self.navigation_active = False\r\n                elif last_status.status in [1, 2, 4, 5, 8]:  # Active/Aborted/etc.\r\n                    self.navigation_active = True\r\n    \r\n    def action_status_callback(self, msg: String):\r\n        """Handle action execution status"""\r\n        try:\r\n            status_data = json.loads(msg.data)\r\n            action_id = status_data.get("action_id", "unknown")\r\n            status = status_data.get("status", "unknown")\r\n            \r\n            if status == "failure":\r\n                rospy.logerr(f"Action {action_id} failed: {status_data.get(\'error\', \'Unknown error\')}")\r\n                self.handle_action_failure(action_id, status_data.get("error", "Unknown error"))\r\n            elif status == "success":\r\n                rospy.loginfo(f"Action {action_id} completed successfully")\r\n                self.publish_feedback(f"Action {action_id} completed successfully")\r\n            \r\n        except json.JSONDecodeError:\r\n            rospy.logerr("Invalid JSON in action status message")\r\n    \r\n    def voice_command_callback(self, msg: String):\r\n        """Process incoming voice command"""\r\n        command_text = msg.data\r\n        rospy.loginfo(f"Received voice command: {command_text}")\r\n        \r\n        # Add command to processing queue\r\n        command_item = {\r\n            "text": command_text,\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "processed": False,\r\n            "attempts": 0,\r\n            "max_attempts": 3\r\n        }\r\n        \r\n        self.action_queue.put(command_item)\r\n        self.publish_status(f"Command received: {command_text[:50]}...")\r\n    \r\n    def process_commands(self):\r\n        """Process commands from the queue in a separate thread"""\r\n        while not rospy.is_shutdown():\r\n            try:\r\n                command_item = self.action_queue.get(timeout=1.0)\r\n                \r\n                # Process the command\r\n                success = self.process_single_command(command_item["text"])\r\n                \r\n                if not success:\r\n                    if command_item["attempts"] < command_item["max_attempts"]:\r\n                        # Put back in queue with incremented attempt count\r\n                        command_item["attempts"] += 1\r\n                        rospy.logwarn(f"Command failed, retrying ({command_item[\'attempts\']}/{command_item[\'max_attempts\']})")\r\n                        time.sleep(1.0)  # Short delay before retry\r\n                        self.action_queue.put(command_item)\r\n                    else:\r\n                        rospy.logerr(f"Command failed after {command_item[\'max_attempts\']} attempts: {command_item[\'text\']}")\r\n                        self.publish_feedback(f"Command failed after maximum attempts: {command_item[\'text\']}")\r\n                \r\n                self.action_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command: {e}")\r\n                continue\r\n    \r\n    def process_single_command(self, command_text: str) > bool:\r\n        """Process a single command through the entire pipeline"""\r\n        rospy.loginfo(f"Processing command: {command_text}")\r\n        \r\n        # Step 1: Contextual understanding\r\n        context = self.get_environment_context()\r\n        \r\n        # Step 2: Use GPT4o to parse command and generate plan\r\n        plan = self.generate_action_plan(command_text, context)\r\n        \r\n        if not plan:\r\n            rospy.logerr("Failed to generate action plan")\r\n            return False\r\n        \r\n        rospy.loginfo(f"Generated plan with {len(plan.get(\'actions\', []))} actions")\r\n        \r\n        # Step 3: Validate plan for safety\r\n        if not self.validate_plan_safety(plan):\r\n            rospy.logerr("Plan failed safety validation")\r\n            self.publish_feedback("Command plan rejected: Safety validation failed")\r\n            return False\r\n        \r\n        # Step 4: Execute plan step by step\r\n        execution_success = self.execute_plan(plan)\r\n        \r\n        if execution_success:\r\n            rospy.loginfo("Command plan executed successfully")\r\n            self.publish_feedback(f"Command completed successfully: {command_text[:30]}...")\r\n            return True\r\n        else:\r\n            rospy.logerr("Command plan execution failed")\r\n            return False\r\n    \r\n    def get_environment_context(self) > Dict:\r\n        """Get current environment context"""\r\n        with self.state_lock:\r\n            context = {\r\n                "robot_state": {\r\n                    "position": {"x": self.robot_position.x, "y": self.robot_position.y},\r\n                    "battery_level": 0.85,  # Would come from actual robot state in real implementation\r\n                    "navigation_status": "ready" if not self.navigation_active else "executing"\r\n                },\r\n                "environment": {\r\n                    "obstacles": self.get_obstacle_information(),\r\n                    "known_locations": self.get_known_locations(),\r\n                    "detected_objects": self.get_detected_objects(),\r\n                    "navigation_map": None  # Would come from map server\r\n                },\r\n                "time_context": {\r\n                    "hour_of_day": rospy.Time.now().to_sec() % 86400 / 3600,\r\n                    "day_of_week": int(rospy.Time.now().to_sec() % 604800 / 86400)\r\n                },\r\n                "constraints": {\r\n                    "safe_distance": self.safe_distance,\r\n                    "max_speed": self.max_linear_speed,\r\n                    "max_angular_speed": self.max_angular_speed\r\n                }\r\n            }\r\n            return context\r\n    \r\n    def get_obstacle_information(self) > List[Dict]:\r\n        """Extract obstacle information from laser scan"""\r\n        if not self.laser_data:\r\n            return []\r\n        \r\n        # Analyze laser scan for obstacles\r\n        obstacles = []\r\n        ranges = self.laser_data.ranges\r\n        angle_min = self.laser_data.angle_min\r\n        angle_increment = self.laser_data.angle_increment\r\n        \r\n        # Sample every 10th point to reduce computation\r\n        for i in range(0, len(ranges), 10):\r\n            if not (np.isinf(ranges[i]) or np.isnan(ranges[i])):\r\n                angle = angle_min + i * angle_increment\r\n                x = ranges[i] * np.cos(angle)\r\n                y = ranges[i] * np.sin(angle)\r\n                \r\n                if ranges[i] < self.safe_distance * 2:  # Consider as potential obstacle\r\n                    obstacles.append({\r\n                        "x": x,\r\n                        "y": y,\r\n                        "distance": ranges[i],\r\n                        "angle": angle\r\n                    })\r\n        \r\n        return obstacles\r\n    \r\n    def get_known_locations(self) > List[Dict]:\r\n        """Get known locations (would come from map or localization)"""\r\n        # In real implementation, this would query a location database\r\n        return [\r\n            {"name": "kitchen", "x": 2.0, "y": 1.0},\r\n            {"name": "living_room", "x": 1.0, "y": 1.0},\r\n            {"name": "bedroom", "x": 1.0, "y": 1.5},\r\n            {"name": "office", "x": 0.5, "y": 2.0},\r\n            {"name": "charging_station", "x": 2.0, "y": 2.0}\r\n        ]\r\n    \r\n    def get_detected_objects(self) > List[Dict]:\r\n        """Get detected objects (would come from perception system)"""\r\n        # In real implementation, this would come from object detection\r\n        # For this example, return empty list (would be populated by vision system)\r\n        if self.image_data is not None:\r\n            # Simulate object detection from image\r\n            # This is a placeholder  real implementation would use actual object detection\r\n            return [\r\n                {"name": "coffee_cup", "type": "container", "location": "kitchen", "distance": 1.5, "confidence": 0.9},\r\n                {"name": "book", "type": "stationery", "location": "living_room", "distance": 2.0, "confidence": 0.8}\r\n            ]\r\n        return []\r\n    \r\n    def generate_action_plan(self, command: str, context: Dict) > Optional[Dict]:\r\n        """Generate action plan using GPT4o based on command and context"""\r\n        prompt = f"""\r\n        User Command: "{command}"\r\n        \r\n        Environment Context: {json.dumps(context, indent=2)}\r\n        \r\n        Available Actions:\r\n         navigate_to(location_name): Move robot to named location\r\n         approach_object(object_name): Move robot to specific object\r\n         inspect_object(object_name): Examine an object with sensors\r\n         grasp_object(object_name): Pick up an object (if manipulator equipped)\r\n         place_object(object_name, location): Place object at location\r\n         open_container(container_name): Open a container/door\r\n         close_container(container_name): Close a container/door\r\n         follow_person(person_name): Follow a person for some distance\r\n         wait(duration_seconds): Pause execution for specified duration\r\n         speak_response(text): Speak text response to user\r\n         find_person(person_name): Locate a specific person\r\n         escort_person(destination): Guide a person to destination\r\n         patrol_area(area_name): Move through predefined area\r\n         charge_robot(): Return to charging station\r\n        \r\n        Generate a detailed action plan that:\r\n        1. Breaks down the command into specific actions\r\n        2. Considers the environmental context\r\n        3. Respects robot capabilities and constraints\r\n        4. Includes safety checks before movement\r\n        5. Handles potential failure modes\r\n        6. Verifies completion of each step\r\n        \r\n        The plan should be in JSON format:\r\n        {{\r\n          "command_original": "{command}",\r\n          "reasoning": "stepbystep reasoning about the plan",\r\n          "actions": [\r\n            {{\r\n              "id": 1,\r\n              "type": "action_type",\r\n              "parameters": {{"param1": "value1", "param2": "value2"}},\r\n              "description": "what this action accomplishes",\r\n              "preconditions": ["condition1", "condition2"],\r\n              "expected_effects": ["effect1", "effect2"],\r\n              "safety_check_needed": true,\r\n              "estimated_duration": 15.0,\r\n              "success_probability": 0.9\r\n            }}\r\n          ],\r\n          "estimated_total_duration": 60.0,\r\n          "overall_confidence": 0.75,\r\n          "failure_recovery": [\r\n            {{"condition": "obstacle_encountered", "action": "navigation_failed_recovery_plan"}},\r\n            {{"condition": "object_not_found", "action": "search_alternative_objects"}},\r\n            {{"condition": "grasp_failed", "action": "retry_grasp_with_different_approach"}}\r\n          ]\r\n        }}\r\n        \r\n        Respond with ONLY the JSON object, no other text.\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4o",\r\n                messages=[\r\n                    {\r\n                        "role": "system",\r\n                        "content": """You are an expert in robot task planning. Generate feasible plans that consider robot capabilities, environmental constraints, and safety. Respond only with valid JSON as specified."""\r\n                    },\r\n                    {\r\n                        "role": "user", \r\n                        "content": prompt\r\n                    }\r\n                ],\r\n                temperature=0.1,\r\n                max_tokens=2000\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON if wrapped in code block\r\n            if response_text.startswith(\'```\'):\r\n                start_idx = response_text.find(\'{\')\r\n                end_idx = response_text.rfind(\'}\') + 1\r\n                if start_idx != 1 and end_idx != 1:\r\n                    response_text = response_text[start_idx:end_idx]\r\n            \r\n            plan_data = json.loads(response_text)\r\n            return plan_data\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error generating action plan: {e}")\r\n            return None\r\n    \r\n    def validate_plan_safety(self, plan: Dict) > bool:\r\n        """Validate the safety of the generated plan"""\r\n        actions = plan.get("actions", [])\r\n        \r\n        for action in actions:\r\n            action_type = action.get("type", "")\r\n            \r\n            # Check navigation safety\r\n            if action_type in ["navigate_to", "approach_object"]:\r\n                params = action.get("parameters", {})\r\n                \r\n                # Check if destination is safe based on current environment\r\n                if "location" in params:\r\n                    if not self.is_safe_navigation_destination(params["location"]):\r\n                        rospy.logerr(f"Safety check failed for destination: {params[\'location\']}")\r\n                        return False\r\n                elif "object_name" in params:\r\n                    object_name = params["object_name"]\r\n                    if not self.is_safe_to_approach_object(object_name):\r\n                        rospy.logerr(f"Safety check failed for approaching object: {object_name}")\r\n                        return False\r\n            \r\n            # Check manipulation safety\r\n            elif action_type in ["grasp_object", "place_object"]:\r\n                if not self.are_manipulation_safeties_met():\r\n                    rospy.logerr("Safety check failed for manipulation action")\r\n                    return False\r\n        \r\n        return True\r\n    \r\n    def is_safe_navigation_destination(self, location_name: str) > bool:\r\n        """Check if navigation destination is safe"""\r\n        # In real implementation, this would check:\r\n        #  Known map for obstacles\r\n        #  Dynamic obstacles from sensors\r\n        #  Reachability of the location\r\n        #  Safety of the area\r\n        \r\n        known_locations = self.get_known_locations()\r\n        target_location = next((loc for loc in known_locations if loc["name"] == location_name), None)\r\n        \r\n        if not target_location:\r\n            rospy.logwarn(f"Unknown location: {location_name}")\r\n            return False\r\n        \r\n        # Check for obstacles in path\r\n        if self.laser_data:\r\n            # Simple check: ensure path is mostly clear\r\n            front_ranges = self.laser_data.ranges[len(self.laser_data.ranges)//220:len(self.laser_data.ranges)//2+20]\r\n            valid_ranges = [r for r in front_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            \r\n            if valid_ranges and min(valid_ranges) < 0.5:  # Dangerous to move if obstacle within 0.5m\r\n                rospy.logwarn(f"Dangerous obstacle detected en route to {location_name}")\r\n                return False\r\n        \r\n        return True\r\n    \r\n    def is_safe_to_approach_object(self, object_name: str) > bool:\r\n        """Check if it\'s safe to approach an object"""\r\n        # Check environment context for the object\r\n        detected_objects = self.get_detected_objects()\r\n        target_object = next((obj for obj in detected_objects if obj["name"] == object_name), None)\r\n        \r\n        if target_object:\r\n            # Check if object is within safe distance range\r\n            if target_object["distance"] > 3.0:  # Too far to reliably approach\r\n                rospy.logwarn(f"Object {object_name} is too far: {target_object[\'distance\']:.2f}m")\r\n                return False\r\n            \r\n            if target_object["distance"] < 0.2:  # Too close already\r\n                rospy.loginfo(f"Already close to object {object_name}")\r\n                return True  # This is OK, just need to stay put\r\n            \r\n            # Check for obstacles in path to object\r\n            if self.laser_data:\r\n                # Estimate object bearing based on approximate location\r\n                # This is a simplification in the example\r\n                return True  # In real implementation, verify path is clear\r\n        \r\n        # If object not detected, we can\'t safely approach\r\n        rospy.logwarn(f"Cannot safely approach undetected object: {object_name}")\r\n        return False\r\n    \r\n    def are_manipulation_safeties_met(self) > bool:\r\n        """Check if manipulator safety conditions are met"""\r\n        # In real implementation, this would check:\r\n        #  Robot position (is it in a safe area for manipulation?)\r\n        #  Object detectability (can we see the object clearly?)\r\n        #  Manipulator status (is it ready?)\r\n        #  Surroundings (are there people nearby?)\r\n        return True  # Placeholder value\r\n    \r\n    def execute_plan(self, plan: Dict) > bool:\r\n        """Execute the action plan"""\r\n        actions = plan.get("actions", [])\r\n        rospy.loginfo(f"Starting execution of plan with {len(actions)} actions")\r\n        \r\n        for i, action in enumerate(actions):\r\n            rospy.loginfo(f"Executing action {i+1}/{len(actions)}: {action.get(\'type\', \'unknown\')}")\r\n            \r\n            # Publish execution status\r\n            status_msg = String()\r\n            status_msg.data = json.dumps({\r\n                "action_id": action.get("id", "unknown"),\r\n                "action_type": action.get("type", "unknown"),\r\n                "progress": f"{i+1}/{len(actions)}",\r\n                "status": "executing"\r\n            })\r\n            self.action_feedback_pub.publish(status_msg)\r\n            \r\n            # Execute action\r\n            success = self.execute_single_action(action)\r\n            \r\n            if not success:\r\n                rospy.logerr(f"Action execution failed: {action}")\r\n                \r\n                # Try recovery based on plan\r\n                recovery_success = self.attempt_recovery(plan, i, action)\r\n                if not recovery_success:\r\n                    rospy.logerr("Recovery attempt failed, terminating plan")\r\n                    return False\r\n            \r\n            # Small delay between actions\r\n            rospy.sleep(0.5)\r\n        \r\n        rospy.loginfo("Plan execution completed successfully")\r\n        return True\r\n    \r\n    def execute_single_action(self, action: Dict) > bool:\r\n        """Execute a single action"""\r\n        action_type = action.get("type", "")\r\n        parameters = action.get("parameters", {})\r\n        \r\n        if action_type == "navigate_to":\r\n            return self.execute_navigation_action(parameters)\r\n        elif action_type == "approach_object":\r\n            return self.execute_approach_action(parameters)\r\n        elif action_type == "inspect_object":\r\n            return self.execute_inspection_action(parameters)\r\n        elif action_type == "grasp_object":\r\n            return self.execute_grasp_action(parameters)\r\n        elif action_type == "place_object":\r\n            return self.execute_place_action(parameters)\r\n        elif action_type == "speak_response":\r\n            return self.execute_speak_action(parameters)\r\n        elif action_type == "wait":\r\n            return self.execute_wait_action(parameters)\r\n        elif action_type == "open_container":\r\n            return self.execute_open_container_action(parameters)\r\n        elif action_type == "close_container":\r\n            return self.execute_close_container_action(parameters)\r\n        else:\r\n            rospy.logwarn(f"Unknown action type: {action_type}")\r\n            return True  # Don\'t fail on unknown actions, just skip them\r\n    \r\n    def execute_navigation_action(self, params: Dict) > bool:\r\n        """Execute navigation action"""\r\n        location_name = params.get("location")\r\n        \r\n        if not location_name:\r\n            rospy.logerr("Navigation action missing location parameter")\r\n            return False\r\n        \r\n        # Find the location in known locations\r\n        known_locations = self.get_known_locations()\r\n        target_location = next((loc for loc in known_locations if loc["name"] == location_name), None)\r\n        \r\n        if not target_location:\r\n            rospy.logerr(f"Unknown location: {location_name}")\r\n            return False\r\n        \r\n        # Create and publish navigation goal\r\n        goal = PoseStamped()\r\n        goal.header.frame_id = "map"\r\n        goal.header.stamp = rospy.Time.now()\r\n        goal.pose.position.x = target_location["x"]\r\n        goal.pose.position.y = target_location["y"]\r\n        goal.pose.position.z = 0.0\r\n        \r\n        # Use simple quaternion (0, 0, 0, 1) for no rotation\r\n        goal.pose.orientation.w = 1.0\r\n        \r\n        self.goal_pub.publish(goal)\r\n        rospy.loginfo(f"Navigation goal sent to {location_name} at ({target_location[\'x\']}, {target_location[\'y\']})")\r\n        \r\n        # Wait for navigation to complete (with timeout)\r\n        start_time = rospy.Time.now().to_sec()\r\n        timeout = 60.0  # 1 minute timeout\r\n        \r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        while (rospy.Time.now().to_sec()  start_time) < timeout:\r\n            if not self.navigation_active:\r\n                rospy.loginfo(f"Navigation to {location_name} completed successfully")\r\n                return True\r\n            rate.sleep()\r\n        \r\n        rospy.logerr(f"Navigation to {location_name} timed out after {timeout} seconds")\r\n        return False\r\n    \r\n    def execute_approach_action(self, params: Dict) > bool:\r\n        """Execute object approach action"""\r\n        object_name = params.get("object_name")\r\n        \r\n        if not object_name:\r\n            rospy.logerr("Approach action missing object_name parameter")\r\n            return False\r\n        \r\n        # Get object location from perception system\r\n        detected_objects = self.get_detected_objects()\r\n        target_object = next((obj for obj in detected_objects if obj["name"] == object_name), None)\r\n        \r\n        if not target_object:\r\n            rospy.logerr(f"Object {object_name} not detected")\r\n            return False\r\n        \r\n        # Approach strategy: navigate to a position 0.5m in front of the object\r\n        # This is simplified  real implementation would require more complex path planning\r\n        object_dist = target_object["distance"]\r\n        if object_dist > 0.5:  # Only navigate if object is farther than our target distance\r\n            # Calculate target position (simplified  assumes object is in front of robot)\r\n            target_x = target_object["x"] if "x" in target_object else 0.5\r\n            target_y = target_object["y"] if "y" in target_object else 0.0\r\n            \r\n            goal = PoseStamped()\r\n            goal.header.frame_id = "map"\r\n            goal.header.stamp = rospy.Time.now()\r\n            goal.pose.position.x = target_x\r\n            goal.pose.position.y = target_y\r\n            goal.pose.position.z = 0.0\r\n            goal.pose.orientation.w = 1.0\r\n            \r\n            self.goal_pub.publish(goal)\r\n            rospy.loginfo(f"Approach goal sent for {object_name}")\r\n            \r\n            # Wait for navigation to complete\r\n            start_time = rospy.Time.now().to_sec()\r\n            timeout = 30.0  # 30 second timeout for approach\r\n            \r\n            rate = rospy.Rate(10)  # 10 Hz\r\n            while (rospy.Time.now().to_sec()  start_time) < timeout:\r\n                if not self.navigation_active:\r\n                    rospy.loginfo(f"Approach to {object_name} completed")\r\n                    return True\r\n                rate.sleep()\r\n            \r\n            rospy.logerr(f"Approach to {object_name} timed out")\r\n            return False\r\n        else:\r\n            # Already close enough to object\r\n            rospy.loginfo(f"Already close enough to {object_name}")\r\n            return True\r\n    \r\n    def execute_speak_action(self, params: Dict) > bool:\r\n        """Execute speech action"""\r\n        text = params.get("text", "")\r\n        if text:\r\n            self.publish_feedback(f"Speaking: {text}")\r\n            rospy.loginfo(f"Speaking: {text}")\r\n            return True\r\n        else:\r\n            rospy.logwarn("Speak action with empty text")\r\n            return False\r\n    \r\n    def execute_wait_action(self, params: Dict) > bool:\r\n        """Execute wait action"""\r\n        duration = params.get("duration", 1.0)\r\n        rospy.loginfo(f"Waiting for {duration} seconds")\r\n        rospy.sleep(duration)\r\n        return True\r\n    \r\n    def execute_inspection_action(self, params: Dict) > bool:\r\n        """Execute object inspection action"""\r\n        object_name = params.get("object_name")\r\n        rospy.loginfo(f"Inspecting object: {object_name}")\r\n        \r\n        # In real implementation, this would trigger perception routines\r\n        # For now, just simulate with a delay\r\n        rospy.sleep(2.0)\r\n        \r\n        # Return to report that inspection is complete\r\n        self.publish_feedback(f"Inspection of {object_name} completed")\r\n        return True\r\n    \r\n    def attempt_recovery(self, plan: Dict, failed_step: int, failed_action: Dict) > bool:\r\n        """Attempt to recover from action failure"""\r\n        recovery_strategies = plan.get("failure_recovery", [])\r\n        failed_condition = f"{failed_action.get(\'type\', \'unknown\')}_failed"\r\n        \r\n        # Find recovery strategy for this condition\r\n        recovery_strategy = next((rec for rec in recovery_strategies if rec.get("condition") == failed_condition), None)\r\n        \r\n        if recovery_strategy:\r\n            rospy.loginfo(f"Attempting recovery: {recovery_strategy[\'action\']}")\r\n            \r\n            # For this example, we\'ll just log the recovery attempt\r\n            # In a real system, you\'d implement specific recovery actions\r\n            if "navigation_failed" in recovery_strategy["action"]:\r\n                # Try alternative navigation approach\r\n                rospy.loginfo("Trying alternative navigation approach")\r\n                return True  # Simulate successful recovery\r\n            elif "object_not_found" in recovery_strategy["action"]:\r\n                # Try searching in wider area\r\n                rospy.loginfo("Expanding search area for object")\r\n                return True  # Simulate successful recovery\r\n            elif "grasp_failed" in recovery_strategy["action"]:\r\n                # Retry with different grasp approach\r\n                rospy.loginfo("Retrying grasp with different approach")\r\n                return True  # Simulate successful recovery\r\n        \r\n        rospy.logwarn(f"No recovery strategy found for condition: {failed_condition}")\r\n        return False  # No recovery strategy available\r\n    \r\n    def publish_status(self, message: str):\r\n        """Publish system status"""\r\n        status_msg = String()\r\n        status_msg.data = json.dumps({\r\n            "message": message,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.system_status_pub.publish(status_msg)\r\n    \r\n    def publish_feedback(self, message: str):\r\n        """Publish action feedback"""\r\n        feedback_msg = String()\r\n        feedback_msg.data = json.dumps({\r\n            "message": message,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.voice_response_pub.publish(feedback_msg)\r\n\r\ndef main():\r\n    api_key = rospy.get_param(\'~openai_api_key\', \'\')\r\n    if not api_key:\r\n        api_key = input("Enter OpenAI API key: ").strip()\r\n    \r\n    if not api_key:\r\n        rospy.logerr("No OpenAI API key provided!")\r\n        return\r\n    \r\n    system = VoiceActionIntegration(api_key)\r\n    \r\n    try:\r\n        rospy.spin()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down VoiceAction Integration System")\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-2-creating-a-safety-validation-layer",children:"Lab 2: Creating a Safety Validation Layer"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create a safety validation system"})," (",(0,a.jsx)(n.code,{children:"safety_validator.py"}),"):","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport numpy as np\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import LaserScan, PointCloud2\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nfrom geometry_msgs.msg import Point\r\nfrom typing import Dict, List, Optional, Tuple\r\nimport threading\r\nimport math\r\nimport time\r\n\r\nclass SafetyValidator:\r\n    def __init__(self):\r\n        # Initialize ROS node\r\n        rospy.init_node(\'safety_validator\', anonymous=True)\r\n        \r\n        # Publishers\r\n        self.safety_status_pub = rospy.Publisher(\'/safety_status\', String, queue_size=10)\r\n        self.emergency_stop_pub = rospy.Publisher(\'/emergency_stop\', Bool, queue_size=10)\r\n        self.safe_command_pub = rospy.Publisher(\'/safe_cmd_vel\', Twist, queue_size=10)\r\n        self.safety_violation_pub = rospy.Publisher(\'/safety_violations\', String, queue_size=10)\r\n        \r\n        # Subscribers\r\n        rospy.Subscriber(\'/cmd_vel\', Twist, self.unsafe_command_callback)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.laser_scan_callback)\r\n        rospy.Subscriber(\'/odom\', Odometry, self.odometry_callback)\r\n        rospy.Subscriber(\'/move_base_simple/goal\', PoseStamped, self.navigation_goal_callback)\r\n        \r\n        # Internal state\r\n        self.laser_scan = None\r\n        self.odom_data = None\r\n        self.robot_pose = Point(x=0.0, y=0.0, z=0.0)\r\n        self.robot_velocity = Point(x=0.0, y=0.0, z=0.0)\r\n        self.safety_violations = []\r\n        self.max_violations_history = 100\r\n        \r\n        # Safety parameters\r\n        self.safety_distance = rospy.get_param(\'~safety_distance\', 0.5)  # meters\r\n        self.max_linear_speed = rospy.get_param(\'~max_linear_speed\', 0.4)\r\n        self.max_angular_speed = rospy.get_param(\'~max_angular_speed\', 0.6)\r\n        self.max_deceleration = rospy.get_param(\'~max_deceleration\', 2.0)  # m/s^2\r\n        self.emergency_stop_distance = rospy.get_param(\'~emergency_stop_distance\', 0.3)\r\n        self.check_frequency = rospy.get_param(\'~check_frequency\', 20.0)  # Hz\r\n        \r\n        # Robot physical dimensions\r\n        self.robot_radius = rospy.get_param(\'~robot_radius\', 0.3)  # meters\r\n        \r\n        # State flags\r\n        self.emergency_stop_active = False\r\n        self.last_safe_command_time = rospy.Time.now().to_sec()\r\n        self.safety_check_interval = 1.0 / self.check_frequency\r\n        \r\n        # Command validation\r\n        self.pending_unsafe_command = None\r\n        self.unsafe_command_lock = threading.Lock()\r\n        \r\n        # Start safety monitoring\r\n        self.safety_timer = rospy.Timer(rospy.Duration(1.0/self.check_frequency), self.safety_check_callback)\r\n        \r\n        rospy.loginfo("Safety Validator initialized and monitoring active")\r\n    \r\n    def laser_scan_callback(self, msg: LaserScan):\r\n        """Update laser scan data"""\r\n        self.laser_scan = msg\r\n    \r\n    def odometry_callback(self, msg: Odometry):\r\n        """Update robot odometry data"""\r\n        self.odom_data = msg\r\n        self.robot_pose.x = msg.pose.pose.position.x\r\n        self.robot_pose.y = msg.pose.pose.position.y\r\n        self.robot_pose.z = msg.pose.pose.position.z\r\n        \r\n        # Get linear velocity from twist\r\n        self.robot_velocity.x = msg.twist.twist.linear.x\r\n        self.robot_velocity.y = msg.twist.twist.linear.y\r\n        self.robot_velocity.z = msg.twist.twist.linear.z\r\n    \r\n    def navigation_goal_callback(self, msg: PoseStamped):\r\n        """Validate navigation goal before execution"""\r\n        goal_x = msg.pose.position.x\r\n        goal_y = msg.pose.position.y\r\n        \r\n        # Check if goal is in a safe location\r\n        if self.is_position_safe(goal_x, goal_y):\r\n            rospy.loginfo(f"Navigation goal ({goal_x}, {goal_y}) is safe, allowing execution")\r\n            # In real implementation, forward to navigation stack\r\n        else:\r\n            rospy.logerr(f"Navigation goal ({goal_x}, {goal_y}) is unsafe, blocking execution")\r\n            self.publish_safety_violation({\r\n                "type": "unsafe_navigation_goal",\r\n                "position": (goal_x, goal_y),\r\n                "timestamp": rospy.Time.now().to_sec()\r\n            })\r\n    \r\n    def is_position_safe(self, x: float, y: float) > bool:\r\n        """Check if a position is safe to navigate to"""\r\n        if not self.laser_scan:\r\n            # If no sensor data available, be conservative\r\n            rospy.logwarn("No laser data available for position safety check, assuming unsafe")\r\n            return False\r\n        \r\n        # Convert position to robot frame\r\n        dx = x  self.robot_pose.x\r\n        dy = y  self.robot_pose.y\r\n        distance_to_position = math.sqrt(dx*dx + dy*dy)\r\n        \r\n        # Check if position is too close to current obstacles\r\n        # This is a simplified check  in real implementation, use path planning\r\n        min_range = min(self.laser_scan.ranges) if self.laser_scan.ranges else float(\'inf\')\r\n        if min_range < self.safety_distance:\r\n            rospy.logwarn(f"Position ({x}, {y}) potentially unsafe due to nearby obstacles")\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def unsafe_command_callback(self, msg: Twist):\r\n        """Validate and potentially modify unsafe commands"""\r\n        with self.unsafe_command_lock:\r\n            # Check if emergency stop is active\r\n            if self.emergency_stop_active:\r\n                # Discard command, robot is in emergency stop state\r\n                rospy.logwarn("Emergency stop active, discarding command")\r\n                return\r\n            \r\n            # Validate the command\r\n            safe_command = self.validate_and_modify_command(msg)\r\n            \r\n            if safe_command:\r\n                # Publish the safe command\r\n                self.safe_command_pub.publish(safe_command)\r\n                rospy.logdebug(f"Safe command published: linear.x={safe_command.linear.x}, angular.z={safe_command.angular.z}")\r\n                \r\n                # Update last safe command time\r\n                self.last_safe_command_time = rospy.Time.now().to_sec()\r\n            else:\r\n                # Command was rejected\r\n                rospy.logwarn("Command rejected for safety reasons")\r\n                self.publish_safety_violation({\r\n                    "type": "rejected_command",\r\n                    "command": {"linear_x": msg.linear.x, "angular_z": msg.angular.z},\r\n                    "reason": "Safety validation failed",\r\n                    "timestamp": rospy.Time.now().to_sec()\r\n                })\r\n    \r\n    def validate_and_modify_command(self, cmd: Twist) > Optional[Twist]:\r\n        """Validate and modify command to ensure safety"""\r\n        if not self.laser_scan:\r\n            # If no laser data, only allow zero velocity commands\r\n            if cmd.linear.x == 0.0 and cmd.angular.z == 0.0:\r\n                return cmd\r\n            else:\r\n                rospy.logwarn("No laser data, rejecting nonzero velocity command")\r\n                return None\r\n        \r\n        # Validate linear speed\r\n        if abs(cmd.linear.x) > self.max_linear_speed:\r\n            rospy.logwarn(f"Linear velocity {cmd.linear.x} exceeds limit {self.max_linear_speed}")\r\n            cmd.linear.x = np.clip(cmd.linear.x, self.max_linear_speed, self.max_linear_speed)\r\n        \r\n        # Validate angular speed\r\n        if abs(cmd.angular.z) > self.max_angular_speed:\r\n            rospy.logwarn(f"Angular velocity {cmd.angular.z} exceeds limit {self.max_angular_speed}")\r\n            cmd.angular.z = np.clip(cmd.angular.z, self.max_angular_speed, self.max_angular_speed)\r\n        \r\n        # Check for forward obstacles if trying to move forward\r\n        if cmd.linear.x > 0:\r\n            if self.has_obstacles_ahead(self.safety_distance):\r\n                # Calculate required stopping distance\r\n                current_speed = self.robot_velocity.x\r\n                if current_speed > 0:\r\n                    stopping_distance = (current_speed ** 2) / (2 * self.max_deceleration)\r\n                else:\r\n                    stopping_distance = 0\r\n                \r\n                # If we can\'t stop in time, slow down or stop\r\n                obstacle_distance = self.get_closest_obstacle_distance()\r\n                if obstacle_distance < (stopping_distance + self.robot_radius + 0.1):  # +0.1m safety margin\r\n                    # Reduce speed proportionally to distance\r\n                    new_speed = (obstacle_distance  self.robot_radius  0.1) / (self.safety_distance  self.robot_radius  0.1) * self.max_linear_speed\r\n                    cmd.linear.x = max(0.0, min(cmd.linear.x, new_speed))\r\n                    rospy.loginfo(f"Reducing forward speed due to obstacle at {obstacle_distance:.2f}m: {cmd.linear.x:.2f}m/s")\r\n        \r\n        # Check for backward obstacles if trying to move backward\r\n        if cmd.linear.x < 0:\r\n            if self.has_obstacles_behind(self.safety_distance):\r\n                current_speed = abs(self.robot_velocity.x)\r\n                if current_speed > 0:\r\n                    stopping_distance = (current_speed ** 2) / (2 * self.max_deceleration)\r\n                else:\r\n                    stopping_distance = 0\r\n                \r\n                # Similar check for backward obstacles\r\n                obstacle_distance = self.get_closest_backward_obstacle_distance()\r\n                if obstacle_distance < (stopping_distance + self.robot_radius + 0.1):\r\n                    new_speed = min(abs(cmd.linear.x), \r\n                                   max(0.0, (obstacle_distance  self.robot_radius  0.1) / (self.safety_distance  self.robot_radius  0.1) * self.max_linear_speed))\r\n                    cmd.linear.x = max(new_speed, cmd.linear.x)\r\n                    rospy.loginfo(f"Reducing backward speed due to rear obstacle")\r\n        \r\n        # Check if emergency distance is violated\r\n        if self.has_emergency_obstacles():\r\n            rospy.logerr("EMERGENCY: Obstacle too close, activating emergency stop")\r\n            self.activate_emergency_stop()\r\n            return None\r\n        \r\n        return cmd\r\n    \r\n    def has_obstacles_ahead(self, check_distance: float) > bool:\r\n        """Check for obstacles in the forward direction"""\r\n        if not self.laser_scan:\r\n            return False\r\n        \r\n        # Check front quarter of laser scan (simplified  check middle portion)\r\n        n_ranges = len(self.laser_scan.ranges)\r\n        front_start = n_ranges // 2  n_ranges // 8  # 3/4 to 5/8 of ranges (front leftright)\r\n        front_end = n_ranges // 2 + n_ranges // 8    # 1/2 + 1/8 = 5/8 to 7/8 of ranges (front rightleft)\r\n        \r\n        for i in range(front_start, front_end):\r\n            if i < len(self.laser_scan.ranges) and self.laser_scan.ranges[i] < check_distance:\r\n                if not (np.isinf(self.laser_scan.ranges[i]) or np.isnan(self.laser_scan.ranges[i])):\r\n                    return True\r\n        return False\r\n    \r\n    def has_obstacles_behind(self, check_distance: float) > bool:\r\n        """Check for obstacles in the rear direction"""\r\n        if not self.laser_scan:\r\n            return False\r\n        \r\n        # Check rear quarter of laser scan\r\n        n_ranges = len(self.laser_scan.ranges)\r\n        rear_start = 3 * n_ranges // 4\r\n        rear_end = n_ranges\r\n        \r\n        for i in range(rear_start, rear_end):\r\n            if self.laser_scan.ranges[i] < check_distance:\r\n                if not (np.isinf(self.laser_scan.ranges[i]) or np.isnan(self.laser_scan.ranges[i])):\r\n                    return True\r\n        \r\n        # Check front part as well (wrapping around)\r\n        front_start = 0\r\n        front_end = n_ranges // 4\r\n        for i in range(front_start, front_end):\r\n            if self.laser_scan.ranges[i] < check_distance:\r\n                if not (np.isinf(self.laser_scan.ranges[i]) or np.isnan(self.laser_scan.ranges[i])):\r\n                    return True\r\n        \r\n        return False\r\n    \r\n    def has_emergency_obstacles(self) > bool:\r\n        """Check for obstacles in emergency stop range"""\r\n        if not self.laser_scan:\r\n            return False\r\n        \r\n        # Check for obstacles closer than emergency stop distance\r\n        for range_val in self.laser_scan.ranges:\r\n            if not (np.isinf(range_val) or np.isnan(range_val)) and range_val < self.emergency_stop_distance:\r\n                return True\r\n        \r\n        return False\r\n    \r\n    def get_closest_obstacle_distance(self) > float:\r\n        """Get distance to closest obstacle in front"""\r\n        if not self.laser_scan:\r\n            return float(\'inf\')\r\n        \r\n        n_ranges = len(self.laser_scan.ranges)\r\n        front_start = n_ranges // 2  n_ranges // 8\r\n        front_end = n_ranges // 2 + n_ranges // 8\r\n        \r\n        min_distance = float(\'inf\')\r\n        for i in range(front_start, front_end):\r\n            if i < len(self.laser_scan.ranges):\r\n                range_val = self.laser_scan.ranges[i]\r\n                if not (np.isinf(range_val) or np.isnan(range_val)):\r\n                    min_distance = min(min_distance, range_val)\r\n        \r\n        return min_distance\r\n    \r\n    def get_closest_backward_obstacle_distance(self) > float:\r\n        """Get distance to closest obstacle in rear"""\r\n        if not self.laser_scan:\r\n            return float(\'inf\')\r\n        \r\n        n_ranges = len(self.laser_scan.ranges)\r\n        rear_ranges = []\r\n        \r\n        # Rear portion\r\n        rear_start = 3 * n_ranges // 4\r\n        rear_end = n_ranges\r\n        rear_ranges.extend(self.laser_scan.ranges[rear_start:rear_end])\r\n        \r\n        # Front wrapping portion\r\n        front_start = 0\r\n        front_end = n_ranges // 4\r\n        rear_ranges.extend(self.laser_scan.ranges[front_start:front_end])\r\n        \r\n        valid_distances = [r for r in rear_ranges if not (np.isinf(r) or np.isnan(r))]\r\n        return min(valid_distances) if valid_distances else float(\'inf\')\r\n    \r\n    def safety_check_callback(self, event):\r\n        """Periodic safety checks"""\r\n        # Check for stale sensor data\r\n        if self.laser_scan:\r\n            time_since_scan = rospy.Time.now().to_sec()  event.current_real.to_sec()\r\n            if time_since_scan > 1.0:  # If laser scan is more than 1 second old\r\n                rospy.logwarn("Laser scan data is stale, entering safety mode")\r\n                self.reduce_speed_for_stale_sensors()\r\n        \r\n        # Check for stuck robot (command sent but no movement)\r\n        if self.odom_data and self.laser_scan:\r\n            # Check if robot should be moving but isn\'t\r\n            if abs(self.robot_velocity.x) < 0.01 and abs(self.robot_velocity.y) < 0.01:  # Robot not moving\r\n                # If there are no obstacles and robot should be moving, this might indicate a problem\r\n                min_dist = min(self.laser_scan.ranges) if self.laser_scan.ranges else float(\'inf\')\r\n                if min_dist > 0.5:  # No obstacles nearby\r\n                    # Check if we recently sent a command\r\n                    # (In real implementation, we\'d track the last command sent)\r\n                    pass  # Potential stuck robot detection\r\n    \r\n    def activate_emergency_stop(self):\r\n        """Activate emergency stop"""\r\n        if not self.emergency_stop_active:\r\n            rospy.logerr("EMERGENCY STOP ACTIVATED")\r\n            self.emergency_stop_active = True\r\n            \r\n            # Send stop command\r\n            stop_cmd = Twist()\r\n            self.safe_command_pub.publish(stop_cmd)\r\n            \r\n            # Publish emergency stop signal\r\n            emergency_msg = Bool()\r\n            emergency_msg.data = True\r\n            self.emergency_stop_pub.publish(emergency_msg)\r\n            \r\n            # Log the violation\r\n            self.publish_safety_violation({\r\n                "type": "emergency_stop_activated",\r\n                "reason": "Obstacle too close",\r\n                "timestamp": rospy.Time.now().to_sec()\r\n            })\r\n    \r\n    def deactivate_emergency_stop(self):\r\n        """Deactivate emergency stop"""\r\n        if self.emergency_stop_active:\r\n            rospy.loginfo("EMERGENCY STOP DEACTIVATED")\r\n            self.emergency_stop_active = False\r\n            \r\n            # Publish emergency stop reset\r\n            emergency_msg = Bool()\r\n            emergency_msg.data = False\r\n            self.emergency_stop_pub.publish(emergency_msg)\r\n    \r\n    def reduce_speed_for_stale_sensors(self):\r\n        """Reduce speed when sensor data is stale"""\r\n        # In real implementation, this would send reduced speed commands to the robot\r\n        # For now, we\'ll just log the action\r\n        rospy.logwarn("Reducing robot speed due to stale sensor data")\r\n    \r\n    def publish_safety_violation(self, violation: Dict):\r\n        """Publish safety violation information"""\r\n        violation_msg = String()\r\n        violation_msg.data = json.dumps(violation)\r\n        self.safety_violation_pub.publish(violation_msg)\r\n        \r\n        # Add to internal history\r\n        self.safety_violations.append(violation)\r\n        if len(self.safety_violations) > self.max_violations_history:\r\n            self.safety_violations = self.safety_violations[self.max_violations_history:]\r\n    \r\n    def get_safety_status(self) > Dict:\r\n        """Get current safety status"""\r\n        status = {\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "emergency_stop_active": self.emergency_stop_active,\r\n            "closest_obstacle_distance": self.get_closest_obstacle_distance() if self.laser_scan else float(\'inf\'),\r\n            "robot_velocity": {\r\n                "linear": self.robot_velocity.x,\r\n                "angular": self.robot_velocity.z\r\n            },\r\n            "robot_position": {\r\n                "x": self.robot_pose.x,\r\n                "y": self.robot_pose.y\r\n            },\r\n            "recent_violations_count": len(self.safety_violations),\r\n            "last_violation": self.safety_violations[1] if self.safety_violations else None\r\n        }\r\n        \r\n        status_msg = String()\r\n        status_msg.data = json.dumps(status)\r\n        self.safety_status_pub.publish(status_msg)\r\n        \r\n        return status\r\n\r\ndef main():\r\n    validator = SafetyValidator()\r\n    \r\n    # Start a timer to publish safety status periodically\r\n    status_timer = rospy.Timer(rospy.Duration(1.0), lambda event: validator.get_safety_status())\r\n    \r\n    try:\r\n        rospy.spin()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down Safety Validator")\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-3-implementing-the-complete-integration-system",children:"Lab 3: Implementing the Complete Integration System"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a launch file for the complete system"})," (",(0,a.jsx)(n.code,{children:"launch/voice_to_action_system.launch.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\r\nfrom launch_ros.actions import Node\r\nfrom launch_ros.substitutions import FindPackageShare\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    openai_api_key_arg = DeclareLaunchArgument(\r\n        'openai_api_key',\r\n        default_value='',\r\n        description='OpenAI API key for GPT4o access'\r\n    )\r\n    \r\n    use_sim_time_arg = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='false',\r\n        description='Use simulation (Gazebo) clock if true'\r\n    )\r\n    \r\n    # Get launch configuration variables\r\n    openai_api_key = LaunchConfiguration('openai_api_key')\r\n    use_sim_time = LaunchConfiguration('use_sim_time')\r\n    \r\n    # Create nodes\r\n    voice_action_node = Node(\r\n        package='robot_voice_control',\r\n        executable='voice_action_integration.py',\r\n        name='voice_action_integration',\r\n        parameters=[\r\n            {'openai_api_key': openai_api_key},\r\n            {'use_sim_time': use_sim_time},\r\n            {'safe_distance': 0.5},\r\n            {'max_linear_speed': 0.3},\r\n            {'max_angular_speed': 0.5}\r\n        ],\r\n        remappings=[\r\n            ('/voice_command', '/voice_transcription/text'),\r\n            ('/cmd_vel', '/unprotected_cmd_vel'),\r\n            ('/move_base_simple/goal', '/move_base/goal')\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    safety_validator_node = Node(\r\n        package='robot_voice_control',\r\n        executable='safety_validator.py',\r\n        name='safety_validator',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time},\r\n            {'safety_distance': 0.5},\r\n            {'max_linear_speed': 0.4},\r\n            {'max_angular_speed': 0.6},\r\n            {'emergency_stop_distance': 0.3},\r\n            {'robot_radius': 0.3}\r\n        ],\r\n        remappings=[\r\n            ('/cmd_vel', '/unprotected_cmd_vel'),\r\n            ('/safe_cmd_vel', '/cmd_vel')\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    # Optional: Add a simple voice recognition node if needed\r\n    # This would typically be a separate package like 'vosk_ros' or 'google_stt_ros'\r\n    voice_recognition_node = Node(\r\n        package='vosk_ros',\r\n        executable='vosk_node',\r\n        name='vosk_node',\r\n        parameters=[\r\n            {'model': 'model_path_here'},  # You'd specify path to vosk model\r\n            {'sample_rate': 16000.0}\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    # Add other necessary nodes for a complete system:\r\n    #  Robot driver\r\n    #  Navigation stack\r\n    #  Perception pipeline (if manipulator included)\r\n    \r\n    # Create launch description\r\n    ld = LaunchDescription()\r\n    \r\n    # Add launch arguments\r\n    ld.add_action(openai_api_key_arg)\r\n    ld.add_action(use_sim_time_arg)\r\n    \r\n    # Add nodes\r\n    ld.add_action(voice_action_node)\r\n    ld.add_action(safety_validator_node)\r\n    # ld.add_action(voice_recognition_node)  # Uncomment if using voice recognition\r\n    \r\n    return ld\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create a comprehensive test script"})," (",(0,a.jsx)(n.code,{children:"test_voice_pipeline.py"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nfrom std_msgs.msg import String\r\nimport time\r\nimport json\r\n\r\nclass VoicePipelineTester:\r\n    def __init__(self):\r\n        rospy.init_node(\'voice_pipeline_tester\', anonymous=True)\r\n        \r\n        # Publishers for testing\r\n        self.voice_cmd_pub = rospy.Publisher(\'/voice_command\', String, queue_size=10)\r\n        self.status_sub = rospy.Subscriber(\'/system_status\', String, self.status_callback)\r\n        self.feedback_sub = rospy.Subscriber(\'/action_feedback\', String, self.feedback_callback)\r\n        \r\n        self.test_results = []\r\n        self.current_test = None\r\n        \r\n        rospy.loginfo("Voice Pipeline Tester initialized")\r\n    \r\n    def status_callback(self, msg):\r\n        """Handle system status updates"""\r\n        try:\r\n            status_data = json.loads(msg.data)\r\n            if self.current_test:\r\n                rospy.loginfo(f"Status for test {self.current_test[\'name\']}: {status_data.get(\'message\', \'no message\')}")\r\n        except json.JSONDecodeError:\r\n            rospy.logerr("Invalid JSON in status message")\r\n    \r\n    def feedback_callback(self, msg):\r\n        """Handle action feedback"""\r\n        try:\r\n            feedback_data = json.loads(msg.data)\r\n            if self.current_test:\r\n                rospy.loginfo(f"Feedback for test {self.current_test[\'name\']}: {feedback_data.get(\'message\', \'no message\')}")\r\n                # Add to test results\r\n                self.current_test["feedback"].append(feedback_data)\r\n        except json.JSONDecodeError:\r\n            rospy.logerr("Invalid JSON in feedback message")\r\n    \r\n    def run_test_suite(self):\r\n        """Run comprehensive test suite"""\r\n        rospy.loginfo("Starting voice pipeline test suite")\r\n        \r\n        # Test 1: Simple navigation command\r\n        self.run_test("simple_navigation", "Go to kitchen")\r\n        \r\n        # Test 2: Complex command with object interaction\r\n        self.run_test("object_interaction", "Approach the red cup in the kitchen and inspect it")\r\n        \r\n        # Test 3: Safety validation (should be blocked if obstacles in path)\r\n        self.run_test("safety_validation", "Move forward toward the obstacle")\r\n        \r\n        # Test 4: Multistep command\r\n        self.run_test("multi_step", "Navigate to bedroom, find John, escort him to kitchen")\r\n        \r\n        # Test 5: Recovery from failure\r\n        self.run_test("recovery", "Go to office and bring me the green notebook")\r\n        \r\n        rospy.loginfo("Test suite completed")\r\n        self.generate_test_report()\r\n    \r\n    def run_test(self, test_name: str, command: str):\r\n        """Run a single test case"""\r\n        rospy.loginfo(f"Running test: {test_name}  Command: \'{command}\'")\r\n        \r\n        self.current_test = {\r\n            "name": test_name,\r\n            "command": command,\r\n            "start_time": rospy.Time.now().to_sec(),\r\n            "feedback": []\r\n        }\r\n        \r\n        # Send command\r\n        cmd_msg = String()\r\n        cmd_msg.data = command\r\n        self.voice_cmd_pub.publish(cmd_msg)\r\n        \r\n        # Wait for execution (max 30 seconds)\r\n        start_time = rospy.Time.now().to_sec()\r\n        timeout = 30.0\r\n        \r\n        while (rospy.Time.now().to_sec()  start_time) < timeout:\r\n            rospy.sleep(0.1)\r\n        \r\n        # Record results\r\n        self.current_test["end_time"] = rospy.Time.now().to_sec()\r\n        self.current_test["duration"] = self.current_test["end_time"]  self.current_test["start_time"]\r\n        \r\n        # Determine success based on feedback\r\n        success = False\r\n        for feedback in self.current_test["feedback"]:\r\n            if "completed successfully" in feedback.get("message", "").lower():\r\n                success = True\r\n                break\r\n        \r\n        self.current_test["success"] = success\r\n        self.test_results.append(self.current_test)\r\n        \r\n        # Log result\r\n        status = "SUCCESS" if success else "FAILED"\r\n        rospy.loginfo(f"Test {test_name} completed with status: {status}")\r\n    \r\n    def generate_test_report(self):\r\n        """Generate test report"""\r\n        rospy.loginfo("\\n=== VOICE PIPELINE TEST RESULTS ===")\r\n        \r\n        for test in self.test_results:\r\n            status = "\u2713 PASS" if test["success"] else "\u2717 FAIL"\r\n            rospy.loginfo(f"{status} {test[\'name\']}: \'{test[\'command\']}\' ({test[\'duration\']:.2f}s)")\r\n        \r\n        total_tests = len(self.test_results)\r\n        passed_tests = sum(1 for t in self.test_results if t["success"])\r\n        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0\r\n        \r\n        rospy.loginfo(f"\\nOverall Success Rate: {success_rate:.1f}% ({passed_tests}/{total_tests} tests passed)")\r\n\r\ndef main():\r\n    tester = VoicePipelineTester()\r\n    \r\n    # Run tests after a brief delay to allow system to initialize\r\n    rospy.sleep(5.0)\r\n    tester.run_test_suite()\r\n    \r\n    rospy.spin()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,a.jsx)(n.p,{children:"Here's a complete integrated system that demonstrates the full voicetoaction pipeline:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_voice_to_action_system.py\r\n\r\nimport rospy\r\nimport openai\r\nimport whisper\r\nimport pyaudio\r\nimport numpy as np\r\nimport json\r\nimport threading\r\nimport queue\r\nimport time\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom actionlib_msgs.msg import GoalStatusArray\r\nfrom typing import Dict, List, Optional\r\nimport torch\r\nimport wave\r\nimport tempfile\r\nimport os\r\n\r\nclass CompleteVoiceToActionSystem:\r\n    """Complete system integrating voice recognition, cognitive planning, navigation and manipulation"""\r\n    \r\n    def __init__(self, api_key: str, whisper_model_size: str = "base"):\r\n        # Initialize ROS\r\n        rospy.init_node(\'complete_voice_to_action_system\', anonymous=True)\r\n        \r\n        # Initialize models\r\n        self.whisper_model = whisper.load_model(whisper_model_size)\r\n        self.openai_client = openai.OpenAI(api_key=api_key)\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        self.goal_pub = rospy.Publisher(\'/move_base_simple/goal\', PoseStamped, queue_size=10)\r\n        self.system_status_pub = rospy.Publisher(\'/system_status\', String, queue_size=10)\r\n        self.voice_response_pub = rospy.Publisher(\'/voice_response\', String, queue_size=10)\r\n        self.action_feedback_pub = rospy.Publisher(\'/action_feedback\', String, queue_size=10)\r\n        \r\n        # Subscribers\r\n        rospy.Subscriber(\'/voice_command\', String, self.voice_command_callback)\r\n        rospy.Subscriber(\'/scan\', LaserScan, self.scan_callback)\r\n        rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\r\n        rospy.Subscriber(\'/move_base/status\', GoalStatusArray, self.navigation_status_callback)\r\n        rospy.Subscriber(\'/action_execution_status\', String, self.action_status_callback)\r\n        \r\n        # Internal state\r\n        self.laser_data = None\r\n        self.image_data = None\r\n        self.robot_position = {"x": 0.0, "y": 0.0, "theta": 0.0}\r\n        self.navigation_active = False\r\n        \r\n        # Audio processing\r\n        self.audio = pyaudio.PyAudio()\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.voice_threshold = 0.01\r\n        self.min_voice_duration = 0.5  # seconds\r\n        self.min_silence_duration = 1.0  # seconds before stopping recording\r\n        \r\n        # Command processing\r\n        self.command_queue = queue.Queue()\r\n        self.processing_thread = threading.Thread(target=self.process_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        # System configuration\r\n        self.safe_distance = rospy.get_param(\'~safe_distance\', 0.5)\r\n        self.max_linear_speed = rospy.get_param(\'~max_linear_speed\', 0.3)\r\n        self.max_angular_speed = rospy.get_param(\'~max_angular_speed\', 0.5)\r\n        self.command_timeout = rospy.get_param(\'~command_timeout\', 30.0)\r\n        \r\n        # For voice activation\r\n        self.is_listening = False\r\n        self.listening_thread = None\r\n        self.voice_activation_enabled = rospy.get_param(\'~voice_activation\', False)\r\n        \r\n        rospy.loginfo("Complete VoicetoAction System initialized")\r\n    \r\n    def start_voice_activation(self):\r\n        """Start voice activation if enabled"""\r\n        if self.voice_activation_enabled:\r\n            self.is_listening = True\r\n            self.listening_thread = threading.Thread(target=self.voice_activation_loop)\r\n            self.listening_thread.daemon = True\r\n            self.listening_thread.start()\r\n            rospy.loginfo("Voice activation started")\r\n    \r\n    def voice_activation_loop(self):\r\n        """Continuous voice activation loop"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        frames = []\r\n        voice_active_count = 0\r\n        silence_count = 0\r\n        voice_frames_needed = int(self.min_voice_duration * self.rate / self.chunk)\r\n        silence_frames_needed = int(self.min_silence_duration * self.rate / self.chunk)\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                # Calculate RMS for voice detection\r\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n                rms = np.sqrt(np.mean(audio_array ** 2))\r\n                \r\n                if rms > self.voice_threshold:\r\n                    # Voice detected\r\n                    voice_active_count += 1\r\n                    silence_count = 0\r\n                    \r\n                    if not recording:\r\n                        if voice_active_count >= voice_frames_needed:\r\n                            # Start recording\r\n                            recording = True\r\n                            frames = []\r\n                            rospy.loginfo("Voice activation detected, starting recording...")\r\n                    \r\n                    if recording:\r\n                        frames.append(data)\r\n                else:\r\n                    # Silence detected\r\n                    voice_active_count = 0\r\n                    if recording:\r\n                        silence_count += 1\r\n                        frames.append(data)\r\n                        \r\n                        if silence_count >= silence_frames_needed:\r\n                            # End of speech detected\r\n                            if len(frames) > voice_frames_needed:  # Ensure minimum speech length\r\n                                rospy.loginfo(f"Speech detected, saving {len(frames)} frames for transcription...")\r\n                                \r\n                                # Save to temp file\r\n                                temp_filename = self.save_audio_frames(frames)\r\n                                \r\n                                # Transcribe and process\r\n                                transcript = self.transcribe_audio(temp_filename)\r\n                                \r\n                                if transcript and transcript.strip():\r\n                                    rospy.loginfo(f"Transcribed: {transcript}")\r\n                                    \r\n                                    # Add to command queue\r\n                                    self.command_queue.put({\r\n                                        "text": transcript.strip(),\r\n                                        "timestamp": rospy.Time.now().to_sec(),\r\n                                        "source": "voice_activation"\r\n                                    })\r\n                            \r\n                            # Reset for next recording\r\n                            recording = False\r\n                            frames = []\r\n                            silence_count = 0\r\n            \r\n            # Clean up temp file\r\n            if os.path.exists(temp_filename):\r\n                os.remove(temp_filename)\r\n                \r\n        except Exception as e:\r\n            rospy.logerr(f"Error in voice activation: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def save_audio_frames(self, frames):\r\n        """Save audio frames to temporary file"""\r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        wf = wave.open(temp_path, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        os.close(temp_fd)\r\n        return temp_path\r\n    \r\n    def transcribe_audio(self, audio_file_path):\r\n        """Transcribe audio using Whisper"""\r\n        try:\r\n            result = self.whisper_model.transcribe(audio_file_path)\r\n            return result[\'text\'].strip()\r\n        except Exception as e:\r\n            rospy.logerr(f"Whisper transcription error: {e}")\r\n            return ""\r\n    \r\n    def scan_callback(self, msg: LaserScan):\r\n        """Update laser scan data"""\r\n        self.laser_data = msg\r\n    \r\n    def image_callback(self, msg: Image):\r\n        """Update image data"""\r\n        self.image_data = msg\r\n    \r\n    def navigation_status_callback(self, msg: GoalStatusArray):\r\n        """Update navigation status"""\r\n        if msg.status_list:\r\n            # Most recent goal status\r\n            last_status = msg.status_list[1]\r\n            if last_status.status == 3:  # Succeeded\r\n                self.navigation_active = False\r\n                self.publish_feedback("Navigation completed successfully")\r\n            elif last_status.status in [1, 2, 4, 5, 8]:  # Active/Aborted/etc.\r\n                self.navigation_active = True\r\n    \r\n    def action_status_callback(self, msg: String):\r\n        """Handle action execution status"""\r\n        try:\r\n            status_data = json.loads(msg.data)\r\n            action_id = status_data.get("action_id", "unknown")\r\n            status = status_data.get("status", "unknown")\r\n            \r\n            if status == "failure":\r\n                rospy.logerr(f"Action {action_id} failed: {status_data.get(\'error\', \'Unknown error\')}")\r\n                self.publish_feedback(f"Action {action_id} failed: {status_data.get(\'error\', \'Unknown error\')}")\r\n            elif status == "success":\r\n                rospy.loginfo(f"Action {action_id} completed successfully")\r\n                self.publish_feedback(f"Action {action_id} completed successfully")\r\n                \r\n        except json.JSONDecodeError:\r\n            rospy.logerr("Invalid JSON in action status message")\r\n    \r\n    def voice_command_callback(self, msg: String):\r\n        """Process incoming voice command"""\r\n        command_text = msg.data\r\n        rospy.loginfo(f"Received voice command: {command_text}")\r\n        \r\n        command_item = {\r\n            "text": command_text,\r\n            "timestamp": rospy.Time.now().to_sec(),\r\n            "source": "external"\r\n        }\r\n        \r\n        self.command_queue.put(command_item)\r\n        self.publish_status(f"Command received: {command_text[:50]}...")\r\n    \r\n    def process_commands(self):\r\n        """Process commands from the queue"""\r\n        while not rospy.is_shutdown():\r\n            try:\r\n                command_item = self.command_queue.get(timeout=1.0)\r\n                \r\n                success = self.process_command_item(command_item)\r\n                \r\n                if not success:\r\n                    rospy.logerr(f"Command processing failed: {command_item[\'text\']}")\r\n                    self.publish_feedback(f"Command failed: {command_item[\'text\'][:50]}...")\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command queue: {e}")\r\n                continue\r\n    \r\n    def process_command_item(self, command_item: Dict) > bool:\r\n        """Process a single command item through the full pipeline"""\r\n        command_text = command_item["text"]\r\n        rospy.loginfo(f"Processing command: {command_text}")\r\n        \r\n        # Get environment context\r\n        context = self.get_environment_context()\r\n        \r\n        # Generate plan using GPT4o\r\n        plan = self.generate_action_plan(command_text, context)\r\n        \r\n        if not plan:\r\n            rospy.logerr("Failed to generate action plan")\r\n            return False\r\n        \r\n        rospy.loginfo(f"Generated plan with {len(plan.get(\'actions\', []))} actions")\r\n        \r\n        # Validate plan safety\r\n        if not self.validate_plan_safety(plan):\r\n            rospy.logerr("Plan failed safety validation")\r\n            self.publish_feedback("Plan rejected: Failed safety validation")\r\n            return False\r\n        \r\n        # Execute plan\r\n        execution_success = self.execute_plan(plan)\r\n        \r\n        if execution_success:\r\n            rospy.loginfo("Command executed successfully")\r\n            self.publish_feedback(f"Command completed: {command_text[:50]}...")\r\n            return True\r\n        else:\r\n            rospy.logerr("Command execution failed")\r\n            self.publish_feedback(f"Command failed: {command_text[:50]}...")\r\n            return False\r\n    \r\n    def get_environment_context(self) > Dict:\r\n        """Get current environment context"""\r\n        context = {\r\n            "robot_state": {\r\n                "position": self.robot_position,\r\n                "battery_level": 0.85,\r\n                "navigation_status": "ready" if not self.navigation_active else "executing"\r\n            },\r\n            "environment": {\r\n                "obstacles": self.get_obstacle_information(),\r\n                "known_locations": self.get_known_locations(),\r\n                "detected_objects": self.get_detected_objects(),\r\n            },\r\n            "constraints": {\r\n                "safe_distance": self.safe_distance,\r\n                "max_linear_speed": self.max_linear_speed,\r\n                "max_angular_speed": self.max_angular_speed\r\n            }\r\n        }\r\n        return context\r\n    \r\n    def get_obstacle_information(self) > List[Dict]:\r\n        """Extract obstacle information from laser scan"""\r\n        if not self.laser_data:\r\n            return []\r\n        \r\n        obstacles = []\r\n        ranges = self.laser_data.ranges\r\n        angle_min = self.laser_data.angle_min\r\n        angle_increment = self.laser_data.angle_increment\r\n        \r\n        # Sample every 20th point to reduce computation\r\n        for i in range(0, len(ranges), 20):\r\n            if not (np.isinf(ranges[i]) or np.isnan(ranges[i])):\r\n                angle = angle_min + i * angle_increment\r\n                x = ranges[i] * np.cos(angle)\r\n                y = ranges[i] * np.sin(angle)\r\n                \r\n                if ranges[i] < self.safe_distance * 2:\r\n                    obstacles.append({\r\n                        "x": float(x),\r\n                        "y": float(y),\r\n                        "distance": float(ranges[i]),\r\n                        "angle": float(angle)\r\n                    })\r\n        \r\n        return obstacles\r\n    \r\n    def get_known_locations(self) > List[Dict]:\r\n        """Get known locations in the environment"""\r\n        # In real implementation, this would come from map server\r\n        return [\r\n            {"name": "kitchen", "x": 2.0, "y": 1.0},\r\n            {"name": "living_room", "x": 1.0, "y": 1.0},\r\n            {"name": "bedroom", "x": 1.0, "y": 1.5},\r\n            {"name": "office", "y": 0.5, "y": 2.0},\r\n            {"name": "charging_station", "x": 2.0, "y": 2.0}\r\n        ]\r\n    \r\n    def get_detected_objects(self) > List[Dict]:\r\n        """Get detected objects from perception system"""\r\n        # In real implementation, this would come from object detection\r\n        if self.image_data:\r\n            # Simulated object detection from image\r\n            return [\r\n                {"name": "red_cup", "type": "container", "distance": 1.2, "location": "kitchen", "confidence": 0.9},\r\n                {"name": "blue_book", "type": "stationery", "distance": 1.8, "location": "office", "confidence": 0.8}\r\n            ]\r\n        return []\r\n    \r\n    def generate_action_plan(self, command: str, context: Dict) > Optional[Dict]:\r\n        """Generate action plan using GPT4o"""\r\n        prompt = f"""\r\n        Command: "{command}"\r\n        \r\n        Environment Context:\r\n        {json.dumps(context, indent=2)}\r\n        \r\n        Available Actions:\r\n         navigate_to(location_name): Move robot to named location\r\n         approach_object(object_name): Move robot near specific object\r\n         inspect_object(object_name): Examine object with sensors\r\n         grasp_object(object_name): Grasp an object (if equipped with manipulator)\r\n         place_object(object_name, location): Place object at location\r\n         open_container(container_name): Open a container/door\r\n         close_container(container_name): Close a container/door\r\n         follow_person(person_name): Follow a person\r\n         wait(duration_seconds): Pause for specified duration\r\n         speak_response(text): Speak a response to user\r\n         find_person(person_name): Locate a specific person\r\n         escort_person(person_name, destination): Guide person to location\r\n         patrol_area(area_name): Move through predefined area\r\n         charge_robot(): Return to charging station\r\n        \r\n        Generate a detailed action plan that:\r\n        1. Breaks down the command into specific, executable actions\r\n        2. Considers the environment context\r\n        3. Respects robot capabilities and constraints\r\n        4. Includes safety checks before movement\r\n        5. Handles potential failure modes\r\n        6. Verifies completion of each step\r\n        7. Includes error recovery strategies\r\n        \r\n        Return as JSON:\r\n        {{\r\n          "original_command": "{command}",\r\n          "reasoning": "stepbystep reasoning about the plan",\r\n          "actions": [\r\n            {{\r\n              "id": 1,\r\n              "type": "action_type",\r\n              "parameters": {{"param1": "value1", "param2": "value2"}},\r\n              "description": "what this step accomplishes",\r\n              "preconditions": ["condition1", "condition2"],\r\n              "expected_effects": ["effect1", "effect2"],\r\n              "safety_check_needed": true,\r\n              "estimated_duration": 15.0,\r\n              "success_probability": 0.85\r\n            }}\r\n          ],\r\n          "estimated_total_duration": 120.0,\r\n          "overall_confidence": 0.75,\r\n          "failure_recovery": [\r\n            {{"condition": "object_not_found", "action": "search_alternatives"}},\r\n            {{"condition": "navigation_failed", "action": "replan_path"}},\r\n            {{"condition": "grasp_failed", "action": "retry_with_different_approach"}}\r\n          ]\r\n        }}\r\n        \r\n        Respond with ONLY the JSON object, no additional text.\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4o",\r\n                messages=[\r\n                    {\r\n                        "role": "system",\r\n                        "content": "You are a robotic task planner. Generate detailed, executable plans with safety considerations and recovery strategies. Respond only with valid JSON."\r\n                    },\r\n                    {\r\n                        "role": "user",\r\n                        "content": prompt\r\n                    }\r\n                ],\r\n                temperature=0.1,\r\n                max_tokens=2000\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON if wrapped in code block\r\n            if response_text.startswith(\'```\'):\r\n                start_idx = response_text.find(\'{\')\r\n                end_idx = response_text.rfind(\'}\') + 1\r\n                if start_idx != 1 and end_idx != 1:\r\n                    response_text = response_text[start_idx:end_idx]\r\n            \r\n            plan_data = json.loads(response_text)\r\n            plan_data["timestamp"] = rospy.Time.now().to_sec()\r\n            \r\n            return plan_data\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error generating action plan: {e}")\r\n            return None\r\n    \r\n    def validate_plan_safety(self, plan: Dict) > bool:\r\n        """Validate the safety of the generated plan"""\r\n        actions = plan.get("actions", [])\r\n        \r\n        for action in actions:\r\n            action_type = action.get("type", "")\r\n            params = action.get("parameters", {})\r\n            \r\n            # Check navigation safety\r\n            if action_type in ["navigate_to", "approach_object"]:\r\n                if "location" in params:\r\n                    if not self.is_safe_navigation_destination(params["location"]):\r\n                        return False\r\n                elif "object_name" in params:\r\n                    if not self.is_safe_to_approach_object(params["object_name"]):\r\n                        return False\r\n            \r\n            # Check manipulation safety\r\n            elif action_type in ["grasp_object", "place_object"]:\r\n                if not self.is_manipulation_safe():\r\n                    return False\r\n        \r\n        return True\r\n    \r\n    def is_safe_navigation_destination(self, location_name: str) > bool:\r\n        """Check if navigation destination is safe"""\r\n        known_locations = self.get_known_locations()\r\n        target_loc = next((loc for loc in known_locations if loc["name"] == location_name), None)\r\n        \r\n        if not target_loc:\r\n            rospy.logwarn(f"Unknown location: {location_name}")\r\n            return False\r\n        \r\n        # Check laser scan for obstacles to destination\r\n        if self.laser_data:\r\n            # For simplicity, check if there\'s a clear path (this would need more sophisticated path planning in reality)\r\n            front_ranges = self.laser_data.ranges[len(self.laser_data.ranges)//220:len(self.laser_data.ranges)//2+20]\r\n            valid_ranges = [r for r in front_ranges if not (np.isinf(r) or np.isnan(r))]\r\n            if valid_ranges and min(valid_ranges) < 0.5:\r\n                return False  # Obstacle in front path\r\n        \r\n        return True\r\n    \r\n    def is_safe_to_approach_object(self, object_name: str) > bool:\r\n        """Check if it\'s safe to approach an object"""\r\n        detected_objects = self.get_detected_objects()\r\n        target_obj = next((obj for obj in detected_objects if obj["name"] == object_name), None)\r\n        \r\n        if not target_obj:\r\n            rospy.logwarn(f"Object {object_name} not detected")\r\n            return False\r\n        \r\n        # Check if object is at a safe distance\r\n        if target_obj["distance"] > 3.0 or target_obj["distance"] < 0.2:\r\n            rospy.logwarn(f"Object {object_name} is too far ({target_obj[\'distance\']}) or too close")\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def execute_plan(self, plan: Dict) > bool:\r\n        """Execute the action plan step by step"""\r\n        actions = plan.get("actions", [])\r\n        rospy.loginfo(f"Executing plan with {len(actions)} actions")\r\n        \r\n        for i, action in enumerate(actions):\r\n            rospy.loginfo(f"Executing action {i+1}/{len(actions)}: {action.get(\'type\', \'unknown\')}")\r\n            \r\n            self.publish_feedback(f"Executing: {action.get(\'description\', action.get(\'type\'))}")\r\n            \r\n            # Execute the action\r\n            success = self.execute_single_action(action)\r\n            \r\n            if not success:\r\n                rospy.logerr(f"Action execution failed: {action}")\r\n                \r\n                # Try recovery\r\n                if not self.attempt_recovery(plan, i, action):\r\n                    rospy.logerr("Recovery failed, terminating plan execution")\r\n                    return False\r\n            \r\n            # Small delay between actions\r\n            rospy.sleep(0.5)\r\n        \r\n        rospy.loginfo("Plan execution completed successfully")\r\n        return True\r\n    \r\n    def execute_single_action(self, action: Dict) > bool:\r\n        """Execute a single action in the plan"""\r\n        action_type = action.get("type", "")\r\n        params = action.get("parameters", {})\r\n        \r\n        if action_type == "navigate_to":\r\n            return self.execute_navigation_action(params)\r\n        elif action_type == "approach_object":\r\n            return self.execute_approach_action(params)\r\n        elif action_type == "inspect_object":\r\n            return self.execute_inspection_action(params)\r\n        elif action_type == "grasp_object":\r\n            return self.execute_grasp_action(params)\r\n        elif action_type == "place_object":\r\n            return self.execute_place_action(params)\r\n        elif action_type == "speak_response":\r\n            return self.execute_speak_action(params)\r\n        elif action_type == "wait":\r\n            return self.execute_wait_action(params)\r\n        else:\r\n            rospy.logwarn(f"Unknown action type: {action_type}, skipping...")\r\n            return True  # Don\'t fail on unknown actions\r\n    \r\n    def execute_navigation_action(self, params: Dict) > bool:\r\n        """Execute navigation action"""\r\n        location_name = params.get("location")\r\n        \r\n        if not location_name:\r\n            rospy.logerr("Navigation action missing location parameter")\r\n            return False\r\n        \r\n        # Get destination from known locations\r\n        known_locations = self.get_known_locations()\r\n        target_loc = next((loc for loc in known_locations if loc["name"] == location_name), None)\r\n        \r\n        if not target_loc:\r\n            rospy.logerr(f"Unknown location: {location_name}")\r\n            return False\r\n        \r\n        # Create and publish navigation goal\r\n        goal = PoseStamped()\r\n        goal.header.frame_id = "map"\r\n        goal.header.stamp = rospy.Time.now()\r\n        goal.pose.position.x = target_loc["x"]\r\n        goal.pose.position.y = target_loc["y"]\r\n        goal.pose.position.z = 0.0\r\n        goal.pose.orientation.w = 1.0  # No rotation\r\n        \r\n        self.goal_pub.publish(goal)\r\n        rospy.loginfo(f"Navigation goal sent to {location_name} at ({target_loc[\'x\']}, {target_loc[\'y\']})")\r\n        \r\n        # Wait for navigation to complete\r\n        start_time = rospy.Time.now().to_sec()\r\n        timeout = 60.0  # 1minute timeout\r\n        \r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        while (rospy.Time.now().to_sec()  start_time) < timeout:\r\n            if not self.navigation_active:\r\n                rospy.loginfo(f"Navigation to {location_name} completed")\r\n                return True\r\n            rate.sleep()\r\n        \r\n        rospy.logerr(f"Navigation to {location_name} timed out")\r\n        return False\r\n    \r\n    def execute_approach_action(self, params: Dict) > bool:\r\n        """Execute object approach action"""\r\n        object_name = params.get("object_name")\r\n        \r\n        if not object_name:\r\n            rospy.logerr("Approach action missing object_name parameter")\r\n            return False\r\n        \r\n        # Find object in detected objects\r\n        detected_objects = self.get_detected_objects()\r\n        target_obj = next((obj for obj in detected_objects if obj["name"] == object_name), None)\r\n        \r\n        if not target_obj:\r\n            rospy.logerr(f"Object {object_name} not detected")\r\n            return False\r\n        \r\n        # Calculate approach pose (1m in front of object)\r\n        approach_x = target_obj["x"]  # Simplified  would need transform computation\r\n        approach_y = target_obj["y"]\r\n        \r\n        goal = PoseStamped()\r\n        goal.header.frame_id = "map"\r\n        goal.header.stamp = rospy.Time.now()\r\n        goal.pose.position.x = approach_x\r\n        goal.pose.position.y = approach_y\r\n        goal.pose.position.z = 0.0\r\n        goal.pose.orientation.w = 1.0\r\n        \r\n        self.goal_pub.publish(goal)\r\n        rospy.loginfo(f"Approach goal sent for {object_name}")\r\n        \r\n        # Wait for approach completion\r\n        start_time = rospy.Time.now().to_sec()\r\n        timeout = 45.0  # 45second timeout for approach\r\n        \r\n        rate = rospy.Rate(10)\r\n        while (rospy.Time.now().to_sec()  start_time) < timeout:\r\n            if not self.navigation_active:\r\n                rospy.loginfo(f"Approach to {object_name} completed")\r\n                return True\r\n            rate.sleep()\r\n        \r\n        rospy.logerr(f"Approach to {object_name} timed out")\r\n        return False\r\n    \r\n    def execute_speak_action(self, params: Dict) > bool:\r\n        """Execute speech response action"""\r\n        text = params.get("text", "")\r\n        if text:\r\n            rospy.loginfo(f"Speaking response: {text}")\r\n            self.publish_feedback(f"Response: {text}")\r\n            return True\r\n        return False\r\n    \r\n    def execute_wait_action(self, params: Dict) > bool:\r\n        """Execute wait action"""\r\n        duration = params.get("duration", 1.0)\r\n        rospy.loginfo(f"Waiting for {duration} seconds")\r\n        rospy.sleep(duration)\r\n        return True\r\n    \r\n    def execute_inspection_action(self, params: Dict) > bool:\r\n        """Execute object inspection action"""\r\n        object_name = params.get("object_name")\r\n        rospy.loginfo(f"Inspecting object: {object_name}")\r\n        \r\n        # In real system, trigger perception routines\r\n        # For simulation, just wait\r\n        rospy.sleep(2.0)\r\n        \r\n        self.publish_feedback(f"Inspection of {object_name} completed")\r\n        return True\r\n    \r\n    def attempt_recovery(self, plan: Dict, failed_step: int, failed_action: Dict) > bool:\r\n        """Attempt to recover from action failure"""\r\n        recovery_strategies = plan.get("failure_recovery", [])\r\n        failed_condition = f"{failed_action.get(\'type\', \'unknown\')}_failed"\r\n        \r\n        # Find recovery strategy\r\n        recovery_strategy = next(\r\n            (rec for rec in recovery_strategies if rec.get("condition") == failed_condition), \r\n            None\r\n        )\r\n        \r\n        if recovery_strategy:\r\n            rospy.loginfo(f"Attempting recovery: {recovery_strategy[\'action\']}")\r\n            # In a real system, implement actual recovery actions\r\n            return True  # Simulate successful recovery\r\n        else:\r\n            rospy.logwarn(f"No recovery strategy for: {failed_condition}")\r\n            return False\r\n    \r\n    def publish_status(self, message: str):\r\n        """Publish system status"""\r\n        status_msg = String()\r\n        status_msg.data = json.dumps({\r\n            "message": message,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.system_status_pub.publish(status_msg)\r\n    \r\n    def publish_feedback(self, message: str):\r\n        """Publish action feedback"""\r\n        feedback_msg = String()\r\n        feedback_msg.data = json.dumps({\r\n            "message": message,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        })\r\n        self.voice_response_pub.publish(feedback_msg)\r\n\r\n   def main():\r\n       api_key = rospy.get_param(\'~openai_api_key\', \'\')\r\n       if not api_key:\r\n           api_key = input("Enter OpenAI API key: ").strip()\r\n       \r\n       if not api_key:\r\n           rospy.logerr("No OpenAI API key provided!")\r\n           return\r\n       \r\n       # Initialize system\r\n       system = CompleteVoiceToActionSystem(api_key)\r\n       \r\n       # Start voice activation if enabled\r\n       system.start_voice_activation()\r\n       \r\n       try:\r\n           rospy.spin()\r\n       except KeyboardInterrupt:\r\n           rospy.loginfo("Shutting down Complete VoicetoAction System")\r\n\r\n   if __name__ == \'__main__\':\r\n       main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"running-the-complete-system",children:"Running the Complete System"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Build the package"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\r\ncolcon build packagesselect robot_voice_control\r\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Run the complete system"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'ros2 launch robot_voice_control voice_to_action_system.launch.py openai_api_key:="<yourapikey>"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Send voice commands"})," (in another terminal):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Simple navigation command\r\nros2 topic pub /voice_command std_msgs/String \"data: 'Go to kitchen'\"\r\n\r\n# Complex command\r\nros2 topic pub /voice_command std_msgs/String \"data: 'Approach the red cup and inspect it'\"\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,a.jsx)(n.p,{children:"Create a complete voicecontrolled robot system that:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implements voice recognition using Whisper for realtime command processing"}),"\n",(0,a.jsx)(n.li,{children:"Integrates with GPT4o for cognitive task planning"}),"\n",(0,a.jsx)(n.li,{children:"Incorporates safety validation at multiple levels (perception, planning, execution)"}),"\n",(0,a.jsx)(n.li,{children:"Creates a multimodal interface connecting voice, vision, and action"}),"\n",(0,a.jsx)(n.li,{children:"Implements error recovery and graceful degradation mechanisms"}),"\n",(0,a.jsx)(n.li,{children:"Evaluates system performance with various input scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Documents the complete system architecture and design decisions"}),"\n",(0,a.jsx)(n.li,{children:"Creates a userfriendly interface for nontechnical operators"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Your project should include:\r\nComplete voice recognition and processing pipeline\r\nGPT4o integration for task planning\r\nMultisensor fusion for enhanced perception\r\nSafety validation throughout the system\r\nError handling and recovery mechanisms\r\nPerformance evaluation and testing\r\nUser interface for command input\r\nDocumentation of system design and decisions"}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered multimodal perception fusion in robotics:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Multimodal Integration"}),": Combining information from different sensor modalities\r\n",(0,a.jsx)(n.strong,{children:"Fusion Architectures"}),": Early, late, and deep fusion approaches\r\n",(0,a.jsx)(n.strong,{children:"TransformerBased Fusion"}),": Using attention mechanisms for crossmodal processing\r\n",(0,a.jsx)(n.strong,{children:"Sensor Calibration"}),": Ensuring proper alignment between modalities\r\n",(0,a.jsx)(n.strong,{children:"Synchronization"}),": Managing temporal alignment of sensor data\r\n",(0,a.jsx)(n.strong,{children:"Fusion Algorithms"}),": Techniques for combining multimodal information\r\n",(0,a.jsx)(n.strong,{children:"Uncertainty Management"}),": Handling uncertainty in fused perceptions\r\n",(0,a.jsx)(n.strong,{children:"Performance Evaluation"}),": Metrics for assessing fusion effectiveness"]}),"\n",(0,a.jsx)(n.p,{children:"Multimodal perception fusion enables robots to develop a more comprehensive understanding of their environment by combining complementary information from multiple sensors, resulting in more robust and accurate perception systems that can handle challenging conditions."})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);