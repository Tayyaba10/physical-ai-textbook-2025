"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[8698],{6142:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vision-language-action/ch17-vision-language-models-robot-perception","title":"ch17-vision-language-models-robot-perception","description":"-----","source":"@site/docs/module-4-vision-language-action/ch17-vision-language-models-robot-perception.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch17-vision-language-models-robot-perception","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch17-vision-language-models-robot-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch17-vision-language-models-robot-perception.md","tags":[],"version":"current","frontMatter":{}}');var i=r(4848),o=r(8453),s=r(7242);const a={},l=void 0,c={},d=[{value:"title: Ch17  VisionLanguage Models for Robot Perception\r\nmodule: 4\r\nchapter: 17\r\nsidebar_label: Ch17: VisionLanguage Models for Robot Perception\r\ndescription: Integrating VisionLanguage Models with robotics for enhanced perception and understanding\r\ntags: [vlm, visionlanguage, robotics, clip, blip, multimodal, perception]\r\ndifficulty: advanced\r\nestimated_duration: 90",id:"title-ch17--visionlanguage-models-for-robot-perceptionmodule-4chapter-17sidebar_label-ch17-visionlanguage-models-for-robot-perceptiondescription-integrating-visionlanguage-models-with-robotics-for-enhanced-perception-and-understandingtags-vlm-visionlanguage-robotics-clip-blip-multimodal-perceptiondifficulty-advancedestimated_duration-90",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"VisionLanguage Models Overview",id:"visionlanguage-models-overview",level:3},{value:"CLIP (Contrastive LanguageImage Pretraining)",id:"clip-contrastive-languageimage-pretraining",level:3},{value:"VisionLanguage Grounding",id:"visionlanguage-grounding",level:3},{value:"Multimodal Embedding Spaces",id:"multimodal-embedding-spaces",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up CLIP Integration with Robotics",id:"lab-1-setting-up-clip-integration-with-robotics",level:3},{value:"Lab 2: Implementing Visual Grounding with Spatial Context",id:"lab-2-implementing-visual-grounding-with-spatial-context",level:3},{value:"Lab 3: Creating a Multimodal Perception Pipeline",id:"lab-3-creating-a-multimodal-perception-pipeline",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"title-ch17--visionlanguage-models-for-robot-perceptionmodule-4chapter-17sidebar_label-ch17-visionlanguage-models-for-robot-perceptiondescription-integrating-visionlanguage-models-with-robotics-for-enhanced-perception-and-understandingtags-vlm-visionlanguage-robotics-clip-blip-multimodal-perceptiondifficulty-advancedestimated_duration-90",children:"title: Ch17  VisionLanguage Models for Robot Perception\r\nmodule: 4\r\nchapter: 17\r\nsidebar_label: Ch17: VisionLanguage Models for Robot Perception\r\ndescription: Integrating VisionLanguage Models with robotics for enhanced perception and understanding\r\ntags: [vlm, visionlanguage, robotics, clip, blip, multimodal, perception]\r\ndifficulty: advanced\r\nestimated_duration: 90"}),"\n","\n",(0,i.jsx)(n.h1,{id:"visionlanguage-models-for-robot-perception",children:"VisionLanguage Models for Robot Perception"}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"Understand VisionLanguage Models (VLMs) and their application in robotics\r\nImplement CLIPbased visual grounding for robotic systems\r\nCreate multimodal perception systems using visionlanguage fusion\r\nIntegrate VLMs with robotic control systems\r\nDevelop object detection and recognition systems enhanced with language understanding\r\nCreate spatial reasoning systems that combine vision and language\r\nEvaluate VLM performance in robotic perception tasks\r\nOptimize VLM inference for realtime robotic applications"}),"\n",(0,i.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,i.jsx)(n.h3,{id:"visionlanguage-models-overview",children:"VisionLanguage Models Overview"}),"\n",(0,i.jsx)(n.p,{children:"VisionLanguage Models (VLMs) represent a breakthrough in artificial intelligence that combines visual understanding with language comprehension. These models learn joint representations of images and text, enabling them to perform tasks that require both visual and linguistic understanding."}),"\n",(0,i.jsx)(s.A,{chart:"\ngraph TD;\n  A[VisionLanguage Models] > B[CLIP];\n  A > C[BLIP];\n  A > D[Flamingo];\n  A > E[BLIP2];\n  A > F[IDEFICS];\n  \n  B > G[Multimodal Encoder];\n  B > H[Contrastive Learning];\n  B > I[ZeroShot Recognition];\n  \n  C > J[Image Captioning];\n  C > K[Visual Question Answering];\n  C > L[ImageText Retrieval];\n  \n  D > M[Sequential Processing];\n  D > N[Recursive Generative];\n  D > O[Openended VisionLanguage Tasks];\n  \n  E > P[VisionLanguage Encoder];\n  E > Q[Language Model Integration];\n  E > R[Generative Capabilities];\n  \n  F > S[Open Vocabulary];\n  F > T[Context Understanding];\n  F > U[Instruction Following];\n  \n  V[Robot Perception] > W[Object Recognition];\n  V > X[Scene Understanding];\n  V > Y[Visual Grounding];\n  V > Z[HumanRobot Interaction];\n  \n  G > W;\n  P > X;\n  M > Y;\n  S > Z;\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style V fill:#FF9800,stroke:#E65100,color:#fff;\n  style W fill:#2196F3,stroke:#0D47A1,color:#fff;\n"}),"\n",(0,i.jsx)(n.h3,{id:"clip-contrastive-languageimage-pretraining",children:"CLIP (Contrastive LanguageImage Pretraining)"}),"\n",(0,i.jsx)(n.p,{children:"CLIP represents a paradigm shift in visual recognition by training a model to match images with their corresponding text descriptions. The model learns visual concepts from natural language supervision, enabling zeroshot transfer to downstream tasks."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Architecture Components:"}),"\r\n",(0,i.jsx)(n.strong,{children:"Image Encoder"}),": Processes images into embeddings (usually Vision Transformer or ResNet)\r\n",(0,i.jsx)(n.strong,{children:"Text Encoder"}),": Processes text into embeddings (usually Transformer)\r\n",(0,i.jsx)(n.strong,{children:"Contrastive Loss"}),": Learns to align matching imagetext pairs while pushing apart nonmatching pairs"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Robotic Applications:"}),"\r\nObject recognition without taskspecific training\r\nVisual grounding for manipulation tasks\r\nScene understanding and context awareness\r\nHumanrobot interaction through natural language"]}),"\n",(0,i.jsx)(n.h3,{id:"visionlanguage-grounding",children:"VisionLanguage Grounding"}),"\n",(0,i.jsx)(n.p,{children:'Visionlanguage grounding connects natural language expressions to visual content, enabling robots to understand commands like "pick up the red mug on the left side of the table."'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Spatial Relations Understanding:"}),"\r\nRelative positioning (left, right, front, behind)\r\nSpatial containment (inside, on top of, under)\r\nSizebased selection (biggest, smallest)\r\nColor and shapebased filtering"]}),"\n",(0,i.jsx)(n.h3,{id:"multimodal-embedding-spaces",children:"Multimodal Embedding Spaces"}),"\n",(0,i.jsx)(n.p,{children:"VLMs create shared embedding spaces where visual and textual information can be directly compared:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Image_I \u2208 R^D \u2190\u2192 Text_T \u2208 R^D\r\nSimilarity(I, T) = cosine(Image_I, Text_T)\n"})}),"\n",(0,i.jsx)(n.p,{children:"This enables robots to perform zeroshot recognition by comparing visual inputs with textual descriptions without requiring labeled training data for each specific object or scene."}),"\n",(0,i.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,i.jsx)(n.h3,{id:"lab-1-setting-up-clip-integration-with-robotics",children:"Lab 1: Setting up CLIP Integration with Robotics"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Install required dependencies"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install openaiclip\r\npip install transformers torch torchvision torchaudio\r\npip install opencvpython\r\npip install rospy sensormsgs cvbridge\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create a CLIPbased object recognizer"})," (",(0,i.jsx)(n.code,{children:"clip_object_recognizer.py"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport clip\r\nimport torch\r\nimport cv2\r\nimport numpy as np\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nfrom typing import List, Dict, Optional\r\nfrom geometry_msgs.msg import Point\r\n\r\nclass ClipObjectRecognizer:\r\n    def __init__(self):\r\n        rospy.init_node(\'clip_object_recognizer\', anonymous=True)\r\n        \r\n        # Load CLIP model\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        self.model, self.preprocess = clip.load("ViTB/32", device=self.device)\r\n        self.model.eval()\r\n        \r\n        # Initialize OpenCV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Publishers and Subscribers\r\n        self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)\r\n        self.recognition_pub = rospy.Publisher("/clip_recognition_results", String, queue_size=10)\r\n        self.annotations_pub = rospy.Publisher("/annotated_image", Image, queue_size=10)\r\n        \r\n        # Object detection parameters\r\n        self.confidence_threshold = rospy.get_param("~confidence_threshold", 0.5)\r\n        self.top_k = rospy.get_param("~top_k", 5)\r\n        \r\n        # Default object categories\r\n        self.default_categories = [\r\n            "person", "bottle", "cup", "fork", "knife", \r\n            "spoon", "bowl", "banana", "apple", "orange",\r\n            "cake", "chair", "couch", "potted plant", "bed",\r\n            "dining table", "toilet", "tv", "laptop", "mouse",\r\n            "remote", "keyboard", "cell phone", "microwave", "oven",\r\n            "toaster", "sink", "refrigerator", "book", "clock",\r\n            "vase", "scissors", "teddy bear", "hair drier", "toothbrush"\r\n        ]\r\n        \r\n        rospy.loginfo("CLIP Object Recognizer initialized")\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Process incoming RGB image"""\r\n        try:\r\n            # Convert ROS Image to OpenCV format\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Run CLIPbased object recognition\r\n            results = self.recognize_objects(cv_image, self.default_categories)\r\n            \r\n            # Publish results\r\n            result_msg = String()\r\n            result_msg.data = str(results)\r\n            self.recognition_pub.publish(result_msg)\r\n            \r\n            # Create annotated image\r\n            annotated_image = self.annotate_image(cv_image, results)\r\n            annotated_msg = self.cv_bridge.cv2_to_imgmsg(annotated_image, "bgr8")\r\n            annotated_msg.header = msg.header\r\n            self.annotations_pub.publish(annotated_msg)\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing image: {e}")\r\n\r\n    def recognize_objects(self, image: np.ndarray, categories: List[str]) > Dict:\r\n        """Recognize objects using CLIP"""\r\n        # Preprocess image\r\n        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\r\n        \r\n        # Tokenize text descriptions\r\n        text_descriptions = [f"a photo of a {cat}" for cat in categories]\r\n        text_tokens = clip.tokenize(text_descriptions).to(self.device)\r\n        \r\n        # Get model predictions\r\n        with torch.no_grad():\r\n            logits_per_image, logits_per_text = self.model(image_tensor, text_tokens)\r\n            probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\r\n        \r\n        # Get topk predictions\r\n        top_indices = np.argsort(probs)[self.top_k:][::1]\r\n        \r\n        results = []\r\n        for idx in top_indices:\r\n            if probs[idx] >= self.confidence_threshold:\r\n                results.append({\r\n                    "category": categories[idx],\r\n                    "confidence": float(probs[idx]),\r\n                    "description": text_descriptions[idx]\r\n                })\r\n        \r\n        return {\r\n            "predictions": results,\r\n            "image_shape": image.shape,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        }\r\n\r\n    def annotate_image(self, image: np.ndarray, results: Dict) > np.ndarray:\r\n        """Add annotation text to image"""\r\n        annotated = image.copy()\r\n        height, width = image.shape[:2]\r\n        \r\n        # Display top predictions\r\n        for i, pred in enumerate(results["predictions"]):\r\n            y_pos = 30 + i * 30\r\n            text = f"{pred[\'category\']}: {pred[\'confidence\']:.2f}"\r\n            \r\n            # Add background rectangle\r\n            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\r\n            cv2.rectangle(annotated, \r\n                        (10, y_pos  text_size[1]  10), \r\n                        (10 + text_size[0], y_pos + 10), \r\n                        (0, 0, 0), 1)\r\n            \r\n            # Add text\r\n            cv2.putText(annotated, text, (10, y_pos), \r\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\r\n        \r\n        return annotated\r\n\r\ndef main():\r\n    recognizer = ClipObjectRecognizer()\r\n    rospy.spin()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-2-implementing-visual-grounding-with-spatial-context",children:"Lab 2: Implementing Visual Grounding with Spatial Context"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a visual grounding system"})," (",(0,i.jsx)(n.code,{children:"visual_grounding_system.py"}),"):","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport clip\r\nimport torch\r\nimport cv2\r\nimport numpy as np\r\nimport openai\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point\r\nfrom cv_bridge import CvBridge\r\nfrom typing import Dict, List, Optional, Tuple\r\nimport json\r\nimport re\r\n\r\nclass VisualGroundingSystem:\r\n    def __init__(self):\r\n        rospy.init_node(\'visual_grounding_system\', anonymous=True)\r\n        \r\n        # Load models\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        self.clip_model, self.clip_preprocess = clip.load("ViTB/32", device=self.device)\r\n        self.clip_model.eval()\r\n        \r\n        # Initialize LLM for spatial reasoning\r\n        self.openai_client = None\r\n        \r\n        # Initialize OpenCV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Publishers and Subscribers\r\n        self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)\r\n        self.command_sub = rospy.Subscriber("/natural_language_command", String, self.command_callback)\r\n        self.grounding_pub = rospy.Publisher("/visual_grounding_results", String, queue_size=10)\r\n        self.region_proposal_pub = rospy.Publisher("/region_proposals", String, queue_size=10)\r\n        \r\n        # Internal state\r\n        self.current_image = None\r\n        self.current_objects = []\r\n        self.current_command = None\r\n        \r\n        # Spatial relation definitions\r\n        self.spatial_relations = {\r\n            "left": ["left of", "to the left of", "left side", "west of"],\r\n            "right": ["right of", "to the right of", "right side", "east of"],\r\n            "above": ["above", "on top of", "over", "up from", "north of"],\r\n            "below": ["below", "under", "beneath", "down from", "south of"],\r\n            "between": ["between", "in between", "in the middle of"],\r\n            "behind": ["behind", "at the back of", "in back of"],\r\n            "in_front_of": ["in front of", "before", "ahead of"],\r\n            "inside": ["inside", "within", "in", "contained by"],\r\n            "outside": ["outside", "beyond", "external to"],\r\n            "near": ["near", "close to", "by", "next to", "beside"]\r\n        }\r\n        \r\n        rospy.loginfo("Visual Grounding System initialized")\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Store current image for grounding operations"""\r\n        try:\r\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Extract objects from current image using CLIP\r\n            self.current_objects = self.extract_objects_in_image(self.current_image)\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing image: {e}")\r\n\r\n    def command_callback(self, msg: String):\r\n        """Process natural language command for visual grounding"""\r\n        self.current_command = msg.data\r\n        rospy.loginfo(f"Processing command: {msg.data}")\r\n        \r\n        if self.current_image is not None:\r\n            result = self.ground_command_in_image(\r\n                self.current_command, \r\n                self.current_image, \r\n                self.current_objects\r\n            )\r\n            \r\n            # Publish grounding result\r\n            result_msg = String()\r\n            result_msg.data = json.dumps(result, indent=2)\r\n            self.grounding_pub.publish(result_msg)\r\n            \r\n            rospy.loginfo(f"Grounding result: {result}")\r\n\r\n    def extract_objects_in_image(self, image: np.ndarray) > List[Dict]:\r\n        """Extract objects using CLIP zeroshot classification"""\r\n        # This is a simplified object extraction  in practice, you\'d use\r\n        # more sophisticated methods like object detection or segmentation\r\n        \r\n        common_objects = [\r\n            "person", "bottle", "cup", "fork", "knife", "spoon", "bowl",\r\n            "banana", "apple", "orange", "cake", "chair", "couch", \r\n            "potted plant", "bed", "dining table", "toilet", "tv", "laptop",\r\n            "mouse", "remote", "keyboard", "cell phone", "microwave", "oven",\r\n            "toaster", "sink", "refrigerator", "book", "clock", "vase",\r\n            "scissors", "teddy bear", "hair drier", "toothbrush", "door",\r\n            "window", "cupboard", "shelf", "cabinet", "box", "bag"\r\n        ]\r\n        \r\n        # Get image features\r\n        image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\r\n        text_descriptions = [f"a photo of a {obj}" for obj in common_objects]\r\n        text_tokens = clip.tokenize(text_descriptions).to(self.device)\r\n        \r\n        with torch.no_grad():\r\n            logits_per_image, _ = self.clip_model(image_tensor, text_tokens)\r\n            probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\r\n        \r\n        # Get objects with confidence above threshold\r\n        threshold = 0.1\r\n        detected_objects = []\r\n        for i, prob in enumerate(probs):\r\n            if prob > threshold:\r\n                # For this example, we\'ll add bounding boxes that represent\r\n                # where we think the object might be located\r\n                bbox = self.estimate_object_bbox(image, common_objects[i])\r\n                detected_objects.append({\r\n                    "name": common_objects[i],\r\n                    "confidence": float(prob),\r\n                    "bbox": bbox,  # [x, y, width, height]\r\n                    "center": [(bbox[0] + bbox[2]/2), (bbox[1] + bbox[3]/2)]  # Center coordinates\r\n                })\r\n        \r\n        return detected_objects\r\n\r\n    def estimate_object_bbox(self, image: np.ndarray, object_name: str) > List[int]:\r\n        """Estimate bounding box for an object in image"""\r\n        # This is a placeholder  in a real implementation, you\'d use\r\n        # object detection models (YOLO, SSD, etc.) to get actual bounding boxes\r\n        height, width = image.shape[:2]\r\n        \r\n        # For demonstration, return a random bounding box\r\n        # In practice, you\'d use actual detection results\r\n        x = np.random.randint(0, width // 2)\r\n        y = np.random.randint(0, height // 2)\r\n        w = np.random.randint(width // 4, width // 2)\r\n        h = np.random.randint(height // 4, height // 2)\r\n        \r\n        return [int(x), int(y), int(w), int(h)]\r\n\r\n    def ground_command_in_image(self, command: str, image: np.ndarray, objects: List[Dict]) > Dict:\r\n        """Ground natural language command in visual scene"""\r\n        # Parse spatial relations from command\r\n        spatial_relations = self.parse_spatial_relations(command)\r\n        \r\n        if spatial_relations:\r\n            # Use spatial reasoning to find target object\r\n            target_object = self.resolve_spatial_query(spatial_relations, objects)\r\n        else:\r\n            # Simple object search\r\n            target_object = self.search_for_object(command, objects)\r\n        \r\n        # Generate region proposals for the target\r\n        region_proposals = self.generate_region_proposals(target_object, image)\r\n        \r\n        # Create result\r\n        result = {\r\n            "command": command,\r\n            "spatial_relations": spatial_relations,\r\n            "target_object": target_object,\r\n            "region_proposals": region_proposals,\r\n            "all_detected_objects": objects,\r\n            "timestamp": rospy.Time.now().to_sec()\r\n        }\r\n        \r\n        return result\r\n\r\n    def parse_spatial_relations(self, command: str) > List[Dict]:\r\n        """Parse spatial relations from natural language command"""\r\n        relations = []\r\n        command_lower = command.lower()\r\n        \r\n        for relation_type, phrases in self.spatial_relations.items():\r\n            for phrase in phrases:\r\n                if phrase in command_lower:\r\n                    # Extract potential object references\r\n                    # This is simplified  a complete implementation would\r\n                    # use more sophisticated NLP parsing\r\n                    parts = command_lower.split(phrase)\r\n                    if len(parts) > 1:\r\n                        subject = self.extract_subject(parts[0])\r\n                        object_ref = self.extract_object_reference(parts[1])\r\n                        \r\n                        relations.append({\r\n                            "relation_type": relation_type,\r\n                            "phrase": phrase,\r\n                            "subject": subject,\r\n                            "reference_object": object_ref,\r\n                            "full_phrase": f"{subject} {phrase} {object_ref}"\r\n                        })\r\n        \r\n        return relations\r\n\r\n    def extract_subject(self, text: str) > str:\r\n        """Extract subject/object noun phrase"""\r\n        # Simplified noun extraction\r\n        # In practice, use NLP parsing (spaCy, NLTK)\r\n        words = text.strip().split()\r\n        if words:\r\n            return words[1]  # Last word as potential object\r\n        return ""\r\n\r\n    def extract_object_reference(self, text: str) > str:\r\n        """Extract object reference from text"""\r\n        # Simplified object reference extraction\r\n        words = text.strip().split()\r\n        if words:\r\n            return words[0]  # First word as potential reference\r\n        return ""\r\n\r\n    def resolve_spatial_query(self, relations: List[Dict], objects: List[Dict]) > Optional[Dict]:\r\n        """Resolve spatial query to find target object"""\r\n        for rel in relations:\r\n            subject = rel.get("subject", "")\r\n            reference_obj = rel.get("reference_object", "")\r\n            relation_type = rel.get("relation_type", "")\r\n            \r\n            # Find reference object\r\n            ref_obj = self.find_object_by_name(reference_obj, objects)\r\n            if not ref_obj:\r\n                continue\r\n            \r\n            # Find subject object based on spatial relation\r\n            target_obj = self.find_object_by_spatial_relation(\r\n                subject, ref_obj, relation_type, objects\r\n            )\r\n            \r\n            if target_obj:\r\n                return target_obj\r\n        \r\n        return None\r\n\r\n    def find_object_by_name(self, name: str, objects: List[Dict]) > Optional[Dict]:\r\n        """Find object by name in objects list"""\r\n        for obj in objects:\r\n            if name.lower() in obj["name"].lower():\r\n                return obj\r\n        return None\r\n\r\n    def find_object_by_spatial_relation(\r\n        self, target_name: str, reference_obj: Dict, relation: str, objects: List[Dict]\r\n    ) > Optional[Dict]:\r\n        """Find object based on spatial relation to reference object"""\r\n        ref_center = reference_obj["center"]\r\n        \r\n        # Filter candidates by name if target name is specified\r\n        candidates = objects\r\n        if target_name:\r\n            candidates = [obj for obj in objects \r\n                        if target_name.lower() in obj["name"].lower()]\r\n        \r\n        # Apply spatial relation filtering\r\n        filtered_candidates = []\r\n        for obj in candidates:\r\n            if obj == reference_obj:  # Skip the reference object itself\r\n                continue\r\n            \r\n            obj_center = obj["center"]\r\n            x_diff = obj_center[0]  ref_center[0]\r\n            y_diff = obj_center[1]  ref_center[1]\r\n            \r\n            # Apply spatial relation filter\r\n            if self.meets_spatial_criteria(x_diff, y_diff, relation):\r\n                filtered_candidates.append(obj)\r\n        \r\n        # Return the candidate with highest confidence\r\n        if filtered_candidates:\r\n            return max(filtered_candidates, key=lambda x: x["confidence"])\r\n        \r\n        return None\r\n\r\n    def meets_spatial_criteria(self, x_diff: float, y_diff: float, relation: str) > bool:\r\n        """Check if spatial relationship is satisfied"""\r\n        EPSILON = 20  # Pixel tolerance for spatial relations\r\n        \r\n        if relation == "left":\r\n            return x_diff < EPSILON\r\n        elif relation == "right":\r\n            return x_diff > EPSILON\r\n        elif relation == "above" or relation == "over":\r\n            return y_diff < EPSILON\r\n        elif relation == "below" or relation == "under":\r\n            return y_diff > EPSILON\r\n        elif relation == "near" or relation == "close to":\r\n            distance = np.sqrt(x_diff**2 + y_diff**2)\r\n            return distance < 100  # Within 100 pixels\r\n        elif relation == "between":\r\n            # This would need more complex logic involving multiple reference points\r\n            return True  # Placeholder\r\n        else:\r\n            return True  # Default to true for other relations\r\n\r\n    def generate_region_proposals(self, target_object: Optional[Dict], image: np.ndarray) > List[Dict]:\r\n        """Generate region proposals for target object"""\r\n        if not target_object:\r\n            return []\r\n        \r\n        bbox = target_object["bbox"]\r\n        center = target_object["center"]\r\n        proposals = []\r\n        \r\n        # Primary region (object bounding box)\r\n        proposals.append({\r\n            "type": "primary",\r\n            "bbox": bbox,\r\n            "center": center,\r\n            "confidence": target_object["confidence"],\r\n            "object_name": target_object["name"]\r\n        })\r\n        \r\n        # Extended region (for context)\r\n        extended_bbox = [\r\n            max(0, bbox[0]  50),  # x\r\n            max(0, bbox[1]  50),  # y\r\n            min(image.shape[1]  bbox[0] + 100, bbox[2] + 100),  # width\r\n            min(image.shape[0]  bbox[1] + 100, bbox[3] + 100)   # height\r\n        ]\r\n        \r\n        proposals.append({\r\n            "type": "extended",\r\n            "bbox": extended_bbox,\r\n            "center": center,\r\n            "confidence": target_object["confidence"] * 0.8,  # Lower confidence for extended region\r\n            "object_name": target_object["name"],\r\n            "is_context": True\r\n        })\r\n        \r\n        return proposals\r\n\r\n    def search_for_object(self, command: str, objects: List[Dict]) > Optional[Dict]:\r\n        """Search for object directly referenced in command"""\r\n        command_lower = command.lower()\r\n        \r\n        for obj in objects:\r\n            if obj["name"].lower() in command_lower:\r\n                return obj\r\n        \r\n        # If no exact match, find object with highest relevance\r\n        best_match = None\r\n        best_score = 0\r\n        \r\n        for obj in objects:\r\n            score = self.calculate_text_similarity(command_lower, obj["name"].lower())\r\n            if score > best_score:\r\n                best_score = score\r\n                best_match = obj\r\n        \r\n        return best_match\r\n\r\n    def calculate_text_similarity(self, text1: str, text2: str) > float:\r\n        """Calculate simple text similarity"""\r\n        words1 = set(text1.split())\r\n        words2 = set(text2.split())\r\n        \r\n        intersection = words1.intersection(words2)\r\n        union = words1.union(words2)\r\n        \r\n        if len(union) == 0:\r\n            return 0.0\r\n        \r\n        return len(intersection) / len(union)\r\n\r\ndef main():\r\n    system = VisualGroundingSystem()\r\n    rospy.spin()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-3-creating-a-multimodal-perception-pipeline",children:"Lab 3: Creating a Multimodal Perception Pipeline"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a complete multimodal perception pipeline"})," (",(0,i.jsx)(n.code,{children:"multimodal_perception_pipeline.py"}),"):","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport clip\r\nimport torch\r\nimport cv2\r\nimport numpy as np\r\nimport openai\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point, Pose, PoseStamped\r\nfrom cv_bridge import CvBridge\r\nfrom typing import Dict, List, Optional, Tuple\r\nimport json\r\nimport threading\r\nimport queue\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass PerceptionResult:\r\n    """Data structure for perception results"""\r\n    timestamp: float\r\n    objects: List[Dict]\r\n    spatial_relations: List[Dict]\r\n    scene_description: str\r\n    confidence: float\r\n    regions_of_interest: List[Dict]\r\n\r\nclass MultimodalPerceptionPipeline:\r\n    def __init__(self):\r\n        rospy.init_node(\'multimodal_perception_pipeline\', anonymous=True)\r\n        \r\n        # Initialize models\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        self.clip_model, self.clip_preprocess = clip.load("ViTB/32", device=self.device)\r\n        self.clip_model.eval()\r\n        \r\n        # Initialize OpenCV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Publishers and Subscribers\r\n        self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)\r\n        self.depth_sub = rospy.Subscriber("/camera/depth/image_raw", Image, self.depth_callback)\r\n        self.command_sub = rospy.Subscriber("/natural_language_query", String, self.query_callback)\r\n        \r\n        self.perception_pub = rospy.Publisher("/multimodal_perception", String, queue_size=10)\r\n        self.roi_pub = rospy.Publisher("/regions_of_interest", String, queue_size=10)\r\n        self.scene_description_pub = rospy.Publisher("/scene_description", String, queue_size=10)\r\n        \r\n        # Data storage\r\n        self.current_image = None\r\n        self.current_depth = None\r\n        self.current_objects = []\r\n        self.perception_queue = queue.Queue()\r\n        self.processing_thread = None\r\n        self.is_running = False\r\n        \r\n        # Parameters\r\n        self.confidence_threshold = rospy.get_param("~confidence_threshold", 0.1)\r\n        self.spatial_threshold = rospy.get_param("~spatial_threshold", 0.2)\r\n        \r\n        # Start processing thread\r\n        self.start_processing_pipeline()\r\n        \r\n        rospy.loginfo("Multimodal Perception Pipeline initialized")\r\n\r\n    def start_processing_pipeline(self):\r\n        """Start the multimodal processing pipeline"""\r\n        self.is_running = True\r\n        self.processing_thread = threading.Thread(target=self.processing_loop)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n    def stop_processing_pipeline(self):\r\n        """Stop the multimodal processing pipeline"""\r\n        self.is_running = False\r\n        if self.processing_thread:\r\n            self.processing_thread.join()\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Process incoming image data"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            self.current_image = cv_image\r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing image: {e}")\r\n\r\n    def depth_callback(self, msg: Image):\r\n        """Process incoming depth data"""\r\n        try:\r\n            cv_depth = self.cv_bridge.imgmsg_to_cv2(msg, "32FC1")\r\n            self.current_depth = cv_depth\r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing depth: {e}")\r\n\r\n    def query_callback(self, msg: String):\r\n        """Process natural language query for multimodal analysis"""\r\n        query = msg.data\r\n        rospy.loginfo(f"Processing query: {query}")\r\n        \r\n        if self.current_image is not None:\r\n            # Create a perception job\r\n            job = {\r\n                "query": query,\r\n                "image": self.current_image,\r\n                "depth": self.current_depth,\r\n                "timestamp": rospy.Time.now().to_sec()\r\n            }\r\n            \r\n            # Add to processing queue\r\n            try:\r\n                self.perception_queue.put_nowait(job)\r\n            except queue.Full:\r\n                rospy.logwarn("Perception queue is full, dropping query")\r\n\r\n    def processing_loop(self):\r\n        """Main processing loop for multimodal perception"""\r\n        rate = rospy.Rate(5)  # Process at most 5 Hz to avoid overwhelming\r\n        \r\n        while self.is_running and not rospy.is_shutdown():\r\n            try:\r\n                # Check for new jobs\r\n                job = self.perception_queue.get(timeout=0.1)\r\n                \r\n                # Process the job\r\n                result = self.process_multimodal_query(\r\n                    job["query"],\r\n                    job["image"],\r\n                    job["depth"]\r\n                )\r\n                \r\n                if result:\r\n                    # Publish results\r\n                    self.publish_perception_result(result)\r\n                    \r\n            except queue.Empty:\r\n                # No new jobs, continue loop\r\n                pass\r\n            except Exception as e:\r\n                rospy.logerr(f"Error in processing loop: {e}")\r\n            \r\n            rate.sleep()\r\n\r\n    def process_multimodal_query(self, query: str, image: np.ndarray, depth: Optional[np.ndarray] = None) > Optional[PerceptionResult]:\r\n        """Process a multimodal query"""\r\n        try:\r\n            # Extract objects using CLIP\r\n            objects = self.extract_objects_with_clip(image)\r\n            \r\n            # Analyze spatial relationships\r\n            spatial_relations = self.analyze_spatial_relations(objects)\r\n            \r\n            # Generate scene description using LLM\r\n            scene_description = self.generate_scene_description(query, objects, spatial_relations)\r\n            \r\n            # Identify regions of interest\r\n            roi = self.identify_regions_of_interest(query, objects, spatial_relations)\r\n            \r\n            # Calculate overall confidence\r\n            confidence = self.calculate_overall_confidence(objects, spatial_relations)\r\n            \r\n            result = PerceptionResult(\r\n                timestamp=rospy.Time.now().to_sec(),\r\n                objects=objects,\r\n                spatial_relations=spatial_relations,\r\n                scene_description=scene_description,\r\n                confidence=confidence,\r\n                regions_of_interest=roi\r\n            )\r\n            \r\n            return result\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing multimodal query: {e}")\r\n            return None\r\n\r\n    def extract_objects_with_clip(self, image: np.ndarray) > List[Dict]:\r\n        """Extract objects using CLIP zeroshot classification"""\r\n        # Common object categories for household/industrial environments\r\n        categories = [\r\n            "person", "bottle", "cup", "fork", "knife", "spoon", "bowl",\r\n            "banana", "apple", "orange", "cake", "chair", "couch", \r\n            "potted plant", "bed", "dining table", "toilet", "tv", "laptop",\r\n            "mouse", "remote", "keyboard", "cell phone", "microwave", "oven",\r\n            "toaster", "sink", "refrigerator", "book", "clock", "vase",\r\n            "scissors", "teddy bear", "hair drier", "toothbrush", "door",\r\n            "window", "cupboard", "shelf", "cabinet", "box", "bag",\r\n            "robot", "humanoid", "arm", "leg", "wheel", "sensor", "lidar",\r\n            "camera", "display", "button", "lever", "knob", "handle",\r\n            "cord", "pipe", "wire", "cable", "structure", "surface"\r\n        ]\r\n        \r\n        # Preprocess image\r\n        image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\r\n        text_descriptions = [f"a photo of a {cat}" for cat in categories]\r\n        text_tokens = clip.tokenize(text_descriptions).to(self.device)\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            logits_per_image, _ = self.clip_model(image_tensor, text_tokens)\r\n            probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\r\n        \r\n        # Get objects above confidence threshold\r\n        detected_objects = []\r\n        for i, prob in enumerate(probs):\r\n            if prob > self.confidence_threshold:\r\n                # Estimate bounding box and position\r\n                bbox = self.estimate_object_bbox_with_clip(categories[i], image)\r\n                detected_objects.append({\r\n                    "name": categories[i],\r\n                    "confidence": float(prob),\r\n                    "bbox": bbox,\r\n                    "center": [bbox[0] + bbox[2]/2, bbox[1] + bbox[3]/2],\r\n                    "description": text_descriptions[i]\r\n                })\r\n        \r\n        # Sort by confidence (highest first)\r\n        detected_objects.sort(key=lambda x: x["confidence"], reverse=True)\r\n        return detected_objects\r\n\r\n    def estimate_object_bbox_with_clip(self, object_name: str, image: np.ndarray) > List[int]:\r\n        """Estimate bounding box for object using CLIPguided approach"""\r\n        # This is a simplified approach  in practice you\'d use\r\n        # more sophisticated techniques like sliding window or attention visualization\r\n        height, width = image.shape[:2]\r\n        \r\n        # For this example, we\'ll use a simple heuristic based on object size\r\n        # In reality, you\'d use attention maps or other methods to localize objects\r\n        \r\n        # Define typical object sizes (as percentage of image)\r\n        object_size_mapping = {\r\n            "person": (0.3, 0.8),  # height range\r\n            "chair": (0.2, 0.5),\r\n            "table": (0.4, 0.9),\r\n            "couch": (0.3, 0.7),\r\n            "bottle": (0.1, 0.3),\r\n            "cup": (0.05, 0.15),\r\n            "laptop": (0.2, 0.3),\r\n            "phone": (0.05, 0.1),\r\n            "door": (0.6, 0.9),\r\n            "window": (0.1, 0.8)\r\n        }\r\n        \r\n        typical_height_range = object_size_mapping.get(object_name, (0.1, 0.4))\r\n        typical_width_range = object_size_mapping.get(object_name, (0.1, 0.4))\r\n        \r\n        # Random position but within reasonable ranges\r\n        h_pct = np.random.uniform(typical_height_range[0], typical_height_range[1])\r\n        w_pct = np.random.uniform(typical_width_range[0], typical_width_range[1])\r\n        \r\n        h = int(h_pct * height)\r\n        w = int(w_pct * width)\r\n        \r\n        x = np.random.randint(0, width  w)\r\n        y = np.random.randint(0, height  h)\r\n        \r\n        return [int(x), int(y), int(w), int(h)]\r\n\r\n    def analyze_spatial_relations(self, objects: List[Dict]) > List[Dict]:\r\n        """Analyze spatial relationships between objects"""\r\n        relations = []\r\n        \r\n        for i, obj1 in enumerate(objects):\r\n            for j, obj2 in enumerate(objects):\r\n                if i != j and obj1["confidence"] > 0.3 and obj2["confidence"] > 0.3:\r\n                    # Calculate spatial relationship\r\n                    rel = self.calculate_spatial_relationship(obj1, obj2)\r\n                    if rel:\r\n                        relations.append(rel)\r\n        \r\n        return relations\r\n\r\n    def calculate_spatial_relationship(self, obj1: Dict, obj2: Dict) > Optional[Dict]:\r\n        """Calculate spatial relationship between two objects"""\r\n        center1 = obj1["center"]\r\n        center2 = obj2["center"]\r\n        \r\n        x_diff = center1[0]  center2[0]\r\n        y_diff = center1[1]  center2[1]\r\n        distance = np.sqrt(x_diff**2 + y_diff**2)\r\n        \r\n        # Determine spatial relation based on relative positions\r\n        if abs(x_diff) > abs(y_diff):\r\n            if x_diff > 0:\r\n                direction = "right"\r\n                relation = f"{obj1[\'name\']} is to the right of {obj2[\'name\']}"\r\n            else:\r\n                direction = "left" \r\n                relation = f"{obj1[\'name\']} is to the left of {obj2[\'name\']}"\r\n        else:\r\n            if y_diff > 0:\r\n                direction = "below"\r\n                relation = f"{obj1[\'name\']} is below {obj2[\'name\']}"\r\n            else:\r\n                direction = "above"\r\n                relation = f"{obj1[\'name\']} is above {obj2[\'name\']}"\r\n        \r\n        # Check proximity\r\n        proximity = "near" if distance < 100 else "far"\r\n        \r\n        return {\r\n            "subject": obj1["name"],\r\n            "relation": direction,\r\n            "object": obj2["name"],\r\n            "distance_pixels": distance,\r\n            "proximity": proximity,\r\n            "description": relation,\r\n            "confidence": min(obj1["confidence"], obj2["confidence"]) * 0.8\r\n        }\r\n\r\n    def generate_scene_description(self, query: str, objects: List[Dict], spatial_relations: List[Dict]) > str:\r\n        """Generate scene description using LLM"""\r\n        # This would use an LLM in practice\r\n        # For now, create a simple description\r\n        if not objects:\r\n            return "The scene appears to be empty or no objects were detected."\r\n        \r\n        # Create object summary\r\n        obj_names = [obj["name"] for obj in objects[:5]]  # Top 5 objects\r\n        obj_list = ", ".join(obj_names)\r\n        \r\n        # Create spatial summary\r\n        spatial_descriptions = []\r\n        for rel in spatial_relations[:3]:  # Top 3 relations\r\n            spatial_descriptions.append(rel["description"])\r\n        \r\n        spatial_summary = "; ".join(spatial_descriptions) if spatial_descriptions else "Objects appear distributed without clear spatial relationships."\r\n        \r\n        description = f"This scene contains: {obj_list}. {spatial_summary}"\r\n        \r\n        return description\r\n\r\n    def identify_regions_of_interest(self, query: str, objects: List[Dict], spatial_relations: List[Dict]) > List[Dict]:\r\n        """Identify regions of interest based on query"""\r\n        roi = []\r\n        \r\n        # Simple approach: match query keywords to objects\r\n        query_lower = query.lower()\r\n        \r\n        for obj in objects:\r\n            if any(keyword in query_lower for keyword in obj["name"].split()):\r\n                roi.append({\r\n                    "object_name": obj["name"],\r\n                    "bbox": obj["bbox"],\r\n                    "confidence": obj["confidence"],\r\n                    "reason": f"Matches query keyword in \'{obj[\'name\']}\'"\r\n                })\r\n        \r\n        # If no direct matches, return top objects\r\n        if not roi and objects:\r\n            for obj in objects[:3]:  # Top 3 objects\r\n                roi.append({\r\n                    "object_name": obj["name"],\r\n                    "bbox": obj["bbox"],\r\n                    "confidence": obj["confidence"],\r\n                    "reason": f"High confidence object: {obj[\'confidence\']:.2f}"\r\n                })\r\n        \r\n        return roi\r\n\r\n    def calculate_overall_confidence(self, objects: List[Dict], spatial_relations: List[Dict]) > float:\r\n        """Calculate overall perception confidence"""\r\n        if not objects:\r\n            return 0.0\r\n        \r\n        # Average of object confidences weighted by number\r\n        object_confidence = sum(obj["confidence"] for obj in objects) / len(objects) if objects else 0.0\r\n        \r\n        # Spatial relation confidence (if any exist)\r\n        spatial_confidence = sum(rel["confidence"] for rel in spatial_relations) / len(spatial_relations) if spatial_relations else 0.0\r\n        \r\n        # Weighted combination\r\n        overall_conf = (0.7 * object_confidence + 0.3 * spatial_confidence)\r\n        \r\n        return min(overall_conf, 1.0)  # Clamp to [0, 1]\r\n\r\n    def publish_perception_result(self, result: PerceptionResult):\r\n        """Publish perception results"""\r\n        # Publish main perception result\r\n        perception_msg = String()\r\n        perception_msg.data = json.dumps({\r\n            "timestamp": result.timestamp,\r\n            "objects": result.objects,\r\n            "spatial_relations": result.spatial_relations,\r\n            "scene_description": result.scene_description,\r\n            "confidence": result.confidence,\r\n            "query_time": rospy.Time.now().to_sec()\r\n        }, indent=2)\r\n        self.perception_pub.publish(perception_msg)\r\n        \r\n        # Publish regions of interest\r\n        roi_msg = String()\r\n        roi_msg.data = json.dumps({\r\n            "timestamp": result.timestamp,\r\n            "regions_of_interest": result.regions_of_interest,\r\n            "total_objects": len(result.objects)\r\n        })\r\n        self.roi_pub.publish(roi_msg)\r\n        \r\n        # Publish scene description separately\r\n        description_msg = String()\r\n        description_msg.data = result.scene_description\r\n        self.scene_description_pub.publish(description_msg)\r\n\r\ndef main():\r\n    pipeline = MultimodalPerceptionPipeline()\r\n    \r\n    try:\r\n        rospy.spin()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down multimodal perception pipeline...")\r\n        pipeline.stop_processing_pipeline()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,i.jsx)(n.p,{children:"Here's a complete example that demonstrates the integration of all components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_vlm_robot_perception.py\r\n\r\nimport rospy\r\nimport clip\r\nimport torch\r\nimport cv2\r\nimport numpy as np\r\nimport openai\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point\r\nfrom cv_bridge import CvBridge\r\nfrom typing import Dict, List, Optional\r\nimport json\r\nimport time\r\nfrom multimodal_perception_pipeline import MultimodalPerceptionPipeline\r\nfrom visual_grounding_system import VisualGroundingSystem\r\nfrom clip_object_recognizer import ClipObjectRecognizer\r\n\r\nclass CompleteVLMPipeline:\r\n   """Complete VisionLanguage Model pipeline for robotic perception"""\r\n   \r\n   def __init__(self):\r\n       rospy.init_node(\'complete_vlm_pipeline\', anonymous=True)\r\n       \r\n       # Initialize individual components\r\n       self.multimodal_pipeline = MultimodalPerceptionPipeline()\r\n       self.visual_grounding = VisualGroundingSystem()\r\n       self.clip_recognizer = ClipObjectRecognizer()\r\n       \r\n       # Publishers\r\n       self.system_status_pub = rospy.Publisher(\'/vlm_system_status\', String, queue_size=10)\r\n       self.integrated_result_pub = rospy.Publisher(\'/integrated_vlm_result\', String, queue_size=10)\r\n       \r\n       # Subscribers\r\n       rospy.Subscriber(\'/robot_command\', String, self.robot_command_callback)\r\n       rospy.Subscriber(\'/vlm_perception\', String, self.perception_result_callback)\r\n       rospy.Subscriber(\'/visual_grounding\', String, self.grounding_result_callback)\r\n       \r\n       # System state\r\n       self.latest_perception = None\r\n       self.latest_grounding = None\r\n       self.system_ready = True\r\n       \r\n       rospy.loginfo("Complete VLM Robotics Pipeline initialized")\r\n   \r\n   def robot_command_callback(self, msg: String):\r\n       """Handle robot commands that require VLM processing"""\r\n       command = msg.data\r\n       rospy.loginfo(f"Processing robot command: {command}")\r\n       \r\n       # Update system status\r\n       status_msg = String()\r\n       status_msg.data = f"Processing command: {command}"\r\n       self.system_status_pub.publish(status_msg)\r\n       \r\n       # Determine command type and route appropriately\r\n       if self.contains_visual_reference(command):\r\n           # This command likely requires visual grounding\r\n           self.route_to_grounding(command)\r\n       else:\r\n           # This command likely requires general scene understanding\r\n           self.route_to_perception(command)\r\n   \r\n   def contains_visual_reference(self, command: str) > bool:\r\n       """Check if command contains visual references"""\r\n       visual_keywords = [\r\n           \'the\', \'that\', \'there\', \'left\', \'right\', \'front\', \'behind\', \r\n           \'above\', \'below\', \'on\', \'in\', \'near\', \'next to\', \'beside\',\r\n           \'red\', \'blue\', \'green\', \'large\', \'small\', \'big\', \'little\'\r\n       ]\r\n       \r\n       command_lower = command.lower()\r\n       return any(keyword in command_lower for keyword in visual_keywords)\r\n   \r\n   def route_to_grounding(self, command: str):\r\n       """Route command to visual grounding system"""\r\n       # Publish to visual grounding system\r\n       cmd_msg = String()\r\n       cmd_msg.data = command\r\n       self.grounding_command_pub.publish(cmd_msg)\r\n   \r\n   def route_to_perception(self, command: str):\r\n       """Route command to perception system"""\r\n       # For general perception, we\'ll use the command to focus attention\r\n       # Publish to perception system along with current context\r\n       context_msg = String()\r\n       context_msg.data = json.dumps({\r\n           "command": command,\r\n           "focus_attention": self.extract_attention_targets(command)\r\n       })\r\n       self.perception_context_pub.publish(context_msg)\r\n   \r\n   def extract_attention_targets(self, command: str) > List[str]:\r\n       """Extract potential attention targets from command"""\r\n       # Simple keyword extraction  in practice, use more sophisticated NLP\r\n       words = command.lower().split()\r\n       targets = []\r\n       \r\n       # Common object words that might be targets\r\n       potential_targets = [\r\n           \'object\', \'item\', \'thing\', \'robot\', \'person\', \'table\', \'chair\',\r\n           \'bottle\', \'cup\', \'box\', \'door\', \'window\', \'cabinet\', \'shelf\'\r\n       ]\r\n       \r\n       for word in words:\r\n           # Remove punctuation\r\n           clean_word = word.strip(\'.,!?;:\')\r\n           if clean_word in potential_targets:\r\n               targets.append(clean_word)\r\n       \r\n       # Also look for color + object combinations\r\n       for i in range(len(words)1):\r\n           if words[i].lower() in [\'red\', \'blue\', \'green\', \'yellow\', \'black\', \'white\'] and words[i+1] in potential_targets:\r\n               targets.append(f"{words[i]} {words[i+1]}")\r\n       \r\n       return targets\r\n   \r\n   def perception_result_callback(self, msg: String):\r\n       """Handle perception results"""\r\n       try:\r\n           result = json.loads(msg.data)\r\n           self.latest_perception = result\r\n           \r\n           # Integrate with other modalities\r\n           integrated_result = self.integrate_perception_results()\r\n           \r\n           if integrated_result:\r\n               self.publish_integrated_result(integrated_result)\r\n               \r\n       except json.JSONDecodeError:\r\n           rospy.logerr("Error decoding perception result")\r\n   \r\n   def grounding_result_callback(self, msg: String):\r\n       """Handle visual grounding results"""\r\n       try:\r\n           result = json.loads(msg.data)\r\n           self.latest_grounding = result\r\n           \r\n           # Integrate with other modalities\r\n           integrated_result = self.integrate_grounding_results()\r\n           \r\n           if integrated_result:\r\n               self.publish_integrated_result(integrated_result)\r\n               \r\n       except json.JSONDecodeError:\r\n           rospy.logerr("Error decoding grounding result")\r\n   \r\n   def integrate_perception_results(self) > Optional[Dict]:\r\n       """Integrate perception results with system state"""\r\n       if not self.latest_perception:\r\n           return None\r\n       \r\n       integrated_result = {\r\n           "timestamp": time.time(),\r\n           "source": "perception",\r\n           "data": self.latest_perception,\r\n           "system_confidence": self.calculate_system_confidence(),\r\n           "recommendations": self.generate_recommendations(self.latest_perception)\r\n       }\r\n       \r\n       return integrated_result\r\n   \r\n   def integrate_grounding_results(self) > Optional[Dict]:\r\n       """Integrate grounding results with system state"""\r\n       if not self.latest_grounding:\r\n           return None\r\n       \r\n       integrated_result = {\r\n           "timestamp": time.time(),\r\n           "source": "grounding",\r\n           "data": self.latest_grounding,\r\n           "system_confidence": self.calculate_system_confidence(),\r\n           "actionables": self.extract_actionable_information(self.latest_grounding)\r\n       }\r\n       \r\n       return integrated_result\r\n   \r\n   def calculate_system_confidence(self) > float:\r\n       """Calculate overall system confidence"""\r\n       # This is a simplified confidence calculation\r\n       # In practice, use more sophisticated methods\r\n       confidence_components = []\r\n       \r\n       if self.latest_perception:\r\n           confidence_components.append(self.latest_perception.get("confidence", 0.5))\r\n       \r\n       if self.latest_grounding:\r\n           # Grounding confidence might come from spatial relation confidence\r\n           relations = self.latest_grounding.get("spatial_relations", [])\r\n           if relations:\r\n               avg_conf = sum(r.get("confidence", 0.5) for r in relations) / len(relations)\r\n               confidence_components.append(avg_conf)\r\n           else:\r\n               confidence_components.append(0.3)  # Lower confidence if no spatial relations\r\n       \r\n       if confidence_components:\r\n           avg_confidence = sum(confidence_components) / len(confidence_components)\r\n           return min(avg_confidence, 1.0)\r\n       else:\r\n           return 0.5  # Default confidence\r\n   \r\n   def generate_recommendations(self, perception_data: Dict) > List[str]:\r\n       """Generate recommendations based on perception data"""\r\n       recommendations = []\r\n       objects = perception_data.get("objects", [])\r\n       \r\n       if len(objects) > 5:\r\n           recommendations.append("Scene is complex with many objects detected")\r\n       \r\n       if any(obj["confidence"] > 0.9 for obj in objects):\r\n           recommendations.append("Highconfidence object detections available")\r\n       \r\n       if any("person" in obj["name"] for obj in objects):\r\n           recommendations.append("Human detected  consider social interaction protocols")\r\n       \r\n       # Add other recommendation logic based on objects detected\r\n       manipulable_objects = [\r\n           obj for obj in objects \r\n           if obj["name"] in ["bottle", "cup", "box", "book", "phone", "laptop"]\r\n       ]\r\n       \r\n       if manipulable_objects:\r\n           recommendations.append(f"Found {len(manipulable_objects)} potentially manipulable objects")\r\n       \r\n       return recommendations\r\n   \r\n   def extract_actionable_information(self, grounding_data: Dict) > List[Dict]:\r\n       """Extract actionable information from grounding results"""\r\n       actionables = []\r\n       \r\n       # Extract target object for manipulation\r\n       target_obj = grounding_data.get("target_object")\r\n       if target_obj:\r\n           actionables.append({\r\n               "type": "manipulation_target",\r\n               "object": target_obj["name"],\r\n               "bbox": target_obj["bbox"],\r\n               "confidence": target_obj["confidence"]\r\n           })\r\n       \r\n       # Extract regions of interest\r\n       roi = grounding_data.get("region_proposals", [])\r\n       for region in roi:\r\n           if region.get("type") == "primary":\r\n               actionables.append({\r\n                   "type": "region_of_interest",\r\n                   "bbox": region["bbox"],\r\n                   "object_name": region["object_name"],\r\n                   "confidence": region["confidence"]\r\n               })\r\n       \r\n       return actionables\r\n   \r\n   def publish_integrated_result(self, result: Dict):\r\n       """Publish integrated VLM results"""\r\n       result_msg = String()\r\n       result_msg.data = json.dumps(result, indent=2)\r\n       self.integrated_result_pub.publish(result_msg)\r\n       \r\n       # Update system status\r\n       status_msg = String()\r\n       status_msg.data = f"Integrated result published with {result.get(\'source\', \'unknown\')} data"\r\n       self.system_status_pub.publish(status_msg)\r\n   \r\n   def run(self):\r\n       """Run the complete VLM pipeline"""\r\n       rospy.loginfo("Starting Complete VLM Robotics Pipeline...")\r\n       \r\n       try:\r\n           rospy.spin()\r\n       except KeyboardInterrupt:\r\n           rospy.loginfo("Shutting down Complete VLM Robotics Pipeline...")\r\n\r\ndef main():\r\n   pipeline = CompleteVLMPipeline()\r\n   pipeline.run()\r\n\r\nif __name__ == \'__main__\':\r\n   main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete multimodal perception system that integrates:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Visionlanguage model for object recognition and scene understanding"}),"\n",(0,i.jsx)(n.li,{children:"Spatial reasoning for locationbased queries"}),"\n",(0,i.jsx)(n.li,{children:"Integration with robotic manipulation planning"}),"\n",(0,i.jsx)(n.li,{children:"Realtime processing capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Confidence estimation for all perception outputs"}),"\n",(0,i.jsx)(n.li,{children:"Error handling and fallback mechanisms"}),"\n",(0,i.jsx)(n.li,{children:"Performance evaluation metrics"}),"\n",(0,i.jsx)(n.li,{children:"Humanrobot interaction through natural language"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Your project should demonstrate:\r\nComplete VLM integration with robotics system\r\nEffective visual grounding for robotic tasks\r\nMultimodal data fusion for enhanced perception\r\nRealtime performance with appropriate optimization\r\nRobustness to different lighting and environmental conditions\r\nNatural language interaction with the robotic system"}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered VisionLanguage Models in robotics:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"VLM Fundamentals"}),": Understanding contrastive learning and joint visuallinguistic embeddings\r\n",(0,i.jsx)(n.strong,{children:"CLIP Integration"}),": Implementing zeroshot object recognition with VisionLanguage models\r\n",(0,i.jsx)(n.strong,{children:"Spatial Reasoning"}),": Creating systems that understand spatial relationships between objects\r\n",(0,i.jsx)(n.strong,{children:"Multimodal Fusion"}),": Combining visual and linguistic information for enhanced perception\r\n",(0,i.jsx)(n.strong,{children:"Visual Grounding"}),": Connecting natural language expressions to visual content\r\n",(0,i.jsx)(n.strong,{children:"Realtime Processing"}),": Optimizing VLMs for robotic applications\r\n",(0,i.jsx)(n.strong,{children:"Confidence Estimation"}),": Assessing the reliability of perception outputs\r\n",(0,i.jsx)(n.strong,{children:"Error Handling"}),": Creating robust systems that handle uncertain perception"]}),"\n",(0,i.jsx)(n.p,{children:"VisionLanguage Models enable robots to understand their environment in more humanlike ways, connecting visual perception with natural language concepts. This capability is essential for creating robots that can interact naturally with humans and understand complex, languagedescribed tasks in diverse environments."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);