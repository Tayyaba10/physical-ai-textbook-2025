"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[8160],{5297:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real","title":"ch15-reinforcement-learning-sim2real","description":"-----","source":"@site/docs/module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-3-ai-robot-brain/ch15-reinforcement-learning-sim2real.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch14-bipedal-locomotion-balance","permalink":"/physical-ai-textbook-2025/docs/module-3-ai-robot-brain/ch14-bipedal-locomotion-balance"},"next":{"title":"ch16-llms-meet-robotics","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch16-llms-meet-robotics"}}');var t=r(4848),i=r(8453),s=r(7242);const o={},l=void 0,d={},c=[{value:"title: Ch15  Reinforcement Learning &amp; SimtoReal\r\nmodule: 3\r\nchapter: 15\r\nsidebar_label: Ch15: Reinforcement Learning &amp; SimtoReal\r\ndescription: Implementing reinforcement learning algorithms for robotics and transferring policies from simulation to reality\r\ntags: [reinforcementlearning, rl, simtoreal, transferlearning, robotics, Isaacgym, Isaacorbit, domainrandomization]\r\ndifficulty: advanced\r\nestimated_duration: 150",id:"title-ch15--reinforcement-learning--simtorealmodule-3chapter-15sidebar_label-ch15-reinforcement-learning--simtorealdescription-implementing-reinforcement-learning-algorithms-for-robotics-and-transferring-policies-from-simulation-to-realitytags-reinforcementlearning-rl-simtoreal-transferlearning-robotics-isaacgym-isaacorbit-domainrandomizationdifficulty-advancedestimated_duration-150",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"Reinforcement Learning in Robotics",id:"reinforcement-learning-in-robotics",level:3},{value:"Types of RL Algorithms for Robotics",id:"types-of-rl-algorithms-for-robotics",level:3},{value:"SimtoReal Transfer Challenges",id:"simtoreal-transfer-challenges",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up Isaac Gym for RL Training",id:"lab-1-setting-up-isaac-gym-for-rl-training",level:3},{value:"Lab 2: Implementing SAC Algorithm for Robot Control",id:"lab-2-implementing-sac-algorithm-for-robot-control",level:3},{value:"Lab 3: Domain Randomization for SimtoReal Transfer",id:"lab-3-domain-randomization-for-simtoreal-transfer",level:3},{value:"Lab 4: Evaluating SimtoReal Transfer",id:"lab-4-evaluating-simtoreal-transfer",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"title-ch15--reinforcement-learning--simtorealmodule-3chapter-15sidebar_label-ch15-reinforcement-learning--simtorealdescription-implementing-reinforcement-learning-algorithms-for-robotics-and-transferring-policies-from-simulation-to-realitytags-reinforcementlearning-rl-simtoreal-transferlearning-robotics-isaacgym-isaacorbit-domainrandomizationdifficulty-advancedestimated_duration-150",children:"title: Ch15  Reinforcement Learning & SimtoReal\r\nmodule: 3\r\nchapter: 15\r\nsidebar_label: Ch15: Reinforcement Learning & SimtoReal\r\ndescription: Implementing reinforcement learning algorithms for robotics and transferring policies from simulation to reality\r\ntags: [reinforcementlearning, rl, simtoreal, transferlearning, robotics, Isaacgym, Isaacorbit, domainrandomization]\r\ndifficulty: advanced\r\nestimated_duration: 150"}),"\n","\n",(0,t.jsx)(e.h1,{id:"reinforcement-learning--simtoreal",children:"Reinforcement Learning & SimtoReal"}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"Understand reinforcement learning applications in robotics\r\nImplement RL algorithms for continuous control tasks\r\nApply domain randomization techniques for simtoreal transfer\r\nEvaluate policy robustness across simulationtoreality gap\r\nImplement system identification and dynamics randomization\r\nCreate robust control policies for physical systems\r\nAssess the effectiveness of simtoreal transfer methods"}),"\n",(0,t.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,t.jsx)(e.h3,{id:"reinforcement-learning-in-robotics",children:"Reinforcement Learning in Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Reinforcement Learning (RL) is particularly applicable to robotics because robots can continuously interact with their environment and learn from trial and error. In robotics, RL agents learn to perform tasks by taking actions in an environment and receiving rewards based on their performance."}),"\n",(0,t.jsx)(s.A,{chart:"\ngraph TD;\n  A[Robot RL Agent] > B[Observation Space];\n  A > C[Action Space];\n  A > D[Reward Function];\n  \n  B > E[Camera Images];\n  B > F[Joint States];\n  B > G[IMU Readings];\n  B > H[Force Sensors];\n  \n  C > I[Joint Efforts];\n  C > J[Motor Commands];\n  C > K[Endeffector Poses];\n  \n  D > L[Task Completion];\n  D > M[Efficiency];\n  D > N[Safety];\n  D > O[Stability];\n  \n  P[Environment] > Q[Physics Simulation];\n  P > R[Real World];\n  P > S[Domain Randomization];\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style P fill:#2196F3,stroke:#0D47A1,color:#fff;\n"}),"\n",(0,t.jsx)(e.h3,{id:"types-of-rl-algorithms-for-robotics",children:"Types of RL Algorithms for Robotics"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Deep Deterministic Policy Gradient (DDPG)"}),": Actorcritic method for continuous action spaces."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Soft ActorCritic (SAC)"}),": Maximum entropy RL algorithm that balances exploration and exploitation."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Proximal Policy Optimization (PPO)"}),": Policy gradient method that clips gradients to prevent large policy updates."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Twin Delayed DDPG (TD3)"}),": Improved version of DDPG that addresses overestimation bias."]}),"\n",(0,t.jsx)(e.h3,{id:"simtoreal-transfer-challenges",children:"SimtoReal Transfer Challenges"}),"\n",(0,t.jsx)(e.p,{children:'The "reality gap" refers to differences between simulation and the real world that can prevent policies trained in simulation from working on real robots:'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Dynamics Mismatch"}),": Differences in friction, motor delays, actuator responses\r\n",(0,t.jsx)(e.strong,{children:"Sensor Noise"}),": Real sensors have different noise characteristics\r\n",(0,t.jsx)(e.strong,{children:"Model Imperfections"}),": Uncertainty in robot and environment models\r\n",(0,t.jsx)(e.strong,{children:"Environmental Factors"}),": Lighting, texture, and physical properties"]}),"\n",(0,t.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,t.jsx)(e.p,{children:"Techniques to make policies robust to simulation imperfections:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Dynamics Randomization"}),": Varying physical parameters randomly\r\n",(0,t.jsx)(e.strong,{children:"Visual Domain Randomization"}),": Changing textures, colors, lighting\r\n",(0,t.jsx)(e.strong,{children:"Control Randomization"}),": Adding delays, noise to control signals"]}),"\n",(0,t.jsx)(e.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,t.jsx)(e.h3,{id:"lab-1-setting-up-isaac-gym-for-rl-training",children:"Lab 1: Setting up Isaac Gym for RL Training"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Install Isaac Gym Preview 4"})," (the last preview version, or Isaac Orbit for newer implementations):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Isaac Gym is part of the Isaac Extensions\r\n# For this example, we'll assume Isaac Orbit or similar environment\r\n\r\n# Create virtual environment\r\npython m venv ~/isaac_rl_env\r\nsource ~/isaac_rl_env/bin/activate\r\npip install torch torchvision\r\npip install gymnasium\r\npip install stablebaselines3[extra]\r\npip install sb3contrib\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Create a basic RL environment using Isaac Sim as physics backend"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# robot_rl_env.py\r\nimport gymnasium as gym\r\nfrom gymnasium import spaces\r\nimport numpy as np\r\nimport torch\r\nimport omni\r\nfrom pxr import Gf, UsdGeom\r\nimport carb\r\n\r\nclass RobotRLEnv(gym.Env):\r\n    """Custom Robot RL Environment that wraps Isaac Sim for training"""\r\n    \r\n    def __init__(self, headless=True):\r\n        super(RobotRLEnv, self).__init__()\r\n        \r\n        # Define action and observation space\r\n        # Continuous action space for joint torques\r\n        self.action_space = spaces.Box(\r\n            low=1.0, high=1.0, shape=(6,), dtype=np.float32  # 6 joints\r\n        )\r\n        \r\n        # Observation space: joint positions, velocities, and IMU readings\r\n        self.observation_space = spaces.Box(\r\n            low=np.inf, high=np.inf, shape=(18,), dtype=np.float32  # 6 pos + 6 vel + 6 IMU\r\n        )\r\n        \r\n        # Environment parameters\r\n        self.target_pos = np.array([1.0, 0.0, 0.0])  # Target position\r\n        self.max_episode_steps = 1000\r\n        self.current_step = 0\r\n        \r\n        # Initialize Isaac Sim components\r\n        self.headless = headless\r\n        self.reset()\r\n        \r\n    def reset(self, seed=None, options=None):\r\n        """Reset the environment to an initial state"""\r\n        super().reset(seed=seed)\r\n        \r\n        # Reset robot position in Isaac Sim\r\n        # In practice, this would involve resetting the Isaac Sim scene\r\n        self.robot_pos = np.array([0.0, 0.0, 0.0])\r\n        self.robot_vel = np.array([0.0, 0.0, 0.0])\r\n        self.joint_positions = np.zeros(6)\r\n        self.joint_velocities = np.zeros(6)\r\n        self.imu_readings = np.zeros(6)  # [acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z]\r\n        \r\n        self.current_step = 0\r\n        \r\n        # Return initial observation\r\n        obs = self._get_observation()\r\n        info = {}  # Additional info dictionary\r\n        \r\n        return obs, info\r\n    \r\n    def step(self, action):\r\n        """Execute one time step within the environment"""\r\n        # Apply action to robot in simulation\r\n        self._apply_action(action)\r\n        \r\n        # Step simulation\r\n        self._step_simulation()\r\n        \r\n        # Get new state\r\n        obs = self._get_observation()\r\n        \r\n        # Calculate reward\r\n        reward = self._calculate_reward()\r\n        \r\n        # Check termination conditions\r\n        terminated = self._check_termination()\r\n        truncated = self.current_step >= self.max_episode_steps\r\n        \r\n        # Increment step counter\r\n        self.current_step += 1\r\n        \r\n        # Additional info\r\n        info = {\r\n            \'distance_to_target\': np.linalg.norm(self.robot_pos  self.target_pos)\r\n        }\r\n        \r\n        return obs, reward, terminated, truncated, info\r\n    \r\n    def _apply_action(self, action):\r\n        """Apply the given action to the robot"""\r\n        # In a real implementation, this would send commands to joints\r\n        # For this example, we\'ll simulate simple dynamics\r\n        torque_scale = 50.0  # Scale factor for torques\r\n        scaled_action = action * torque_scale\r\n        \r\n        # Update joint positions (simplified Euler integration)\r\n        dt = 0.01  # Time step\r\n        self.joint_velocities += scaled_action * dt * 0.1  # Acceleration\r\n        self.joint_positions += self.joint_velocities * dt  # Velocity integration\r\n        \r\n        # Update robot position based on joint movements (very simplified)\r\n        self.robot_pos += np.array([action[0] * dt * 0.1, action[1] * dt * 0.1, 0.0])\r\n    \r\n    def _step_simulation(self):\r\n        """Step the simulation forward"""\r\n        # In Isaac Sim, this would trigger one simulation step\r\n        # For this example, we\'ll just update the IMU readings\r\n        self.imu_readings = np.random.normal(0.0, 0.1, size=6)  # Add noise\r\n        \r\n        # In a real implementation, this would step physics in Isaac Sim\r\n    \r\n    def _get_observation(self):\r\n        """Get current observation from the environment"""\r\n        return np.concatenate([\r\n            self.joint_positions,\r\n            self.joint_velocities,\r\n            self.imu_readings\r\n        ])\r\n    \r\n    def _calculate_reward(self):\r\n        """Calculate reward for current state"""\r\n        # Distance to target\r\n        dist_to_target = np.linalg.norm(self.robot_pos  self.target_pos)\r\n        \r\n        # Reward based on getting closer to target\r\n        reward = dist_to_target  # Negative distance (closer = higher reward)\r\n        \r\n        # Add bonus for reaching target\r\n        if dist_to_target < 0.1:\r\n            reward += 100  # Large bonus for reaching target\r\n        \r\n        # Penalty for taking too much action (energy efficiency)\r\n        action_penalty = 0.01 * np.sum(np.abs(self.joint_velocities))\r\n        reward += action_penalty\r\n        \r\n        return reward\r\n    \r\n    def _check_termination(self):\r\n        """Check if episode should terminate"""\r\n        # Terminate if robot reaches target\r\n        dist_to_target = np.linalg.norm(self.robot_pos  self.target_pos)\r\n        if dist_to_target < 0.1:\r\n            return True\r\n        \r\n        # Don\'t terminate normally  let truncated handle max steps\r\n        return False\r\n    \r\n    def close(self):\r\n        """Clean up resources"""\r\n        # Close Isaac Sim connections in real implementation\r\n        pass\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-2-implementing-sac-algorithm-for-robot-control",children:"Lab 2: Implementing SAC Algorithm for Robot Control"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create a Soft ActorCritic implementation"})," (",(0,t.jsx)(e.code,{children:"sac_robot_controller.py"}),"):","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch.distributions import Normal\r\nimport random\r\nimport copy\r\nfrom collections import namedtuple, deque\r\nimport gymnasium as gym\r\nfrom robot_rl_env import RobotRLEnv\r\nimport matplotlib.pyplot as plt\r\n\r\n# Experience replay buffer\r\nTransition = namedtuple('Transition', \r\n                       ('state', 'action', 'next_state', 'reward', 'done'))\r\n\r\nclass ReplayBuffer:\r\n    def __init__(self, capacity):\r\n        self.buffer = deque(maxlen=capacity)\r\n    \r\n    def push(self, *args):\r\n        self.buffer.append(Transition(*args))\r\n    \r\n    def sample(self, batch_size):\r\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\r\n        samples = [self.buffer[idx] for idx in indices]\r\n        return Transition(*zip(*samples))\r\n    \r\n    def __len__(self):\r\n        return len(self.buffer)\r\n\r\n# Neural Network architectures\r\nclass ValueNetwork(nn.Module):\r\n    def __init__(self, state_dim, hidden_dim=256):\r\n        super(ValueNetwork, self).__init__()\r\n        \r\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\r\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.fc3 = nn.Linear(hidden_dim, 1)\r\n        \r\n        # Initialize weights\r\n        nn.init.xavier_uniform_(self.fc1.weight)\r\n        nn.init.xavier_uniform_(self.fc2.weight)\r\n        nn.init.xavier_uniform_(self.fc3.weight)\r\n    \r\n    def forward(self, state):\r\n        x = F.relu(self.fc1(state))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\n        return x\r\n\r\nclass QNetwork(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(QNetwork, self).__init__()\r\n        \r\n        # Q1\r\n        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\r\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.fc3 = nn.Linear(hidden_dim, 1)\r\n        \r\n        # Q2\r\n        self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)\r\n        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.fc6 = nn.Linear(hidden_dim, 1)\r\n        \r\n        # Initialize weights\r\n        for layer in [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6]:\r\n            nn.init.xavier_uniform_(layer.weight)\r\n    \r\n    def forward(self, state, action):\r\n        sa = torch.cat([state, action], 1)\r\n        \r\n        q1 = F.relu(self.fc1(sa))\r\n        q1 = F.relu(self.fc2(q1))\r\n        q1 = self.fc3(q1)\r\n        \r\n        q2 = F.relu(self.fc4(sa))\r\n        q2 = F.relu(self.fc5(q2))\r\n        q2 = self.fc6(q2)\r\n        \r\n        return q1, q2\r\n\r\nclass GaussianPolicy(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=20, log_std_max=2):\r\n        super(GaussianPolicy, self).__init__()\r\n        \r\n        self.log_std_min = log_std_min\r\n        self.log_std_max = log_std_max\r\n        \r\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\r\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\r\n        \r\n        self.fc_mean = nn.Linear(hidden_dim, action_dim)\r\n        self.fc_log_std = nn.Linear(hidden_dim, action_dim)\r\n        \r\n        # Initialize weights\r\n        nn.init.xavier_uniform_(self.fc1.weight)\r\n        nn.init.xavier_uniform_(self.fc2.weight)\r\n        nn.init.xavier_uniform_(self.fc_mean.weight)\r\n        nn.init.xavier_uniform_(self.fc_log_std.weight)\r\n    \r\n    def forward(self, state):\r\n        x = F.relu(self.fc1(state))\r\n        x = F.relu(self.fc2(x))\r\n        \r\n        mean = self.fc_mean(x)\r\n        log_std = self.fc_log_std(x)\r\n        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\r\n        \r\n        return mean, log_std\r\n    \r\n    def sample(self, state):\r\n        mean, log_std = self.forward(state)\r\n        std = log_std.exp()\r\n        \r\n        normal = Normal(mean, std)\r\n        x_t = normal.rsample()  # Reparameterization trick\r\n        action = torch.tanh(x_t)  # Squash to [1, 1]\r\n        log_prob = normal.log_prob(x_t)\r\n        \r\n        # Compute log probability of squashed Gaussian (corrected for tanh)\r\n        log_prob = torch.log(1  action.pow(2) + 1e6)\r\n        log_prob = log_prob.sum(1, keepdim=True)\r\n        \r\n        return action, log_prob\r\n\r\n# Soft ActorCritic Agent\r\nclass SACAgent:\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256, lr=3e4, gamma=0.99, tau=5e3,\r\n                 alpha=0.2, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\r\n        \r\n        self.device = device\r\n        \r\n        # Networks\r\n        self.actor = GaussianPolicy(state_dim, action_dim, hidden_dim).to(device)\r\n        self.critic = QNetwork(state_dim, action_dim, hidden_dim).to(device)\r\n        self.critic_target = QNetwork(state_dim, action_dim, hidden_dim).to(device)\r\n        self.value = ValueNetwork(state_dim, hidden_dim).to(device)\r\n        self.value_target = ValueNetwork(state_dim, hidden_dim).to(device)\r\n        \r\n        # Copy critic to target networks\r\n        self.critic_target.load_state_dict(self.critic.state_dict())\r\n        self.value_target.load_state_dict(self.value.state_dict())\r\n        \r\n        # Optimizers\r\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\r\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\r\n        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\r\n        \r\n        # Hyperparameters\r\n        self.gamma = gamma\r\n        self.tau = tau\r\n        self.alpha = alpha\r\n        self.target_entropy = torch.prod(torch.Tensor(action_dim).to(device)).item()\r\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\r\n        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\r\n    \r\n    def select_action(self, state, evaluate=False):\r\n        \"\"\"Select action using the policy\"\"\"\r\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\r\n        \r\n        if evaluate:\r\n            # For evaluation, use mean action (deterministic)\r\n            mean, _ = self.actor(state)\r\n            action = torch.tanh(mean)\r\n        else:\r\n            # For training, sample from distribution\r\n            action, _ = self.actor.sample(state)\r\n        \r\n        return action.cpu().data.numpy().flatten()\r\n    \r\n    def update_parameters(self, memory, batch_size):\r\n        \"\"\"Update the network parameters\"\"\"\r\n        # Sample a batch from memory\r\n        transitions = memory.sample(batch_size)\r\n        batch = Transition(*zip(*transitions))\r\n        \r\n        # Convert to tensors\r\n        state_batch = torch.FloatTensor(np.vstack(batch.state)).to(self.device)\r\n        action_batch = torch.FloatTensor(np.vstack(batch.action)).to(self.device)\r\n        next_state_batch = torch.FloatTensor(np.vstack(batch.next_state)).to(self.device)\r\n        reward_batch = torch.FloatTensor(np.vstack(batch.reward)).to(self.device)\r\n        done_mask = torch.BoolTensor(np.vstack(batch.done)).to(self.device)\r\n        \r\n        # Critic update\r\n        with torch.no_grad():\r\n            next_action, next_log_prob = self.actor.sample(next_state_batch)\r\n            next_q_values = self.critic_target(next_state_batch, next_action)\r\n            next_q_value = torch.min(*next_q_values)  self.alpha * next_log_prob\r\n            expected_q_value = reward_batch + (self.gamma * next_q_value * ~done_mask)\r\n        \r\n        # Get current Q values\r\n        current_q_values = self.critic(state_batch, action_batch)\r\n        critic_loss = F.mse_loss(current_q_values[0], expected_q_value) + \\\r\n                      F.mse_loss(current_q_values[1], expected_q_value)\r\n        \r\n        # Optimize Critic\r\n        self.critic_optimizer.zero_grad()\r\n        critic_loss.backward()\r\n        self.critic_optimizer.step()\r\n        \r\n        # Actor update\r\n        predicted_action, predicted_log_prob = self.actor.sample(state_batch)\r\n        predicted_q_value = self.critic(state_batch, predicted_action)\r\n        predicted_q_value = torch.min(*predicted_q_value)\r\n        actor_loss = (self.alpha * predicted_log_prob  predicted_q_value).mean()\r\n        \r\n        # Optimize Actor\r\n        self.actor_optimizer.zero_grad()\r\n        actor_loss.backward()\r\n        self.actor_optimizer.step()\r\n        \r\n        # Temperature parameter update\r\n        alpha_loss = (self.log_alpha * (predicted_log_prob + self.target_entropy).detach()).mean()\r\n        self.alpha_optimizer.zero_grad()\r\n        alpha_loss.backward()\r\n        self.alpha_optimizer.step()\r\n        self.alpha = self.log_alpha.exp()\r\n        \r\n        # Soft update target networks\r\n        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\r\n            target_param.data.copy_(target_param.data * (1.0  self.tau) + param.data * self.tau)\r\n        \r\n        return critic_loss.item(), actor_loss.item()\r\n    \r\n    def save_checkpoint(self, filepath):\r\n        \"\"\"Save the model checkpoint\"\"\"\r\n        checkpoint = {\r\n            'actor_state_dict': self.actor.state_dict(),\r\n            'critic_state_dict': self.critic.state_dict(),\r\n            'value_state_dict': self.value.state_dict(),\r\n            'critic_target_state_dict': self.critic_target.state_dict(),\r\n            'value_target_state_dict': self.value_target.state_dict(),\r\n            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\r\n            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\r\n            'value_optimizer_state_dict': self.value_optimizer.state_dict(),\r\n            'alpha': self.alpha,\r\n            'log_alpha': self.log_alpha\r\n        }\r\n        torch.save(checkpoint, filepath)\r\n    \r\n    def load_checkpoint(self, filepath):\r\n        \"\"\"Load the model checkpoint\"\"\"\r\n        checkpoint = torch.load(filepath)\r\n        self.actor.load_state_dict(checkpoint['actor_state_dict'])\r\n        self.critic.load_state_dict(checkpoint['critic_state_dict'])\r\n        self.value.load_state_dict(checkpoint['value_state_dict'])\r\n        self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\r\n        self.value_target.load_state_dict(checkpoint['value_target_state_dict'])\r\n        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\r\n        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\r\n        self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])\r\n        self.alpha = checkpoint['alpha']\r\n        self.log_alpha = checkpoint['log_alpha']\r\n\r\ndef train_sac_agent(env, episodes=1000, max_steps=1000):\r\n    \"\"\"Train the SAC agent\"\"\"\r\n    # Get state and action dimensions\r\n    state_dim = env.observation_space.shape[0]\r\n    action_dim = env.action_space.shape[0]\r\n    \r\n    # Initialize agent\r\n    agent = SACAgent(state_dim, action_dim)\r\n    \r\n    # Initialize replay buffer\r\n    replay_buffer = ReplayBuffer(capacity=100000)\r\n    \r\n    # Training parameters\r\n    batch_size = 256\r\n    update_every = 1\r\n    scores = []\r\n    avg_scores = []\r\n    \r\n    for episode in range(episodes):\r\n        state, _ = env.reset()\r\n        total_reward = 0\r\n        \r\n        for step in range(max_steps):\r\n            # Select action\r\n            action = agent.select_action(state)\r\n            \r\n            # Take action in environment\r\n            next_state, reward, terminated, truncated, info = env.step(action)\r\n            done = terminated or truncated\r\n            \r\n            # Store transition in replay buffer\r\n            replay_buffer.push(state, action, next_state, reward, done)\r\n            \r\n            # Update state\r\n            state = next_state\r\n            total_reward += reward\r\n            \r\n            # Update network parameters if enough samples available\r\n            if len(replay_buffer) > batch_size and step % update_every == 0:\r\n                agent.update_parameters(replay_buffer, batch_size)\r\n            \r\n            if done:\r\n                break\r\n        \r\n        scores.append(total_reward)\r\n        \r\n        # Calculate average score over last 100 episodes\r\n        if len(scores) >= 100:\r\n            avg_score = np.mean(scores[100:])\r\n            avg_scores.append(avg_score)\r\n        else:\r\n            avg_scores.append(np.mean(scores))\r\n        \r\n        # Log progress\r\n        if episode % 10 == 0:\r\n            print(f'Episode {episode}, Average Score: {avg_scores[1]:.2f}')\r\n        \r\n        # Stop if solved\r\n        if avg_scores[1] >= 90 and len(avg_scores) > 10:  # Adjustable threshold\r\n            print(f'Solved in {episode} episodes!')\r\n            break\r\n    \r\n    # Plot training progress\r\n    plt.figure(figsize=(12, 5))\r\n    \r\n    plt.subplot(1, 2, 1)\r\n    plt.plot(scores)\r\n    plt.title('Training Scores')\r\n    plt.xlabel('Episode')\r\n    plt.ylabel('Score')\r\n    \r\n    plt.subplot(1, 2, 2)\r\n    plt.plot(avg_scores)\r\n    plt.title('Average Scores (100 Episode Window)')\r\n    plt.xlabel('Episode')\r\n    plt.ylabel('Average Score')\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig('sac_training_progress.png')\r\n    plt.show()\r\n    \r\n    return agent, scores, avg_scores\r\n\r\ndef main():\r\n    \"\"\"Main training function\"\"\"\r\n    # Create environment\r\n    env = RobotRLEnv(headless=True)\r\n    \r\n    print(\"Starting SAC training...\")\r\n    print(f\"State dimension: {env.observation_space.shape[0]}\")\r\n    print(f\"Action dimension: {env.action_space.shape[0]}\")\r\n    \r\n    # Train the agent\r\n    agent, scores, avg_scores = train_sac_agent(env, episodes=500, max_steps=500)\r\n    \r\n    # Save trained agent\r\n    agent.save_checkpoint('sac_robot_agent.pth')\r\n    print(\"Training complete. Model saved.\")\r\n    \r\n    # Close environment\r\n    env.close()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-3-domain-randomization-for-simtoreal-transfer",children:"Lab 3: Domain Randomization for SimtoReal Transfer"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create a domain randomization wrapper"})," (",(0,t.jsx)(e.code,{children:"domain_randomization_wrapper.py"}),"):","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport gymnasium as gym\r\nfrom gymnasium import spaces\r\nimport numpy as np\r\nimport random\r\n\r\nclass DomainRandomizationWrapper(gym.Wrapper):\r\n    """Domain randomization wrapper for RL environment"""\r\n    \r\n    def __init__(self, env, randomization_params=None):\r\n        super().__init__(env)\r\n        \r\n        if randomization_params is None:\r\n            # Default randomization parameters\r\n            self.randomization_params = {\r\n                \'friction_range\': (0.1, 1.0),           # Range for friction coefficient\r\n                \'mass_range\': (0.8, 1.2),              # Range for link masses\r\n                \'com_range\': (0.02, 0.02),            # Range for center of mass offsets\r\n                \'motor_delay_range\': (0.0, 0.02),       # Motor delay in seconds\r\n                \'sensor_noise_std\': 0.01,              # Standard deviation for sensor noise\r\n                \'control_delay_range\': (0.0, 0.01),     # Control delay in seconds\r\n                \'gravity_range\': (1, 1),              # Range for gravity variation (m/s\xb2)\r\n                \'visual_randomization\': True            # Enable visual domain randomization\r\n            }\r\n        else:\r\n            self.randomization_params = randomization_params\r\n        \r\n        # Store initial parameters\r\n        self.initial_env_params = {}\r\n        \r\n        # Initialize randomized parameters\r\n        self.current_randomization = {}\r\n        \r\n        # Apply initial randomization\r\n        self.randomize_environment()\r\n    \r\n    def reset(self, seed=None, options=None):\r\n        """Reset with randomization"""\r\n        # Apply new randomization\r\n        self.randomize_environment()\r\n        \r\n        # Reset the wrapped environment\r\n        return self.env.reset(seed=seed, options=options)\r\n    \r\n    def step(self, action):\r\n        """Step with potential control delay and noise"""\r\n        # Add random control delay\r\n        if random.random() < 0.1:  # 10% chance of delayed action\r\n            delay = random.uniform(\r\n                self.randomization_params[\'control_delay_range\'][0],\r\n                self.randomization_params[\'control_delay_range\'][1]\r\n            )\r\n            # In a real implementation, we\'d handle delayed action application\r\n            pass\r\n        \r\n        # Apply action (possibly with modification for randomization)\r\n        obs, reward, terminated, truncated, info = self.env.step(action)\r\n        \r\n        # Add sensor noise\r\n        obs = self.add_sensor_noise(obs)\r\n        \r\n        # Modify reward based on randomization (to simulate reality gap)\r\n        reward = self.modify_reward_for_realism(reward)\r\n        \r\n        return obs, reward, terminated, truncated, info\r\n    \r\n    def randomize_environment(self):\r\n        """Randomize environment parameters"""\r\n        # Randomize friction\r\n        friction_coeff = random.uniform(\r\n            self.randomization_params[\'friction_range\'][0],\r\n            self.randomization_params[\'friction_range\'][1]\r\n        )\r\n        self.current_randomization[\'friction\'] = friction_coeff\r\n        \r\n        # Randomize masses\r\n        mass_multipliers = {}\r\n        for i in range(6):  # Assuming 6 links\r\n            multiplier = random.uniform(\r\n                self.randomization_params[\'mass_range\'][0],\r\n                self.randomization_params[\'mass_range\'][1]\r\n            )\r\n            mass_multipliers[f\'link_{i}_mass\'] = multiplier\r\n        self.current_randomization[\'mass_multipliers\'] = mass_multipliers\r\n        \r\n        # Randomize center of mass\r\n        com_offsets = {}\r\n        for i in range(6):\r\n            offset = random.uniform(\r\n                self.randomization_params[\'com_range\'][0],\r\n                self.randomization_params[\'com_range\'][1]\r\n            )\r\n            com_offsets[f\'link_{i}_com_offset\'] = offset\r\n        self.current_randomization[\'com_offsets\'] = com_offsets\r\n        \r\n        # Randomize gravity\r\n        gravity_variation = random.uniform(\r\n            self.randomization_params[\'gravity_range\'][0],\r\n            self.randomization_params[\'gravity_range\'][1]\r\n        )\r\n        self.current_randomization[\'gravity_variation\'] = gravity_variation\r\n        \r\n        # Apply randomization to Isaac Sim or simulation parameters\r\n        self.apply_randomization_to_simulation()\r\n    \r\n    def add_sensor_noise(self, obs):\r\n        """Add noise to observations"""\r\n        noise_std = self.randomization_params[\'sensor_noise_std\']\r\n        noise = np.random.normal(0, noise_std, size=obs.shape)\r\n        return obs + noise\r\n    \r\n    def modify_reward_for_realism(self, reward):\r\n        """Modify reward to account for simtoreal considerations"""\r\n        # In simtoreal transfer, we might want to penalize behaviors that don\'t\r\n        # translate well to the real world (e.g., overly aggressive motions)\r\n        return reward\r\n    \r\n    def apply_randomization_to_simulation(self):\r\n        """Apply randomization to the underlying simulation environment"""\r\n        # In a real implementation, this would interface with Isaac Sim to modify:\r\n        #  Friction parameters\r\n        #  Link masses and inertias\r\n        #  Center of mass offsets\r\n        #  Joint damping\r\n        #  Actuator properties\r\n        # etc.\r\n        pass\r\n    \r\n    def get_current_randomization(self):\r\n        """Get current randomization parameters"""\r\n        return self.current_randomization\r\n    \r\n    def sample_randomizations(self, num_samples=100):\r\n        """Sample multiple randomizations for training diversity"""\r\n        samples = []\r\n        for _ in range(num_samples):\r\n            # Temporarily store current randomization\r\n            old_randomization = self.current_randomization.copy()\r\n            \r\n            # Apply new randomization\r\n            self.randomize_environment()\r\n            samples.append(self.current_randomization.copy())\r\n            \r\n            # Restore old randomization\r\n            self.current_randomization = old_randomization\r\n        \r\n        return samples\r\n\r\n# Enhanced robot environment with domain randomization\r\nclass RandomizedRobotRLEnv(DomainRandomizationWrapper):\r\n    """Robot environment with domain randomization for simtoreal transfer"""\r\n    \r\n    def __init__(self, headless=True, randomization_params=None):\r\n        # Create base environment\r\n        base_env = RobotRLEnv(headless=headless)\r\n        \r\n        # Wrap with domain randomization\r\n        super().__init__(base_env, randomization_params)\r\n        \r\n        # Store original observation space and modify if needed\r\n        self.original_observation_space = base_env.observation_space\r\n    \r\n    def get_randomization_info(self):\r\n        """Get information about current randomization parameters"""\r\n        return {\r\n            \'randomization_active\': True,\r\n            \'current_params\': self.get_current_randomization(),\r\n            \'param_ranges\': self.randomization_params\r\n        }\r\n\r\n# Example of how to train with domain randomization\r\ndef train_with_domain_randomization():\r\n    """Train an agent with domain randomization enabled"""\r\n    # Define randomization parameters\r\n    randomization_params = {\r\n        \'friction_range\': (0.1, 1.5),\r\n        \'mass_range\': (0.7, 1.3),\r\n        \'com_range\': (0.03, 0.03),\r\n        \'motor_delay_range\': (0.0, 0.03),\r\n        \'sensor_noise_std\': 0.015,\r\n        \'control_delay_range\': (0.0, 0.015),\r\n        \'gravity_range\': (1.5, 1.5),\r\n        \'visual_randomization\': True\r\n    }\r\n    \r\n    # Create randomized environment\r\n    env = RandomizedRobotRLEnv(headless=True, randomization_params=randomization_params)\r\n    \r\n    print("Environment with domain randomization created.")\r\n    print(f"Randomization info: {env.get_randomization_info()}")\r\n    \r\n    # Proceed with training  the same SAC training code can now work with\r\n    # the randomized environment\r\n    \r\n    # Example: Get randomization info during training\r\n    obs, info = env.reset()\r\n    print(f"Environment parameters randomized: {env.get_current_randomization()}")\r\n    \r\n    # Take a few steps to see how randomization affects the environment\r\n    for i in range(5):\r\n        action = env.action_space.sample()\r\n        obs, reward, terminated, truncated, info = env.step(action)\r\n        print(f"Step {i}: Obs shape={obs.shape}, Reward={reward:.2f}")\r\n        \r\n        if terminated or truncated:\r\n            env.reset()\r\n    \r\n    env.close()\r\n    print("Domain randomization training environment test complete.")\r\n\r\nif __name__ == "__main__":\r\n    train_with_domain_randomization()\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-4-evaluating-simtoreal-transfer",children:"Lab 4: Evaluating SimtoReal Transfer"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create a transfer evaluation system"})," (",(0,t.jsx)(e.code,{children:"transfer_evaluation.py"}),"):","\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.spatial.distance import pdist, squareform\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.manifold import TSNE\r\nimport seaborn as sns\r\nfrom collections import defaultdict\r\nimport pandas as pd\r\n\r\nclass TransferEvaluator:\r\n    \"\"\"Evaluate simtoreal transfer effectiveness\"\"\"\r\n    \r\n    def __init__(self):\r\n        self.simulation_episodes = []\r\n        self.real_world_episodes = []\r\n        self.transfer_metrics = {}\r\n    \r\n    def collect_simulation_data(self, agent, sim_env, num_episodes=100):\r\n        \"\"\"Collect data from simulation environment\"\"\"\r\n        print(f\"Collecting {num_episodes} episodes from simulation...\")\r\n        \r\n        for ep in range(num_episodes):\r\n            obs, _ = sim_env.reset()\r\n            episode_data = {\r\n                'states': [],\r\n                'actions': [],\r\n                'rewards': [],\r\n                'next_states': [],\r\n                'dones': []\r\n            }\r\n            \r\n            step_count = 0\r\n            while True:\r\n                # Use trained agent to select action\r\n                action = agent.select_action(obs)\r\n                next_obs, reward, terminated, truncated, info = sim_env.step(action)\r\n                done = terminated or truncated\r\n                \r\n                # Store data\r\n                episode_data['states'].append(obs.copy())\r\n                episode_data['actions'].append(action.copy())\r\n                episode_data['rewards'].append(reward)\r\n                episode_data['next_states'].append(next_obs.copy())\r\n                episode_data['dones'].append(done)\r\n                \r\n                obs = next_obs\r\n                step_count += 1\r\n                \r\n                if done or step_count > 500:  # Max steps\r\n                    break\r\n            \r\n            self.simulation_episodes.append(episode_data)\r\n            \r\n            if ep % 20 == 0:\r\n                print(f\"  Collected {ep+1}/{num_episodes} simulation episodes\")\r\n        \r\n        print(f\"Simulation data collected for {len(self.simulation_episodes)} episodes\")\r\n    \r\n    def collect_real_world_data(self, real_env, num_episodes=20):\r\n        \"\"\"Collect data from real robot (or a more realistic simulation)\"\"\"\r\n        print(f\"Collecting {num_episodes} episodes from real world...\")\r\n        \r\n        for ep in range(num_episodes):\r\n            obs, _ = real_env.reset()\r\n            episode_data = {\r\n                'states': [],\r\n                'actions': [],\r\n                'rewards': [],\r\n                'next_states': [],\r\n                'dones': []\r\n            }\r\n            \r\n            step_count = 0\r\n            while True:\r\n                # For real evaluation, you might use a different control strategy\r\n                # or human demonstration  this is a placeholder\r\n                action = real_env.action_space.sample()  # Random action in this example\r\n                next_obs, reward, terminated, truncated, info = real_env.step(action)\r\n                done = terminated or truncated\r\n                \r\n                # Store data\r\n                episode_data['states'].append(obs.copy())\r\n                episode_data['actions'].append(action.copy())\r\n                episode_data['rewards'].append(reward)\r\n                episode_data['next_states'].append(next_obs.copy())\r\n                episode_data['dones'].append(done)\r\n                \r\n                obs = next_obs\r\n                step_count += 1\r\n                \r\n                if done or step_count > 500:  # Max steps\r\n                    break\r\n            \r\n            self.real_world_episodes.append(episode_data)\r\n            \r\n            if ep % 5 == 0:\r\n                print(f\"  Collected {ep+1}/{num_episodes} real world episodes\")\r\n        \r\n        print(f\"Real world data collected for {len(self.real_world_episodes)} episodes\")\r\n    \r\n    def compute_transfer_score(self, agent, source_env, target_env, num_eval_episodes=10):\r\n        \"\"\"Compute transfer score by evaluating agent on target environment\"\"\"\r\n        print(\"Computing transfer score...\")\r\n        \r\n        total_rewards = []\r\n        success_count = 0\r\n        total_steps = 0\r\n        \r\n        for episode in range(num_eval_episodes):\r\n            obs, _ = target_env.reset()\r\n            episode_reward = 0\r\n            steps = 0\r\n            \r\n            while True:\r\n                action = agent.select_action(obs, evaluate=True)\r\n                obs, reward, terminated, truncated, info = target_env.step(action)\r\n                done = terminated or truncated\r\n                \r\n                episode_reward += reward\r\n                steps += 1\r\n                total_steps += 1\r\n                \r\n                # Check for success condition (customizable)\r\n                if hasattr(target_env, 'is_successful') and target_env.is_successful():\r\n                    success_count += 1\r\n                \r\n                if done or steps > 1000:  # Max steps\r\n                    break\r\n            \r\n            total_rewards.append(episode_reward)\r\n        \r\n        avg_reward = np.mean(total_rewards)\r\n        success_rate = success_count / num_eval_episodes\r\n        avg_length = total_steps / num_eval_episodes\r\n        \r\n        self.transfer_metrics = {\r\n            'avg_reward': avg_reward,\r\n            'success_rate': success_rate,\r\n            'avg_episode_length': avg_length,\r\n            'eval_episodes': num_eval_episodes\r\n        }\r\n        \r\n        print(f\"Transfer Score Results:\")\r\n        print(f\"  Average Reward: {avg_reward:.2f}\")\r\n        print(f\"  Success Rate: {success_rate:.2f}\")\r\n        print(f\"  Avg Episode Length: {avg_length:.2f}\")\r\n        \r\n        return avg_reward, success_rate\r\n    \r\n    def analyze_dynamics_difference(self):\r\n        \"\"\"Analyze differences in dynamics between sim and real\"\"\"\r\n        if not self.simulation_episodes or not self.real_world_episodes:\r\n            print(\"Need to collect both sim and real data first\")\r\n            return\r\n        \r\n        # Compute state similarities\r\n        sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])\r\n        real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])\r\n        \r\n        # Use statistical tests to compare distributions\r\n        sim_means = np.mean(sim_states, axis=0)\r\n        real_means = np.mean(real_states, axis=0)\r\n        \r\n        sim_stds = np.std(sim_states, axis=0)\r\n        real_stds = np.std(real_states, axis=0)\r\n        \r\n        differences = {\r\n            'mean_diff': np.abs(sim_means  real_means),\r\n            'std_diff': np.abs(sim_stds  real_stds),\r\n            'relative_mean_diff': np.abs(sim_means  real_means) / np.abs(sim_means + 1e8),\r\n            'relative_std_diff': np.abs(sim_stds  real_stds) / np.abs(sim_stds + 1e8)\r\n        }\r\n        \r\n        print(\"\\nDynamics Difference Analysis:\")\r\n        print(f\"  Mean state difference: {np.mean(differences['mean_diff']):.4f}\")\r\n        print(f\"  STD state difference: {np.mean(differences['std_diff']):.4f}\")\r\n        print(f\"  Relative mean difference: {np.mean(differences['relative_mean_diff']):.4f}\")\r\n        print(f\"  Relative STD difference: {np.mean(differences['relative_std_diff']):.4f}\")\r\n        \r\n        return differences\r\n    \r\n    def visualize_state_space_alignment(self):\r\n        \"\"\"Visualize state space alignment between sim and real\"\"\"\r\n        if not self.simulation_episodes or not self.real_world_episodes:\r\n            print(\"Need to collect both sim and real data first\")\r\n            return\r\n        \r\n        # Sample states from both environments\r\n        sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])\r\n        real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])\r\n        \r\n        # Sample equal number of states\r\n        min_len = min(len(sim_states), len(real_states))\r\n        sim_sample = sim_states[np.random.choice(len(sim_states), min_len, replace=False)]\r\n        real_sample = real_states[np.random.choice(len(real_states), min_len, replace=False)]\r\n        \r\n        # Combine samples for visualization\r\n        all_states = np.vstack([sim_sample, real_sample])\r\n        labels = ['Simulation'] * min_len + ['Real'] * min_len\r\n        \r\n        # Apply dimensionality reduction (PCA) for visualization\r\n        pca = PCA(n_components=2)\r\n        pca_result = pca.fit_transform(all_states)\r\n        \r\n        plt.figure(figsize=(12, 5))\r\n        \r\n        # PCA visualization\r\n        plt.subplot(1, 2, 1)\r\n        sim_mask = np.array(labels) == 'Simulation'\r\n        real_mask = np.array(labels) == 'Real'\r\n        \r\n        plt.scatter(pca_result[sim_mask, 0], pca_result[sim_mask, 1], \r\n                   alpha=0.6, label='Simulation', c='blue')\r\n        plt.scatter(pca_result[real_mask, 0], pca_result[real_mask, 1], \r\n                   alpha=0.6, label='Real', c='red')\r\n        plt.title('State Space Alignment (PCA)')\r\n        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\r\n        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\r\n        plt.legend()\r\n        \r\n        # Action comparison (if available)\r\n        if self.simulation_episodes and self.real_world_episodes:\r\n            sim_actions = np.concatenate([ep['actions'] for ep in self.simulation_episodes])\r\n            real_actions = np.concatenate([ep['actions'] for ep in self.real_world_episodes])\r\n            \r\n            plt.subplot(1, 2, 2)\r\n            plt.hist(sim_actions.flatten(), bins=50, alpha=0.5, label='Simulation Actions', density=True)\r\n            plt.hist(real_actions.flatten(), bins=50, alpha=0.5, label='Real Actions', density=True)\r\n            plt.title('Action Distribution Comparison')\r\n            plt.xlabel('Action Values')\r\n            plt.ylabel('Density')\r\n            plt.legend()\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig('transfer_analysis.png')\r\n        plt.show()\r\n    \r\n    def compute_divergence_metrics(self):\r\n        \"\"\"Compute divergence between sim and real distributions\"\"\"\r\n        if not self.simulation_episodes or not self.real_world_episodes:\r\n            print(\"Need to collect both sim and real data first\")\r\n            return\r\n        \r\n        # Sample states from both environments\r\n        sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])\r\n        real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])\r\n        \r\n        # Compute MMD (Maximum Mean Discrepancy) approximation\r\n        # This is a simplified version  in practice, use kernel methods\r\n        sample_size = min(len(sim_states), len(real_states), 1000)\r\n        sim_subsample = sim_states[np.random.choice(len(sim_states), sample_size, replace=False)]\r\n        real_subsample = real_states[np.random.choice(len(real_states), sample_size, replace=False)]\r\n        \r\n        # Euclidean distance between samples\r\n        distances = np.linalg.norm(sim_subsample[:, np.newaxis, :]  real_subsample[np.newaxis, :, :], axis=2)\r\n        mmd_approx = np.mean(distances)  # Simplified MMD approximation\r\n        \r\n        # Store metrics\r\n        self.divergence_metrics = {\r\n            'mmd_approx': mmd_approx,\r\n            'sim_mean': np.mean(sim_states, axis=0),\r\n            'real_mean': np.mean(real_states, axis=0),\r\n            'sim_std': np.std(sim_states, axis=0),\r\n            'real_std': np.std(real_states, axis=0)\r\n        }\r\n        \r\n        print(f\"\\nDivergence Metrics:\")\r\n        print(f\"  MMD Approximation: {mmd_approx:.4f}\")\r\n        print(f\"  Mean state difference: {np.mean(np.abs(self.divergence_metrics['sim_mean']  self.divergence_metrics['real_mean'])):.4f}\")\r\n        \r\n        return self.divergence_metrics\r\n\r\ndef evaluate_transfer(agent, sim_env, real_env, num_eval_episodes=10):\r\n    \"\"\"Full transfer evaluation pipeline\"\"\"\r\n    evaluator = TransferEvaluator()\r\n    \r\n    print(\"=== SimtoReal Transfer Evaluation ===\")\r\n    \r\n    # Optionally collect fresh data (in practice, you'd already have this)\r\n    # evaluator.collect_simulation_data(agent, sim_env)\r\n    # evaluator.collect_real_world_data(real_env)\r\n    \r\n    # Compute transfer score\r\n    avg_reward, success_rate = evaluator.compute_transfer_score(\r\n        agent, sim_env, real_env, num_eval_episodes\r\n    )\r\n    \r\n    # Perform analysis\r\n    evaluator.analyze_dynamics_difference()\r\n    evaluator.visualize_state_space_alignment()\r\n    evaluator.compute_divergence_metrics()\r\n    \r\n    print(f\"\\n=== Summary ===\")\r\n    print(f\"Transfer Success Rate: {success_rate:.2f}\")\r\n    print(f\"Average Reward in Target Domain: {avg_reward:.2f}\")\r\n    \r\n    return evaluator\r\n\r\nif __name__ == \"__main__\":\r\n    # Example usage would require actual trained agent and environments\r\n    print(\"Transfer evaluation module loaded.\")\r\n    print(\"Use evaluate_transfer() with trained agent and environments.\")\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,t.jsx)(e.p,{children:"Here's a complete example that demonstrates the entire RL training and evaluation pipeline:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_rl_transfer_example.py\r\n\r\nimport numpy as np\r\nimport torch\r\nimport gymnasium as gym\r\nfrom robot_rl_env import RobotRLEnv\r\nfrom sac_robot_controller import SACAgent, train_sac_agent\r\nfrom domain_randomization_wrapper import RandomizedRobotRLEnv\r\nfrom transfer_evaluation import evaluate_transfer, TransferEvaluator\r\n\r\ndef create_environments_with_randomization():\r\n    """Create environments with different levels of randomization"""\r\n    \r\n    # Base environment parameters\r\n    base_params = {\r\n        \'friction_range\': (0.1, 1.0),\r\n        \'mass_range\': (0.8, 1.2),\r\n        \'com_range\': (0.02, 0.02),\r\n        \'sensor_noise_std\': 0.01,\r\n        \'gravity_range\': (1, 1)\r\n    }\r\n    \r\n    # High randomization environment (for training)\r\n    high_random_params = {\r\n        \'friction_range\': (0.05, 1.5),\r\n        \'mass_range\': (0.6, 1.4),\r\n        \'com_range\': (0.05, 0.05),\r\n        \'sensor_noise_std\': 0.03,\r\n        \'gravity_range\': (2, 2),\r\n        \'motor_delay_range\': (0.0, 0.03),\r\n        \'control_delay_range\': (0.0, 0.02)\r\n    }\r\n    \r\n    # Low randomization environment (closer to real)\r\n    low_random_params = {\r\n        \'friction_range\': (0.7, 1.1),\r\n        \'mass_range\': (0.9, 1.1),\r\n        \'com_range\': (0.01, 0.01),\r\n        \'sensor_noise_std\': 0.005,\r\n        \'gravity_range\': (0.5, 0.5)\r\n    }\r\n    \r\n    # Create environments\r\n    base_env = RobotRLEnv(headless=True)\r\n    high_random_env = RandomizedRobotRLEnv(headless=True, randomization_params=high_random_params)\r\n    low_random_env = RandomizedRobotRLEnv(headless=True, randomization_params=low_random_params)\r\n    \r\n    return base_env, high_random_env, low_random_env\r\n\r\ndef train_with_domain_randomization():\r\n    """Train agent with domain randomization for better simtoreal transfer"""\r\n    print("=== Training with Domain Randomization ===")\r\n    \r\n    # Create randomized environment\r\n    _, high_random_env, _ = create_environments_with_randomization()\r\n    \r\n    print("Training agent with high domain randomization...")\r\n    \r\n    # Get state and action dimensions\r\n    state_dim = high_random_env.observation_space.shape[0]\r\n    action_dim = high_random_env.action_space.shape[0]\r\n    \r\n    # Initialize agent\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n    agent = SACAgent(state_dim, action_dim, device=device)\r\n    \r\n    # Training parameters\r\n    episodes = 300\r\n    max_steps = 300\r\n    \r\n    # Training loop\r\n    replay_buffer = []  # Simplified placeholder\r\n    batch_size = 256\r\n    scores = []\r\n    \r\n    for episode in range(episodes):\r\n        state, _ = high_random_env.reset()\r\n        total_reward = 0\r\n        \r\n        for step in range(max_steps):\r\n            # Select action with exploration noise\r\n            action = agent.select_action(state, evaluate=False)\r\n            \r\n            # Take action in randomized environment\r\n            next_state, reward, terminated, truncated, info = high_random_env.step(action)\r\n            done = terminated or truncated\r\n            \r\n            # Store transition (simplified  in real impl, add to replay buffer)\r\n            # replay_buffer.push(state, action, next_state, reward, done)\r\n            \r\n            state = next_state\r\n            total_reward += reward\r\n            \r\n            # Update network periodically (simplified)\r\n            if len(replay_buffer) > batch_size and step % 10 == 0:\r\n                # agent.update_parameters(replay_buffer, batch_size)\r\n                pass\r\n            \r\n            if done:\r\n                break\r\n        \r\n        scores.append(total_reward)\r\n        \r\n        # Log progress\r\n        if episode % 20 == 0:\r\n            avg_score = np.mean(scores[20:]) if len(scores) >= 20 else np.mean(scores)\r\n            print(f\'Episode {episode}, Average Score: {avg_score:.2f}\')\r\n            print(f\'  Current randomization: {high_random_env.get_current_randomization()}\')\r\n    \r\n    print(f"Training completed. Final average score: {np.mean(scores[50:]):.2f}")\r\n    \r\n    return agent, high_random_env, scores\r\n\r\ndef run_complete_transfer_pipeline():\r\n    """Run complete pipeline: train, evaluate transfer"""\r\n    print("=== Complete RL SimtoReal Pipeline ===")\r\n    \r\n    # Step 1: Train with domain randomization\r\n    agent, train_env, training_scores = train_with_domain_randomization()\r\n    \r\n    # Step 2: Create different environments for evaluation\r\n    base_env, _, low_random_env = create_environments_with_randomization()\r\n    \r\n    # Step 3: Evaluate transfer performance\r\n    evaluator = evaluate_transfer(agent, train_env, low_random_env, num_eval_episodes=5)\r\n    \r\n    # Step 4: Finetune if needed (transfer learning)\r\n    print("\\n=== Transfer Learning Adjustment ===")\r\n    print("If transfer score is low, consider:")\r\n    print(" Increasing domain randomization range")\r\n    print(" Adding more realistic sensor noise models")\r\n    print(" Implementing system identification for dynamics")\r\n    print(" Using domain adaptation techniques")\r\n    print(" Applying domain randomization curriculum")\r\n    \r\n    # Step 5: Final evaluation\r\n    print("\\n=== Final Results ===")\r\n    print(f"Training completed with {len(training_scores)} episodes")\r\n    print(f"Final training score: {training_scores[1]:.2f}")\r\n    print("Transfer evaluation completed successfully")\r\n    \r\n    # Calculate improvement metrics\r\n    if len(training_scores) > 100:\r\n        early_perf = np.mean(training_scores[:100])\r\n        late_perf = np.mean(training_scores[100:])\r\n        improvement = (late_perf  early_perf) / (early_perf + 1e8) * 100\r\n        print(f"Learning improvement: {improvement:.2f}%")\r\n    \r\n    return agent, evaluator\r\n\r\ndef main():\r\n    """Main function to run complete RL pipeline"""\r\n    print("Starting Complete Reinforcement Learning & SimtoReal Pipeline")\r\n    \r\n    try:\r\n        agent, evaluator = run_complete_transfer_pipeline()\r\n        \r\n        print("\\nPipeline execution completed successfully!")\r\n        print("The trained agent can now be tested on real hardware")\r\n        print("(following safety protocols and with proper physical constraints)")\r\n        \r\n        # Save final model\r\n        agent.save_checkpoint(\'final_rl_agent.pth\')\r\n        print("Final model saved as \'final_rl_agent.pth\'")\r\n        \r\n    except Exception as e:\r\n        print(f"Error during execution: {str(e)}")\r\n        import traceback\r\n        traceback.print_exc()\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,t.jsx)(e.p,{children:"Create a complete reinforcement learning system for a specific robotic task (e.g., hopper locomotion, manipulator reaching) that:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implements domain randomization techniques to improve simtoreal transfer"}),"\n",(0,t.jsx)(e.li,{children:"Trains multiple RL algorithms (PPO, SAC, DDPG) and compares their performance"}),"\n",(0,t.jsx)(e.li,{children:"Evaluates the simtoreal transfer gap using multiple metrics"}),"\n",(0,t.jsx)(e.li,{children:"Implements system identification to characterize the real robot's dynamics"}),"\n",(0,t.jsx)(e.li,{children:"Applies domain adaptation techniques to improve transfer"}),"\n",(0,t.jsx)(e.li,{children:"Tests the policy on a physical robot or realistic simulation"}),"\n",(0,t.jsx)(e.li,{children:"Documents the transfer performance and identifies key factors affecting success"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Your project should include:\r\nComplete RL training pipeline with domain randomization\r\nMultiple RL algorithm implementations\r\nTransfer evaluation system with metrics\r\nDynamics characterization and adaptation\r\nPerformance comparison across algorithms\r\nRecommendations for improving simtoreal transfer"}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter covered reinforcement learning and simtoreal transfer:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"RL Algorithms"}),": Deep reinforcement learning methods suitable for robotics\r\n",(0,t.jsx)(e.strong,{children:"Domain Randomization"}),": Techniques to make simtoreal transfer more robust\r\n",(0,t.jsx)(e.strong,{children:"Transfer Evaluation"}),": Methods to assess the effectiveness of simtoreal policies\r\n",(0,t.jsx)(e.strong,{children:"Dynamics Modeling"}),": Approaches to characterize and adapt to reality gaps\r\n",(0,t.jsx)(e.strong,{children:"System Identification"}),": Techniques to understand real robot dynamics\r\n",(0,t.jsx)(e.strong,{children:"Practical Considerations"}),": Safety and practical constraints for realworld deployment"]}),"\n",(0,t.jsx)(e.p,{children:"Successful simtoreal transfer requires careful attention to the differences between simulation and reality, with domain randomization being one of the most effective techniques to create robust policies that generalize to the real world."})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}}}]);