"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[9470],{6260:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"module-4-vision-language-action/ch20-capstone-autonomous-humanoid","title":"ch20-capstone-autonomous-humanoid","description":"-----","source":"@site/docs/module-4-vision-language-action/ch20-capstone-autonomous-humanoid.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/ch20-capstone-autonomous-humanoid","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch20-capstone-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-4-vision-language-action/ch20-capstone-autonomous-humanoid.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch19-multimodal-perception-fusion","permalink":"/physical-ai-textbook-2025/docs/module-4-vision-language-action/ch19-multimodal-perception-fusion"}}');var o=r(4848),i=r(8453),a=r(7242);const s={},l=void 0,c={},p=[{value:"title: Ch20  Capstone  Autonomous Humanoid (Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate)\r\nmodule: 4\r\nchapter: 20\r\nsidebar_label: Ch20: Capstone  Autonomous Humanoid\r\ndescription: Capstone project integrating all modules for voicecontrolled autonomous humanoid robot\r\ntags: [capstone, humanoid, autonomous, voicecontrol, navigation, manipulation, integration]\r\ndifficulty: advanced\r\nestimated_duration: 180",id:"title-ch20--capstone--autonomous-humanoid-voice--plan--navigate--manipulatemodule-4chapter-20sidebar_label-ch20-capstone--autonomous-humanoiddescription-capstone-project-integrating-all-modules-for-voicecontrolled-autonomous-humanoid-robottags-capstone-humanoid-autonomous-voicecontrol-navigation-manipulation-integrationdifficulty-advancedestimated_duration-180",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"Complete Humanoid System Architecture",id:"complete-humanoid-system-architecture",level:3},{value:"VoicetoAction Pipeline",id:"voicetoaction-pipeline",level:3},{value:"Integration Challenges",id:"integration-challenges",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Setting up the Complete Humanoid System",id:"lab-1-setting-up-the-complete-humanoid-system",level:3},{value:"Lab 2: Creating the VoicetoAction Pipeline",id:"lab-2-creating-the-voicetoaction-pipeline",level:3},{value:"Lab 3: Implementing the Complete Navigation and Manipulation System",id:"lab-3-implementing-the-complete-navigation-and-manipulation-system",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Launch file for the complete system:",id:"launch-file-for-the-complete-system",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"title-ch20--capstone--autonomous-humanoid-voice--plan--navigate--manipulatemodule-4chapter-20sidebar_label-ch20-capstone--autonomous-humanoiddescription-capstone-project-integrating-all-modules-for-voicecontrolled-autonomous-humanoid-robottags-capstone-humanoid-autonomous-voicecontrol-navigation-manipulation-integrationdifficulty-advancedestimated_duration-180",children:"title: Ch20  Capstone  Autonomous Humanoid (Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate)\r\nmodule: 4\r\nchapter: 20\r\nsidebar_label: Ch20: Capstone  Autonomous Humanoid\r\ndescription: Capstone project integrating all modules for voicecontrolled autonomous humanoid robot\r\ntags: [capstone, humanoid, autonomous, voicecontrol, navigation, manipulation, integration]\r\ndifficulty: advanced\r\nestimated_duration: 180"}),"\n","\n",(0,o.jsx)(n.h1,{id:"capstone--autonomous-humanoid-voice--plan--navigate--manipulate",children:"Capstone  Autonomous Humanoid (Voice \u2192 Plan \u2192 Navigate \u2192 Manipulate)"}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"Integrate all previous modules (ROS 2, Digital Twins, Isaac Platform, VisionLanguageAction)\r\nImplement a complete voicecontrolled humanoid robot system\r\nCreate an endtoend pipeline from voice input to physical action\r\nDemonstrate advanced capabilities: voice recognition, cognitive planning, navigation, and manipulation\r\nIntegrate perception, planning, and control systems\r\nEvaluate the complete autonomous humanoid system\r\nDocument performance and limitations of the integrated system"}),"\n",(0,o.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(n.h3,{id:"complete-humanoid-system-architecture",children:"Complete Humanoid System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The autonomous humanoid system integrates all the modules learned throughout the course into a cohesive architecture:"}),"\n",(0,o.jsx)(a.A,{chart:"\ngraph TB;\n  A[Voice Command] > B[ASR];\n  B > C[NLP];\n  C > D[Task Planning];\n  D > E[Navigation];\n  D > F[Manipulation];\n  E > G[Path Planning];\n  E > H[Local Navigation];\n  F > I[Grasp Planning];\n  F > J[Arm Control];\n  \n  K[Perception System] > L[Computer Vision];\n  K > M[LiDAR Processing];\n  K > N[IMU Integration];\n  K > O[Sensor Fusion];\n  \n  L > P[Object Detection];\n  M > Q[Environment Mapping];\n  N > R[State Estimation];\n  O > S[Scene Understanding];\n  \n  P > D;\n  Q > E;\n  R > E;\n  S > D;\n  \n  G > H;\n  I > J;\n  H > T[Humanoid Control];\n  J > T;\n  T > U[Humanoid Robot];\n  \n  V[Real World] > U;\n  U > W[Sensors];\n  W > K;\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style D fill:#2196F3,stroke:#0D47A1,color:#fff;\n  style T fill:#FF9800,stroke:#E65100,color:#fff;\n  style U fill:#E91E63,stroke:#AD1457,color:#fff;\n"}),"\n",(0,o.jsx)(n.h3,{id:"voicetoaction-pipeline",children:"VoicetoAction Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The system processes user commands through multiple stages:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Automatic Speech Recognition"}),": Converts voice to text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Processing"}),": Interprets user intent"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Planning"}),": Decomposes highlevel goals into actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception Processing"}),": Understands the environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Controls the physical robot"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,o.jsxs)(n.p,{children:["Key challenges in integrating all modules:\r\n",(0,o.jsx)(n.strong,{children:"Timing Coordination"}),": Ensuring all subsystems operate in harmony\r\n",(0,o.jsx)(n.strong,{children:"Data Consistency"}),": Maintaining consistent coordinate frames and time stamps\r\n",(0,o.jsx)(n.strong,{children:"Error Propagation"}),": Managing how errors in one module affect others\r\n",(0,o.jsx)(n.strong,{children:"Resource Management"}),": Efficiently allocating computational resources across modules"]}),"\n",(0,o.jsx)(n.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,o.jsx)(n.h3,{id:"lab-1-setting-up-the-complete-humanoid-system",children:"Lab 1: Setting up the Complete Humanoid System"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create the main system orchestration"})," (",(0,o.jsx)(n.code,{children:"humanoid_system_orchestrator.py"}),"):","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport numpy as np\r\nimport openai\r\nimport whisper\r\nimport torch\r\nimport torch.nn as nn\r\nimport json\r\nimport threading\r\nimport queue\r\nfrom typing import Dict, List, Optional, Any\r\nfrom std_msgs.msg import String, Bool, Float32MultiArray\r\nfrom geometry_msgs.msg import Pose, Twist, Point\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu, JointState\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nfrom actionlib_msgs.msg import GoalStatusArray\r\nfrom humanoid_control_msgs.msg import JointCommand, CartesianCommand\r\nimport py_trees\r\nimport py_trees_ros\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport actionlib\r\n\r\nclass HumanoidSystemOrchestrator:\r\n    def __init__(self):\r\n        rospy.init_node(\'humanoid_system_orchestrator\', anonymous=True)\r\n        \r\n        # Initialize components\r\n        self.cv_bridge = CvBridge()\r\n        self.task_queue = queue.Queue()\r\n        self.execution_tree = None\r\n        \r\n        # Humanoid configuration\r\n        self.robot_config = {\r\n            \'height\': 1.5,  # meters\r\n            \'weight\': 30,   # kg\r\n            \'max_linear_speed\': 0.3,  # m/s\r\n            \'max_angular_speed\': 0.5,  # rad/s\r\n            \'arm_dof\': 6,\r\n            \'leg_dof\': 6,\r\n            \'joint_limits\': {},  # Will be populated from URDF\r\n        }\r\n        \r\n        # Publishers and Subscribers\r\n        self.voice_cmd_sub = rospy.Subscriber(\'/voice_command\', String, self.voice_callback)\r\n        self.task_plan_pub = rospy.Publisher(\'/task_plan\', String, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/humanoid_status\', String, queue_size=10)\r\n        self.feedback_pub = rospy.Publisher(\'/humanoid_feedback\', String, queue_size=10)\r\n        \r\n        # Perception subscribers\r\n        self.rgb_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.rgb_callback)\r\n        self.lidar_sub = rospy.Subscriber(\'/scan\', LaserScan, self.lidar_callback)\r\n        self.imu_sub = rospy.Subscriber(\'/imu/data\', Imu, self.imu_callback)\r\n        self.joint_state_sub = rospy.Subscriber(\'/joint_states\', JointState, self.joint_state_callback)\r\n        \r\n        # Control publishers\r\n        self.joint_cmd_pub = rospy.Publisher(\'/joint_commands\', JointCommand, queue_size=10)\r\n        self.cartesian_cmd_pub = rospy.Publisher(\'/cartesian_commands\', CartesianCommand, queue_size=10)\r\n        self.base_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\r\n        \r\n        # Action clients\r\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\r\n        self.move_base_client.wait_for_server()\r\n        \r\n        # System state\r\n        self.current_pose = Pose()\r\n        self.joint_positions = {}\r\n        self.perception_data = {}\r\n        self.is_executing = False\r\n        self.current_task = None\r\n        \r\n        # LLM integration\r\n        self.openai_client = None\r\n        self.whisper_model = None\r\n        \r\n        rospy.loginfo("Humanoid System Orchestrator initialized")\r\n    \r\n    def setup_llm_integration(self, api_key: str, whisper_model_size: str = "base"):\r\n        """Initialize LLM and Whisper integration"""\r\n        openai.api_key = api_key\r\n        self.openai_client = openai.OpenAI(api_key=api_key)\r\n        self.whisper_model = whisper.load_model(whisper_model_size)\r\n        rospy.loginfo("LLM integration initialized")\r\n    \r\n    def voice_callback(self, msg: String):\r\n        """Handle incoming voice commands"""\r\n        command_text = msg.data\r\n        rospy.loginfo(f"Received voice command: {command_text}")\r\n        \r\n        # Process command and add to execution queue\r\n        task = self.process_voice_command(command_text)\r\n        if task:\r\n            self.task_queue.put(task)\r\n            rospy.loginfo(f"Queued task: {task[\'action\']}")\r\n    \r\n    def process_voice_command(self, command: str) > Optional[Dict]:\r\n        """Process voice command with LLM and create task"""\r\n        if not self.openai_client:\r\n            rospy.logerr("LLM not initialized")\r\n            return None\r\n        \r\n        # Get perception context\r\n        context = self.get_perception_context()\r\n        \r\n        # Create prompt for task planning\r\n        prompt = f"""\r\n        User Command: "{command}"\r\n        \r\n        Environment Context: {json.dumps(context, indent=2)}\r\n        \r\n        Available Actions:\r\n         navigate_to(location_name): Move humanoid to named location\r\n         approach_object(object_name): Navigate to an object\r\n         pick_up_object(object_name, grasp_pose): Grasp an object\r\n         place_object(object_name, location): Place object at location\r\n         inspect_object(object_name): Examine an object\r\n         follow_person(person_name): Follow a person\r\n         answer_question(question): Respond to question about environment\r\n         open_container(container_name): Open door/drawer\r\n         close_container(container_name): Close door/drawer\r\n         wave_to(person_name): Wave to a person\r\n         take_posture(posture_name): Change body posture\r\n         speak_response(text): Speak a response\r\n        \r\n        Decompose the user command into specific executable tasks.\r\n        Consider the environment context and robot capabilities.\r\n        \r\n        Return as a JSON object with structure:\r\n        {{\r\n          "action": "action_name",\r\n          "parameters": {{"param1": "value1", "param2": "value2"}},\r\n          "reasoning": "why this action is appropriate",\r\n          "estimated_duration": 60.0,\r\n          "confidence": 0.8\r\n        }}\r\n        \r\n        Respond only with the JSON object.\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4o",\r\n                messages=[\r\n                    {\r\n                        "role": "system",\r\n                        "content": "You are a humanoid robot task planner. Generate specific, executable robotic tasks based on user commands and environment context. Respond only with the JSON object as specified."\r\n                    },\r\n                    {\r\n                        "role": "user",\r\n                        "content": prompt\r\n                    }\r\n                ],\r\n                temperature=0.1,\r\n                max_tokens=1000\r\n            )\r\n            \r\n            response_text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            if response_text.startswith(\'```\'):\r\n                start_idx = response_text.find(\'{\')\r\n                end_idx = response_text.rfind(\'}\') + 1\r\n                response_text = response_text[start_idx:end_idx]\r\n            \r\n            task = json.loads(response_text)\r\n            return task\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing voice command: {e}")\r\n            return None\r\n    \r\n    def get_perception_context(self) > Dict:\r\n        """Get current perception context"""\r\n        return {\r\n            "robot_pose": {\r\n                "x": self.current_pose.position.x,\r\n                "y": self.current_pose.position.y,\r\n                "z": self.current_pose.position.z,\r\n                "qx": self.current_pose.orientation.x,\r\n                "qy": self.current_pose.orientation.y,\r\n                "qz": self.current_pose.orientation.z,\r\n                "qw": self.current_pose.orientation.w\r\n            },\r\n            "joint_positions": self.joint_positions,\r\n            "known_locations": ["kitchen", "living_room", "bedroom", "office", "entrance"],\r\n            "detected_objects": self.perception_data.get("objects", []),\r\n            "people_present": self.perception_data.get("people", []),\r\n            "robot_capabilities": ["navigation", "manipulation", "grasping", "speech", "posture_control"],\r\n            "current_battery": self.perception_data.get("battery_level", 0.8)\r\n        }\r\n    \r\n    def rgb_callback(self, msg: Image):\r\n        """Process RGB camera data"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Run object detection and people recognition\r\n            detection_result = self.run_perception_pipeline(cv_image)\r\n            self.perception_data.update(detection_result)\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing RGB image: {e}")\r\n    \r\n    def run_perception_pipeline(self, image):\r\n        """Run perception pipeline (in real implementation, this would use YOLO, etc.)"""\r\n        # In a real implementation, this would use object detection models\r\n        # For this example, we\'ll simulate detection\r\n        return {\r\n            "objects": [\r\n                {"name": "bottle", "position": [1.2, 0.5, 0.0], "class": "container"},\r\n                {"name": "chair", "position": [0.0, 1.0, 0.0], "class": "furniture"}\r\n            ],\r\n            "people": [\r\n                {"name": "person1", "position": [2.0, 1.0, 0.0]}\r\n            ]\r\n        }\r\n    \r\n    def lidar_callback(self, msg: LaserScan):\r\n        """Process LiDAR data"""\r\n        # Process for navigation planning\r\n        self.perception_data[\'lidar_ranges\'] = list(msg.ranges)\r\n        self.perception_data[\'lidar_angle_min\'] = msg.angle_min\r\n        self.perception_data[\'lidar_angle_max\'] = msg.angle_max\r\n        self.perception_data[\'lidar_angle_increment\'] = msg.angle_increment\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """Process IMU data"""\r\n        self.perception_data[\'imu\'] = {\r\n            \'linear_acceleration\': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z],\r\n            \'angular_velocity\': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\r\n            \'orientation\': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\r\n        }\r\n    \r\n    def joint_state_callback(self, msg: JointState):\r\n        """Process joint state data"""\r\n        for i, name in enumerate(msg.name):\r\n            self.joint_positions[name] = msg.position[i]\r\n    \r\n    def execute_task(self, task: Dict):\r\n        """Execute a single task"""\r\n        action = task[\'action\']\r\n        params = task.get(\'parameters\', {})\r\n        \r\n        rospy.loginfo(f"Executing task: {action} with params: {params}")\r\n        \r\n        if action == \'navigate_to\':\r\n            return self.execute_navigation_task(params)\r\n        elif action == \'pick_up_object\':\r\n            return self.execute_pickup_task(params)\r\n        elif action == \'place_object\':\r\n            return self.execute_place_task(params)\r\n        elif action == \'approach_object\':\r\n            return self.execute_approach_task(params)\r\n        elif action == \'speak_response\':\r\n            return self.execute_speech_task(params)\r\n        elif action == \'inspect_object\':\r\n            return self.execute_inspection_task(params)\r\n        else:\r\n            rospy.logwarn(f"Unknown action: {action}")\r\n            return False\r\n    \r\n    def execute_navigation_task(self, params: Dict) > bool:\r\n        """Execute navigation task"""\r\n        location = params.get(\'location_name\')\r\n        \r\n        # In a real implementation, this would look up location coordinates\r\n        # For now, we\'ll use simple coordinates\r\n        location_coords = {\r\n            \'kitchen\': (2.0, 1.0),\r\n            \'living_room\': (0.0, 0.0),\r\n            \'bedroom\': (1.0, 2.0),\r\n            \'office\': (1.0, 1.0),\r\n            \'entrance\': (0.0, 2.0)\r\n        }\r\n        \r\n        if location not in location_coords:\r\n            rospy.logerr(f"Unknown location: {location}")\r\n            return False\r\n        \r\n        x, y = location_coords[location]\r\n        \r\n        # Create navigation goal\r\n        goal = MoveBaseGoal()\r\n        goal.target_pose.header.frame_id = "map"\r\n        goal.target_pose.header.stamp = rospy.Time.now()\r\n        goal.target_pose.pose.position.x = x\r\n        goal.target_pose.pose.position.y = y\r\n        goal.target_pose.pose.position.z = 0.0\r\n        goal.target_pose.pose.orientation.w = 1.0\r\n        \r\n        # Send goal\r\n        self.move_base_client.send_goal(goal)\r\n        \r\n        # Wait for result\r\n        finished_within_time = self.move_base_client.wait_for_result(rospy.Duration(180))  # 3 minutes\r\n        \r\n        if not finished_within_time:\r\n            self.move_base_client.cancel_goal()\r\n            rospy.logerr("Navigation took too long")\r\n            return False\r\n        \r\n        state = self.move_base_client.get_state()\r\n        if state == 3:  # SUCCEEDED\r\n            rospy.loginfo(f"Successfully navigated to {location}")\r\n            return True\r\n        else:\r\n            rospy.logerr(f"Navigation failed with state: {state}")\r\n            return False\r\n    \r\n    def execute_approach_task(self, params: Dict) > bool:\r\n        """Execute approach object task"""\r\n        object_name = params.get(\'object_name\')\r\n        \r\n        # Find object in perception data\r\n        target_object = None\r\n        for obj in self.perception_data.get(\'objects\', []):\r\n            if obj[\'name\'] == object_name:\r\n                target_object = obj\r\n                break\r\n        \r\n        if not target_object:\r\n            rospy.logerr(f"Object {object_name} not found")\r\n            return False\r\n        \r\n        # Navigate to object position (with safe distance)\r\n        obj_pos = target_object[\'position\']\r\n        goal = MoveBaseGoal()\r\n        goal.target_pose.header.frame_id = "map"\r\n        goal.target_pose.header.stamp = rospy.Time.now()\r\n        goal.target_pose.pose.position.x = obj_pos[0]  0.5  # 0.5m in front of object\r\n        goal.target_pose.pose.position.y = obj_pos[1]\r\n        goal.target_pose.pose.position.z = 0.0\r\n        goal.target_pose.pose.orientation.w = 1.0\r\n        \r\n        self.move_base_client.send_goal(goal)\r\n        finished = self.move_base_client.wait_for_result(rospy.Duration(60))\r\n        \r\n        if finished:\r\n            state = self.move_base_client.get_state()\r\n            if state == 3:  # SUCCEEDED\r\n                rospy.loginfo(f"Approached {object_name}")\r\n                return True\r\n        \r\n        rospy.logerr(f"Failed to approach {object_name}")\r\n        return False\r\n    \r\n    def execute_pickup_task(self, params: Dict) > bool:\r\n        """Execute pickup object task"""\r\n        # In a real implementation, this would involve complex manipulation planning\r\n        # For now, we\'ll simulate the action\r\n        object_name = params.get(\'object_name\')\r\n        rospy.loginfo(f"Picked up {object_name}")\r\n        return True\r\n    \r\n    def execute_place_task(self, params: Dict) > bool:\r\n        """Execute place object task"""\r\n        object_name = params.get(\'object_name\')\r\n        location = params.get(\'location\')\r\n        rospy.loginfo(f"Placed {object_name} at {location}")\r\n        return True\r\n    \r\n    def execute_speech_task(self, params: Dict) > bool:\r\n        """Execute speech task"""\r\n        text = params.get(\'text\', \'\')\r\n        rospy.loginfo(f"Speaking: {text}")\r\n        # In real implementation, this would use texttospeech\r\n        return True\r\n    \r\n    def execute_inspection_task(self, params: Dict) > bool:\r\n        """Execute inspection task"""\r\n        object_name = params.get(\'object_name\')\r\n        rospy.loginfo(f"Inspected {object_name}")\r\n        return True\r\n    \r\n    def run_execution_loop(self):\r\n        """Main execution loop"""\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        \r\n        while not rospy.is_shutdown():\r\n            try:\r\n                # Check for tasks in queue\r\n                while not self.task_queue.empty():\r\n                    task = self.task_queue.get_nowait()\r\n                    \r\n                    if task:\r\n                        self.current_task = task\r\n                        rospy.loginfo(f"Starting execution of task: {task[\'action\']}")\r\n                        \r\n                        success = self.execute_task(task)\r\n                        \r\n                        if success:\r\n                            rospy.loginfo(f"Successfully completed task: {task[\'action\']}")\r\n                            feedback = {\r\n                                \'status\': \'success\',\r\n                                \'task\': task[\'action\'],\r\n                                \'timestamp\': rospy.Time.now().to_sec()\r\n                            }\r\n                        else:\r\n                            rospy.logerr(f"Failed to execute task: {task[\'action\']}")\r\n                            feedback = {\r\n                                \'status\': \'failure\', \r\n                                \'task\': task[\'action\'],\r\n                                \'timestamp\': rospy.Time.now().to_sec(),\r\n                                \'error\': \'Execution failed\'\r\n                            }\r\n                        \r\n                        # Publish feedback\r\n                        feedback_msg = String()\r\n                        feedback_msg.data = json.dumps(feedback)\r\n                        self.feedback_pub.publish(feedback_msg)\r\n                        \r\n                        # Update current task\r\n                        self.current_task = None\r\n                \r\n                rate.sleep()\r\n                \r\n            except queue.Empty:\r\n                # No tasks in queue, continue loop\r\n                pass\r\n            except Exception as e:\r\n                rospy.logerr(f"Error in execution loop: {e}")\r\n\r\n# Example usage\r\ndef main():\r\n    orchestrator = HumanoidSystemOrchestrator()\r\n    \r\n    # Setup API key\r\n    api_key = input("Enter OpenAI API key: ")\r\n    if api_key:\r\n        orchestrator.setup_llm_integration(api_key)\r\n    \r\n    try:\r\n        orchestrator.run_execution_loop()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down humanoid system orchestrator...")\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"lab-2-creating-the-voicetoaction-pipeline",children:"Lab 2: Creating the VoicetoAction Pipeline"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implement the complete voice processing pipeline"})," (",(0,o.jsx)(n.code,{children:"voice_to_action_pipeline.py"}),"):","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport whisper\r\nimport openai\r\nimport torch\r\nimport json\r\nimport numpy as np\r\nimport pyaudio\r\nimport wave\r\nimport queue\r\nimport threading\r\nimport time\r\nfrom std_msgs.msg import String, Bool\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Pose\r\nfrom typing import Optional, Dict, Any\r\nfrom cv_bridge import CvBridge\r\n\r\nclass VoiceToActionPipeline:\r\n    def __init__(self, api_key: str, whisper_model_size: str = "base"):\r\n        rospy.init_node(\'voice_to_action_pipeline\', anonymous=True)\r\n        \r\n        # Initialize Whisper model\r\n        self.whisper_model = whisper.load_model(whisper_model_size)\r\n        self.openai_client = openai.OpenAI(api_key=api_key)\r\n        \r\n        # Initialize components\r\n        self.cv_bridge = CvBridge()\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000  # Whisper expects 16kHz\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.audio = pyaudio.PyAudio()\r\n        \r\n        # Publishers and Subscribers\r\n        self.voice_cmd_pub = rospy.Publisher(\'/voice_command\', String, queue_size=10)\r\n        self.status_pub = rospy.Publisher(\'/voice_pipeline_status\', String, queue_size=10)\r\n        self.perception_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.perception_callback)\r\n        rospy.Subscriber(\'/toggle_voice_control\', Bool, self.toggle_callback)\r\n        \r\n        # System state\r\n        self.is_listening = False\r\n        self.perception_data = {}\r\n        self.listening_thread = None\r\n        self.processing_thread = None\r\n        self.context_memory = []\r\n        self.max_context_items = 20\r\n        \r\n        rospy.loginfo("VoicetoAction Pipeline initialized")\r\n    \r\n    def toggle_callback(self, msg: Bool):\r\n        """Toggle voice control on/off"""\r\n        if msg.data:\r\n            self.start_listening()\r\n        else:\r\n            self.stop_listening()\r\n    \r\n    def perception_callback(self, msg):\r\n        """Process perception data for context"""\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            # Simple object detection simulation\r\n            h, w, _ = cv_image.shape\r\n            # Simulate detecting an object at center\r\n            if np.mean(cv_image[h//250:h//2+50, w//250:w//2+50]) > 100:  # Simple threshold\r\n                self.perception_data[\'objects\'] = [{\'name\': \'object\', \'position\': \'center\'}]\r\n        except:\r\n            pass\r\n    \r\n    def start_listening(self):\r\n        """Start voice recognition"""\r\n        if self.is_listening:\r\n            return\r\n        \r\n        self.is_listening = True\r\n        \r\n        # Start audio recording thread\r\n        self.listening_thread = threading.Thread(target=self._record_audio_continuously)\r\n        self.listening_thread.daemon = True\r\n        self.listening_thread.start()\r\n        \r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self._process_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        rospy.loginfo("VoicetoAction Pipeline started")\r\n        self.status_pub.publish(String(data="Voice pipeline active"))\r\n    \r\n    def stop_listening(self):\r\n        """Stop voice recognition"""\r\n        self.is_listening = False\r\n        rospy.loginfo("VoicetoAction Pipeline stopped")\r\n        self.status_pub.publish(String(data="Voice pipeline inactive"))\r\n    \r\n    def _record_audio_continuously(self):\r\n        """Continuously record audio and detect speech"""\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        recording = False\r\n        frames = []\r\n        voice_threshold = 0.01\r\n        silence_duration = 0\r\n        voice_duration = 0\r\n        min_voice_duration = 0.5  # seconds\r\n        min_silence_duration = 1.0  # seconds\r\n        max_recording_duration = 10.0  # seconds\r\n        \r\n        try:\r\n            while self.is_listening:\r\n                data = stream.read(self.chunk, exception_on_overflow=False)\r\n                \r\n                # Convert to numpy for analysis\r\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n                rms = np.sqrt(np.mean(audio_array ** 2))\r\n                \r\n                if rms > voice_threshold:\r\n                    if not recording:\r\n                        # Start recording\r\n                        recording = True\r\n                        frames = [data]\r\n                        silence_duration = 0\r\n                        voice_duration = 0\r\n                    else:\r\n                        # Continue recording\r\n                        frames.append(data)\r\n                        voice_duration += self.chunk / self.rate\r\n                        silence_duration = 0\r\n                else:\r\n                    if recording:\r\n                        # Accumulate silence\r\n                        frames.append(data)  # Keep in buffer (might be trailing speech)\r\n                        silence_duration += self.chunk / self.rate\r\n                        voice_duration += self.chunk / self.rate\r\n                        \r\n                        # Check if we have enough speech and silence to trigger transcription\r\n                        if (silence_duration > min_silence_duration and \r\n                            voice_duration > min_voice_duration):\r\n                            # Finished speaking\r\n                            if len(frames) > 0:\r\n                                # Save to temp file\r\n                                temp_file = self._save_frames_to_wav(frames)\r\n                                self.command_queue.put(temp_file)\r\n                                \r\n                                rospy.loginfo(f"Recorded speech segment, duration: {voice_duration:.2f}s")\r\n                                \r\n                                # Reset for next recording\r\n                                recording = False\r\n                                frames = []\r\n                                silence_duration = 0\r\n                                voice_duration = 0\r\n                        elif voice_duration > max_recording_duration:\r\n                            # Max duration reached, transcribe what we have\r\n                            if len(frames) > 0:\r\n                                temp_file = self._save_frames_to_wav(frames)\r\n                                self.command_queue.put(temp_file)\r\n                                \r\n                                rospy.loginfo(f"Max recording duration reached, transcribing: {voice_duration:.2f}s")\r\n                                \r\n                                # Reset for next recording\r\n                                recording = False\r\n                                frames = []\r\n                                silence_duration = 0\r\n                                voice_duration = 0\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in audio recording: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n    \r\n    def _save_frames_to_wav(self, frames):\r\n        """Save audio frames to a temporary WAV file"""\r\n        import tempfile\r\n        import os\r\n        \r\n        temp_fd, temp_path = tempfile.mkstemp(suffix=\'.wav\')\r\n        wf = wave.open(temp_path, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        os.close(temp_fd)\r\n        \r\n        return temp_path\r\n    \r\n    def _process_commands(self):\r\n        """Process transcribed commands"""\r\n        while self.is_listening or not self.command_queue.empty():\r\n            try:\r\n                audio_file = self.command_queue.get(timeout=1.0)\r\n                \r\n                # Transcribe audio\r\n                result = self.whisper_model.transcribe(audio_file)\r\n                transcription = result[\'text\'].strip()\r\n                \r\n                if transcription:\r\n                    rospy.loginfo(f"Transcribed: {transcription}")\r\n                    \r\n                    # Enhance with context\r\n                    contextual_command = self._enhance_with_context(transcription)\r\n                    \r\n                    # Publish command\r\n                    cmd_msg = String()\r\n                    cmd_msg.data = contextual_command\r\n                    self.voice_cmd_pub.publish(cmd_msg)\r\n                    \r\n                    # Update context memory\r\n                    self._update_context(transcription, contextual_command)\r\n                    \r\n                    rospy.loginfo(f"Published contextual command: {contextual_command}")\r\n                else:\r\n                    rospy.loginfo("Empty transcription")\r\n                \r\n                # Cleanup temp file\r\n                if os.path.exists(audio_file):\r\n                    os.remove(audio_file)\r\n                \r\n                self.command_queue.task_done()\r\n                \r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                rospy.logerr(f"Error processing command: {e}")\r\n    \r\n    def _enhance_with_context(self, transcription: str) > str:\r\n        """Enhance transcription with environmental context"""\r\n        # Get recent context\r\n        recent_context = self._get_recent_context()\r\n        \r\n        prompt = f"""\r\n        Original Voice Command: "{transcription}"\r\n        \r\n        Environmental Context: {recent_context}\r\n        \r\n        Enhance the original command with environmental context. \r\n        If the original command refers to objects or locations that can be clarified, \r\n        make those references more specific based on the context.\r\n        If the command is ambiguous, use the context to disambiguate.\r\n        If the command is clear as is, return it unchanged.\r\n        \r\n        Return only the enhanced command, nothing else.\r\n        """\r\n        \r\n        try:\r\n            response = self.openai_client.chat.completions.create(\r\n                model="gpt4o",\r\n                messages=[\r\n                    {\r\n                        "role": "system",\r\n                        "content": "Enhance voice commands with environmental context. Return only the enhanced command text."\r\n                    },\r\n                    {\r\n                        "role": "user",\r\n                        "content": prompt\r\n                    }\r\n                ],\r\n                temperature=0.1,\r\n                max_tokens=200\r\n            )\r\n            \r\n            enhanced_command = response.choices[0].message.content.strip()\r\n            return enhanced_command\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error enhancing command with context: {e}")\r\n            return transcription  # Return original if enhancement fails\r\n    \r\n    def _get_recent_context(self) > str:\r\n        """Get recent context from memory"""\r\n        if not self.context_memory:\r\n            return "No recent interactions. Environment has common household objects and locations."\r\n        \r\n        recent_items = self.context_memory[5:]  # Last 5 interactions\r\n        context_str = "Recent interactions: "\r\n        context_str += "; ".join([item[\'original\'] for item in recent_items])\r\n        context_str += f". Currently perceived objects: {self.perception_data.get(\'objects\', [])}"\r\n        \r\n        return context_str\r\n    \r\n    def _update_context(self, original: str, enhanced: str):\r\n        """Update context memory with new interaction"""\r\n        context_item = {\r\n            \'timestamp\': time.time(),\r\n            \'original\': original,\r\n            \'enhanced\': enhanced\r\n        }\r\n        \r\n        self.context_memory.append(context_item)\r\n        \r\n        # Keep only recent items\r\n        if len(self.context_memory) > self.max_context_items:\r\n            self.context_memory = self.context_memory[self.max_context_items:]\r\n\r\ndef main():\r\n    api_key = input("Enter OpenAI API key: ")\r\n    if not api_key:\r\n        rospy.logerr("No API key provided")\r\n        return\r\n    \r\n    pipeline = VoiceToActionPipeline(api_key)\r\n    \r\n    try:\r\n        pipeline.start_listening()\r\n        rospy.spin()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo("Shutting down voicetoaction pipeline...")\r\n        pipeline.stop_listening()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"lab-3-implementing-the-complete-navigation-and-manipulation-system",children:"Lab 3: Implementing the Complete Navigation and Manipulation System"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create the navigation and manipulation controller"})," (",(0,o.jsx)(n.code,{children:"nav_manip_controller.py"}),"):","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rospy\r\nimport numpy as np\r\nimport py_trees\r\nimport py_trees_ros\r\nimport actionlib\r\nimport threading\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose, PoseStamped, Twist, Point\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nfrom actionlib_msgs.msg import GoalStatusArray\r\nfrom humanoid_control_msgs.msg import JointCommand, CartesianCommand\r\nfrom sensor_msgs.msg import LaserScan, Image, JointState\r\nfrom moveit_commander import MoveGroupCommander, PlanningSceneInterface\r\nfrom moveit_msgs.msg import CollisionObject\r\nfrom shape_msgs.msg import SolidPrimitive\r\nfrom visualization_msgs.msg import Marker\r\nfrom tf.transformations import quaternion_from_euler, euler_from_quaternion\r\nimport tf2_ros\r\nimport tf2_geometry_msgs\r\nimport math\r\n\r\nclass NavigationManipulationController:\r\n    def __init__(self):\r\n        rospy.init_node('nav_manip_controller', anonymous=True)\r\n        \r\n        # Initialize MoveIt! interface\r\n        self.robot_commander = MoveGroupCommander(\"arm\")\r\n        self.scene = PlanningSceneInterface()\r\n        self.tf_buffer = tf2_ros.Buffer()\r\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer)\r\n        \r\n        # Action clients\r\n        self.move_base_client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\r\n        self.move_base_client.wait_for_server()\r\n        \r\n        # Publishers and Subscribers\r\n        self.nav_task_sub = rospy.Subscriber('/navigation_task', String, self.navigation_callback)\r\n        self.manip_task_sub = rospy.Subscriber('/manipulation_task', String, self.manipulation_callback)\r\n        self.status_pub = rospy.Publisher('/nav_manip_status', String, queue_size=10)\r\n        self.marker_pub = rospy.Publisher('/visualization_marker', Marker, queue_size=10)\r\n        \r\n        # State\r\n        self.current_pose = Pose()\r\n        self.joint_states = {}\r\n        self.laser_data = None\r\n        self.is_navigating = False\r\n        self.is_manipulating = False\r\n        \r\n        # Robot parameters\r\n        self.arm_group_name = \"arm\"\r\n        self.gripper_group_name = \"gripper\"\r\n        \r\n        rospy.loginfo(\"Navigation and Manipulation Controller initialized\")\r\n    \r\n    def navigation_callback(self, msg: String):\r\n        \"\"\"Handle navigation tasks\"\"\"\r\n        try:\r\n            task_data = json.loads(msg.data)\r\n            task_type = task_data.get('type')\r\n            \r\n            if task_type == 'move_to_location':\r\n                location = task_data.get('location')\r\n                success = self.move_to_location(location)\r\n                self.report_status(f\"Move to {location}: {'Success' if success else 'Failed'}\")\r\n            elif task_type == 'approach_object':\r\n                object_name = task_data.get('object_name')\r\n                success = self.approach_object(object_name)\r\n                self.report_status(f\"Approach {object_name}: {'Success' if success else 'Failed'}\")\r\n        except json.JSONDecodeError:\r\n            rospy.logerr(f\"Invalid navigation task data: {msg.data}\")\r\n    \r\n    def manipulation_callback(self, msg: String):\r\n        \"\"\"Handle manipulation tasks\"\"\"\r\n        try:\r\n            task_data = json.loads(msg.data)\r\n            task_type = task_data.get('type')\r\n            \r\n            if task_type == 'pick_object':\r\n                object_pose = task_data.get('pose')\r\n                success = self.pick_object(object_pose)\r\n                self.report_status(f\"Pick object: {'Success' if success else 'Failed'}\")\r\n            elif task_type == 'place_object':\r\n                location_pose = task_data.get('pose')\r\n                success = self.place_object(location_pose)\r\n                self.report_status(f\"Place object: {'Success' if success else 'Failed'}\")\r\n        except json.JSONDecodeError:\r\n            rospy.logerr(f\"Invalid manipulation task data: {msg.data}\")\r\n    \r\n    def move_to_location(self, location_name: str) > bool:\r\n        \"\"\"Move to named location\"\"\"\r\n        # In a real implementation, these would be mapped to coordinates\r\n        location_map = {\r\n            'kitchen': (2.0, 1.0, 0.0),\r\n            'living_room': (0.0, 0.0, 0.0),\r\n            'bedroom': (1.0, 2.0, 0.0),\r\n            'office': (1.0, 1.0, 0.0),\r\n            'dining_room': (2.0, 1.0, 0.0)\r\n        }\r\n        \r\n        if location_name not in location_map:\r\n            rospy.logerr(f\"Unknown location: {location_name}\")\r\n            return False\r\n        \r\n        x, y, theta = location_map[location_name]\r\n        \r\n        # Create and send navigation goal\r\n        goal = MoveBaseGoal()\r\n        goal.target_pose.header.frame_id = \"map\"\r\n        goal.target_pose.header.stamp = rospy.Time.now()\r\n        goal.target_pose.pose.position.x = x\r\n        goal.target_pose.pose.position.y = y\r\n        goal.target_pose.pose.position.z = 0.0\r\n        \r\n        # Convert theta to quaternion\r\n        quat = quaternion_from_euler(0, 0, theta)\r\n        goal.target_pose.pose.orientation.x = quat[0]\r\n        goal.target_pose.pose.orientation.y = quat[1]\r\n        goal.target_pose.pose.orientation.z = quat[2]\r\n        goal.target_pose.pose.orientation.w = quat[3]\r\n        \r\n        return self.execute_navigation_goal(goal)\r\n    \r\n    def approach_object(self, object_name: str) > bool:\r\n        \"\"\"Approach a named object\"\"\"\r\n        # In a real system, this would get object position from perception\r\n        # For this example, we'll use hardcoded positions\r\n        object_positions = {\r\n            'table': (1.5, 0.5, 0.0),\r\n            'chair': (0.5, 1.0, 0.0),\r\n            'bottle': (2.0, 1.0, 0.0),\r\n            'box': (0.5, 1.5, 0.0)\r\n        }\r\n        \r\n        if object_name not in object_positions:\r\n            rospy.logerr(f\"Unknown object: {object_name}\")\r\n            return False\r\n        \r\n        obj_x, obj_y, _ = object_positions[object_name]\r\n        \r\n        # Calculate a position 1m in front of the object\r\n        current_x, current_y = self.get_current_position()\r\n        direction_to_obj = math.atan2(obj_y  current_y, obj_x  current_x)\r\n        \r\n        # Position 1m away from object facing it\r\n        approach_x = obj_x  1.0 * math.cos(direction_to_obj)\r\n        approach_y = obj_y  1.0 * math.sin(direction_to_obj)\r\n        \r\n        goal = MoveBaseGoal()\r\n        goal.target_pose.header.frame_id = \"map\"\r\n        goal.target_pose.header.stamp = rospy.Time.now()\r\n        goal.target_pose.pose.position.x = approach_x\r\n        goal.target_pose.pose.position.y = approach_y\r\n        goal.target_pose.pose.position.z = 0.0\r\n        \r\n        # Orient towards the object\r\n        quat = quaternion_from_euler(0, 0, direction_to_obj)\r\n        goal.target_pose.pose.orientation.x = quat[0]\r\n        goal.target_pose.pose.orientation.y = quat[1]\r\n        goal.target_pose.pose.orientation.z = quat[2]\r\n        goal.target_pose.pose.orientation.w = quat[3]\r\n        \r\n        return self.execute_navigation_goal(goal)\r\n    \r\n    def execute_navigation_goal(self, goal: MoveBaseGoal) > bool:\r\n        \"\"\"Execute navigation goal with monitoring\"\"\"\r\n        self.is_navigating = True\r\n        \r\n        # Send goal\r\n        self.move_base_client.send_goal(goal)\r\n        \r\n        # Monitor progress\r\n        rate = rospy.Rate(10)  # 10 Hz\r\n        start_time = rospy.Time.now()\r\n        timeout_duration = rospy.Duration(120)  # 2 minutes timeout\r\n        \r\n        while not rospy.is_shutdown():\r\n            # Check current status\r\n            state = self.move_base_client.get_state()\r\n            \r\n            if state == 3:  # SUCCEEDED\r\n                self.is_navigating = False\r\n                rospy.loginfo(\"Navigation successful\")\r\n                return True\r\n            elif state in [4, 5, 8]:  # ABORTED, REJECTED, LOST\r\n                self.is_navigating = False\r\n                rospy.logerr(f\"Navigation failed with state: {state}\")\r\n                return False\r\n            \r\n            # Check timeout\r\n            if rospy.Time.now()  start_time > timeout_duration:\r\n                self.move_base_client.cancel_goal()\r\n                self.is_navigating = False\r\n                rospy.logerr(\"Navigation timeout\")\r\n                return False\r\n            \r\n            rate.sleep()\r\n    \r\n    def pick_object(self, pose: Dict) > bool:\r\n        \"\"\"Pick up an object at the given pose\"\"\"\r\n        self.is_manipulating = True\r\n        \r\n        try:\r\n            # Set target pose\r\n            target_pose = Pose()\r\n            target_pose.position.x = pose['position']['x']\r\n            target_pose.position.y = pose['position']['y']\r\n            target_pose.position.z = pose['position']['z']\r\n            target_pose.orientation.x = pose['orientation']['x']\r\n            target_pose.orientation.y = pose['orientation']['y']\r\n            target_pose.orientation.z = pose['orientation']['z']\r\n            target_pose.orientation.w = pose['orientation']['w']\r\n            \r\n            # Plan motion\r\n            self.robot_commander.set_pose_target(target_pose)\r\n            plan = self.robot_commander.plan()\r\n            \r\n            if plan.joint_trajectory.points:\r\n                # Execute motion\r\n                success = self.robot_commander.execute(plan, wait=True)\r\n                self.is_manipulating = False\r\n                return success\r\n            else:\r\n                rospy.logerr(\"Failed to plan motion to object\")\r\n                self.is_manipulating = False\r\n                return False\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error in pick operation: {e}\")\r\n            self.is_manipulating = False\r\n            return False\r\n    \r\n    def place_object(self, pose: Dict) > bool:\r\n        \"\"\"Place object at the given pose\"\"\"\r\n        self.is_manipulating = True\r\n        \r\n        try:\r\n            # Set target pose\r\n            target_pose = Pose()\r\n            target_pose.position.x = pose['position']['x']\r\n            target_pose.position.y = pose['position']['y']\r\n            target_pose.position.z = pose['position']['z']\r\n            target_pose.orientation.x = pose['orientation']['x']\r\n            target_pose.orientation.y = pose['orientation']['y']\r\n            target_pose.orientation.z = pose['orientation']['z']\r\n            target_pose.orientation.w = pose['orientation']['w']\r\n            \r\n            # Plan motion\r\n            self.robot_commander.set_pose_target(target_pose)\r\n            plan = self.robot_commander.plan()\r\n            \r\n            if plan.joint_trajectory.points:\r\n                # Execute motion\r\n                success = self.robot_commander.execute(plan, wait=True)\r\n                self.is_manipulating = False\r\n                return success\r\n            else:\r\n                rospy.logerr(\"Failed to plan motion to placement location\")\r\n                self.is_manipulating = False\r\n                return False\r\n        except Exception as e:\r\n            rospy.logerr(f\"Error in place operation: {e}\")\r\n            self.is_manipulating = False\r\n            return False\r\n    \r\n    def get_current_position(self) > Tuple[float, float]:\r\n        \"\"\"Get robot's current position from TF\"\"\"\r\n        try:\r\n            transform = self.tf_buffer.lookup_transform(\r\n                \"map\", \"base_link\", rospy.Time(0), rospy.Duration(1.0))\r\n            \r\n            x = transform.transform.translation.x\r\n            y = transform.transform.translation.y\r\n            \r\n            return x, y\r\n        except Exception as e:\r\n            rospy.logwarn(f\"Could not get current position: {e}\")\r\n            return 0.0, 0.0  # Default to origin if TF unavailable\r\n    \r\n    def report_status(self, status_msg: str):\r\n        \"\"\"Report status to monitoring system\"\"\"\r\n        status = String()\r\n        status.data = status_msg\r\n        self.status_pub.publish(status)\r\n        rospy.loginfo(status_msg)\r\n\r\ndef main():\r\n    controller = NavigationManipulationController()\r\n    \r\n    try:\r\n        rospy.spin()\r\n    except KeyboardInterrupt:\r\n        rospy.loginfo(\"Shutting down navigation and manipulation controller...\")\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,o.jsx)(n.p,{children:"Here's the complete integrated system that combines all components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# complete_autonomous_humanoid_system.py\r\n\r\nimport rospy\r\nimport json\r\nimport threading\r\nimport time\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import Pose\r\nimport sys\r\n\r\n# Import all components\r\nfrom humanoid_system_orchestrator import HumanoidSystemOrchestrator\r\nfrom voice_to_action_pipeline import VoiceToActionPipeline\r\nfrom nav_manip_controller import NavigationManipulationController\r\n\r\nclass CompleteAutonomousHumanoidSystem:\r\n    """Complete autonomous humanoid system integrating all modules"""\r\n    \r\n    def __init__(self):\r\n        rospy.init_node(\'complete_autonomous_humanoid\', anonymous=True)\r\n        \r\n        # System state\r\n        self.system_active = False\r\n        self.api_key = None\r\n        self.components = {}\r\n        \r\n        # Publishers\r\n        self.system_status_pub = rospy.Publisher(\'/autonomous_humanoid_status\', String, queue_size=10)\r\n        self.user_feedback_pub = rospy.Publisher(\'/user_feedback\', String, queue_size=10)\r\n        \r\n        # Subscribers\r\n        rospy.Subscriber(\'/system_control\', String, self.system_control_callback)\r\n        \r\n        rospy.loginfo("Complete Autonomous Humanoid System initialized")\r\n    \r\n    def system_control_callback(self, msg: String):\r\n        """Handle system control commands"""\r\n        command = json.loads(msg.data)\r\n        action = command.get(\'action\')\r\n        \r\n        if action == \'start_system\':\r\n            self.start_system()\r\n        elif action == \'stop_system\':\r\n            self.stop_system()\r\n        elif action == \'configure_api_key\':\r\n            self.api_key = command.get(\'api_key\')\r\n            self.configure_components()\r\n        elif action == \'execute_demo\':\r\n            self.execute_demo_scenario()\r\n    \r\n    def configure_components(self):\r\n        """Configure all system components with API key"""\r\n        if not self.api_key:\r\n            rospy.logerr("No API key available for configuration")\r\n            return\r\n        \r\n        # Configure orchestrator\r\n        if \'orchestrator\' in self.components:\r\n            self.components[\'orchestrator\'].setup_llm_integration(self.api_key)\r\n        \r\n        # Configure voice pipeline\r\n        if \'voice_pipeline\' in self.components:\r\n            # The voice pipeline would need to be recreated or reconfigured\r\n            pass\r\n    \r\n    def start_system(self):\r\n        """Start the complete autonomous humanoid system"""\r\n        if self.system_active:\r\n            rospy.logwarn("System already active")\r\n            return\r\n        \r\n        rospy.loginfo("Starting complete autonomous humanoid system...")\r\n        \r\n        # Initialize API key if not already set\r\n        if not self.api_key:\r\n            self.api_key = input("Enter OpenAI API key: ")\r\n        \r\n        if not self.api_key:\r\n            rospy.logerr("No API key provided, cannot start system")\r\n            return\r\n        \r\n        # Initialize all components\r\n        try:\r\n            # 1. Initialize the orchestrator\r\n            orchestrator = HumanoidSystemOrchestrator()\r\n            orchestrator.setup_llm_integration(self.api_key)\r\n            self.components[\'orchestrator\'] = orchestrator\r\n            \r\n            # 2. Initialize voicetoaction pipeline\r\n            voice_pipeline = VoiceToActionPipeline(self.api_key)\r\n            self.components[\'voice_pipeline\'] = voice_pipeline\r\n            \r\n            # 3. Initialize navmanip controller\r\n            nav_manip_controller = NavigationManipulationController()\r\n            self.components[\'nav_manip_controller\'] = nav_manip_controller\r\n            \r\n            # Start components in separate threads\r\n            self.execution_thread = threading.Thread(\r\n                target=self.run_execution_loop, \r\n                args=(orchestrator,)\r\n            )\r\n            self.execution_thread.daemon = True\r\n            self.execution_thread.start()\r\n            \r\n            # Start voice pipeline\r\n            voice_thread = threading.Thread(\r\n                target=self.run_voice_pipeline, \r\n                args=(voice_pipeline,)\r\n            )\r\n            voice_thread.daemon = True\r\n            voice_thread.start()\r\n            \r\n            self.system_active = True\r\n            status_msg = String()\r\n            status_msg.data = "System started successfully"\r\n            self.system_status_pub.publish(status_msg)\r\n            rospy.loginfo("All system components initialized and running")\r\n            \r\n        except Exception as e:\r\n            rospy.logerr(f"Error starting system: {e}")\r\n            import traceback\r\n            traceback.print_exc()\r\n    \r\n    def stop_system(self):\r\n        """Stop the complete system"""\r\n        rospy.loginfo("Stopping complete autonomous humanoid system...")\r\n        \r\n        self.system_active = False\r\n        \r\n        # Stop voice pipeline\r\n        if \'voice_pipeline\' in self.components:\r\n            self.components[\'voice_pipeline\'].stop_listening()\r\n        \r\n        status_msg = String()\r\n        status_msg.data = "System stopped"\r\n        self.system_status_pub.publish(status_msg)\r\n    \r\n    def run_execution_loop(self, orchestrator):\r\n        """Run the orchestrator execution loop"""\r\n        try:\r\n            orchestrator.run_execution_loop()\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in execution loop: {e}")\r\n    \r\n    def run_voice_pipeline(self, voice_pipeline):\r\n        """Run the voice pipeline"""\r\n        try:\r\n            voice_pipeline.start_listening()\r\n        except Exception as e:\r\n            rospy.logerr(f"Error in voice pipeline: {e}")\r\n    \r\n    def execute_demo_scenario(self):\r\n        """Execute a demo scenario"""\r\n        if not self.system_active:\r\n            rospy.logerr("System not active, cannot execute demo")\r\n            return\r\n        \r\n        rospy.loginfo("Starting demo scenario...")\r\n        \r\n        # Example sequence of highlevel commands\r\n        demo_commands = [\r\n            "Navigate to the kitchen",\r\n            "Approach the table",\r\n            "Pick up the red cup",\r\n            "Place the cup in the sink",\r\n            "Return to the living room"\r\n        ]\r\n        \r\n        # Publish commands to the system\r\n        voice_cmd_pub = rospy.Publisher(\'/voice_command\', String, queue_size=10)\r\n        \r\n        for i, command in enumerate(demo_commands):\r\n            rospy.loginfo(f"Executing demo command {i+1}/5: {command}")\r\n            \r\n            # Publish command\r\n            cmd_msg = String()\r\n            cmd_msg.data = command\r\n            voice_cmd_pub.publish(cmd_msg)\r\n            \r\n            # Wait between commands\r\n            time.sleep(5)\r\n        \r\n        rospy.loginfo("Demo scenario completed")\r\n        feedback_msg = String()\r\n        feedback_msg.data = "Demo scenario completed"\r\n        self.user_feedback_pub.publish(feedback_msg)\r\n    \r\n    def run(self):\r\n        """Main run loop"""\r\n        rospy.loginfo("Complete Autonomous Humanoid System running...")\r\n        \r\n        # For demo purposes, start the system automatically\r\n        self.start_system()\r\n        \r\n        try:\r\n            rospy.spin()\r\n        except KeyboardInterrupt:\r\n            rospy.loginfo("Shutting down complete autonomous humanoid system...")\r\n            self.stop_system()\r\n\r\ndef main():\r\n    """Main function to run the complete system"""\r\n    system = CompleteAutonomousHumanoidSystem()\r\n    system.run()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"launch-file-for-the-complete-system",children:"Launch file for the complete system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<launch>\r\n  <! Complete Autonomous Humanoid System >\r\n  <node name="complete_autonomous_humanoid" pkg="humanoid_robot" type="complete_autonomous_humanoid_system.py" output="screen">\r\n  </node>\r\n  \r\n  <! VoicetoAction Pipeline >\r\n  <node name="voice_to_action_pipeline" pkg="humanoid_robot" type="voice_to_action_pipeline.py" output="screen">\r\n  </node>\r\n  \r\n  <! Navigation and Manipulation Controller >\r\n  <node name="nav_manip_controller" pkg="humanoid_robot" type="nav_manip_controller.py" output="screen">\r\n  </node>\r\n  \r\n  <! Perception System >\r\n  <node name="perception_pipeline" pkg="perception" type="perception_pipeline.py" output="screen">\r\n  </node>\r\n  \r\n  <! Robot Hardware Interface >\r\n  <node name="hardware_interface" pkg="ros_control" type="robot_hw_interface.py" output="screen">\r\n  </node>\r\n  \r\n  <! MoveIt! Configuration >\r\n  <include file="$(find my_robot_moveit_config)/launch/move_group.launch"/>\r\n  <include file="$(find my_robot_moveit_config)/launch/moveit_rviz.launch"/>\r\n\r\n  <! TF Tree >\r\n  <node name="robot_state_publisher" pkg="robot_state_publisher" type="robot_state_publisher" />\r\n  \r\n  <! Example sensor drivers >\r\n  <group ns="sensors">\r\n    <node name="camera_driver" pkg="usb_cam" type="usb_cam_node" output="screen">\r\n      <param name="video_device" value="/dev/video0"/>\r\n      <param name="image_width" value="640"/>\r\n      <param name="image_height" value="480"/>\r\n      <param name="pixel_format" value="yuyv"/>\r\n    </node>\r\n    \r\n    <node name="lidar_driver" pkg="velodyne_driver" type="velodyne_node" output="screen">\r\n      <param name="device_ip" value="192.168.1.201"/>\r\n      <param name="port" value="2368"/>\r\n    </node>\r\n    \r\n    <node name="imu_driver" pkg="razor_imu_9dof" type="imu_node" output="screen"/>\r\n  </group>\r\n  \r\n  <! Static transforms for the humanoid >\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.0 0.0 0.0 0.0 0.0 0.0 base_link torso" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.0 0.2 1.0 0.0 0.0 0.0 torso head" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.3 0.0 0.8 0.0 0.0 0.0 torso left_shoulder" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.3 0.0 0.8 0.0 0.0 0.0 torso right_shoulder" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.0 0.1 0.8 0.0 0.0 0.0 torso pelvis" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.15 0.1 1.0 0.0 0.0 0.0 pelvis left_hip" />\r\n  <node name="static_transform_publisher" pkg="tf2_ros" type="static_transform_publisher" \r\n        args="0.15 0.1 1.0 0.0 0.0 0.0 pelvis right_hip" />\r\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete autonomous humanoid system that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Integrates voice recognition and natural language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Implements cognitive task planning with GPT4o"}),"\n",(0,o.jsx)(n.li,{children:"Executes navigation and manipulation tasks"}),"\n",(0,o.jsx)(n.li,{children:"Incorporates multimodal perception fusion"}),"\n",(0,o.jsx)(n.li,{children:"Demonstrates complete voice \u2192 plan \u2192 navigate \u2192 manipulate pipeline"}),"\n",(0,o.jsx)(n.li,{children:"Evaluates system performance and robustness"}),"\n",(0,o.jsx)(n.li,{children:"Documents the full integration and its challenges"}),"\n",(0,o.jsx)(n.li,{children:"Creates a userfriendly interface for command input"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Your project should include:\r\nComplete integration of all four modules\r\nWorking voicetoaction pipeline\r\nCognitive planning and task execution\r\nNavigation and manipulation capabilities\r\nMultimodal perception system\r\nPerformance evaluation and error handling\r\nDemo scenarios showing the complete pipeline\r\nDetailed documentation of integration challenges"}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter served as the capstone project integrating all modules:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Module 1 (ROS 2)"}),": Used for system architecture and communication between components\r\n",(0,o.jsx)(n.strong,{children:"Module 2 (Digital Twins)"}),": Applied for simulation and validation before realworld deployment\r\n",(0,o.jsx)(n.strong,{children:"Module 3 (Isaac Platform)"}),": Provided AIpowered perception and control systems\r\n",(0,o.jsx)(n.strong,{children:"Module 4 (VisionLanguageAction)"}),": Enabled the voicetoaction pipeline"]}),"\n",(0,o.jsx)(n.p,{children:"The complete autonomous humanoid system demonstrates:\r\nVoice recognition and natural language understanding\r\nCognitive task planning using LLMs\r\nNavigation and manipulation capabilities\r\nMultimodal perception fusion\r\nEndtoend integration from voice input to physical action"}),"\n",(0,o.jsx)(n.p,{children:"This capstone project showcases the integration of all technologies learned throughout the course, creating a sophisticated autonomous humanoid robot capable of understanding and executing complex voice commands in realworld environments."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}}}]);