"use strict";(globalThis.webpackChunkai_native_robotics_textbook=globalThis.webpackChunkai_native_robotics_textbook||[]).push([[7580],{2257:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>m,contentTitle:()=>l,default:()=>u,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-2-digital-twin/ch8-simulating-sensors","title":"ch8-simulating-sensors","description":"-----","source":"@site/docs/module-2-digital-twin/ch8-simulating-sensors.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/ch8-simulating-sensors","permalink":"/physical-ai-textbook-2025/docs/module-2-digital-twin/ch8-simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/Tayyaba10/physical-ai-textbook-2025/edit/main/docs/module-2-digital-twin/ch8-simulating-sensors.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"ch7-gazebo-ignition-setup-building","permalink":"/physical-ai-textbook-2025/docs/module-2-digital-twin/ch7-gazebo-ignition-setup-building"},"next":{"title":"ch9-unity-hdrp-visualization","permalink":"/physical-ai-textbook-2025/docs/module-2-digital-twin/ch9-unity-hdrp-visualization"}}');var s=e(4848),a=e(8453),o=e(7242);const t={},l=void 0,m={},c=[{value:"title: Ch8  Simulating LiDAR, Depth Cameras &amp; IMUs\r\nmodule: 2\r\nchapter: 8\r\nsidebar_label: Ch8: Simulating LiDAR, Depth Cameras &amp; IMUs\r\ndescription: Setting up and using sensor simulation in Gazebo for robotics applications\r\ntags: [lidar, camera, imus, sensors, simulation, perception, robotics]\r\ndifficulty: intermediate\r\nestimated_duration: 90",id:"title-ch8--simulating-lidar-depth-cameras--imusmodule-2chapter-8sidebar_label-ch8-simulating-lidar-depth-cameras--imusdescription-setting-up-and-using-sensor-simulation-in-gazebo-for-robotics-applicationstags-lidar-camera-imus-sensors-simulation-perception-roboticsdifficulty-intermediateestimated_duration-90",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Theory",id:"theory",level:2},{value:"LiDAR Sensors in Simulation",id:"lidar-sensors-in-simulation",level:3},{value:"Depth Cameras in Simulation",id:"depth-cameras-in-simulation",level:3},{value:"IMU Sensors in Simulation",id:"imu-sensors-in-simulation",level:3},{value:"StepbyStep Labs",id:"stepbystep-labs",level:2},{value:"Lab 1: Configuring LiDAR Sensors in Gazebo",id:"lab-1-configuring-lidar-sensors-in-gazebo",level:3},{value:"Lab 2: Configuring Depth Cameras in Gazebo",id:"lab-2-configuring-depth-cameras-in-gazebo",level:3},{value:"Lab 3: Configuring IMU Sensors in Gazebo",id:"lab-3-configuring-imu-sensors-in-gazebo",level:3},{value:"Lab 4: Integrating Sensors with ROS 2",id:"lab-4-integrating-sensors-with-ros-2",level:3},{value:"Runnable Code Example",id:"runnable-code-example",level:2},{value:"Processing LiDAR data for mapping (<code>lidar_processing.py</code>):",id:"processing-lidar-data-for-mapping-lidar_processingpy",level:3},{value:"Miniproject",id:"miniproject",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"title-ch8--simulating-lidar-depth-cameras--imusmodule-2chapter-8sidebar_label-ch8-simulating-lidar-depth-cameras--imusdescription-setting-up-and-using-sensor-simulation-in-gazebo-for-robotics-applicationstags-lidar-camera-imus-sensors-simulation-perception-roboticsdifficulty-intermediateestimated_duration-90",children:"title: Ch8  Simulating LiDAR, Depth Cameras & IMUs\r\nmodule: 2\r\nchapter: 8\r\nsidebar_label: Ch8: Simulating LiDAR, Depth Cameras & IMUs\r\ndescription: Setting up and using sensor simulation in Gazebo for robotics applications\r\ntags: [lidar, camera, imus, sensors, simulation, perception, robotics]\r\ndifficulty: intermediate\r\nestimated_duration: 90"}),"\n","\n",(0,s.jsx)(r.h1,{id:"simulating-lidar-depth-cameras--imus",children:"Simulating LiDAR, Depth Cameras & IMUs"}),"\n",(0,s.jsx)(r.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(r.p,{children:"Understand the principles behind LiDAR, depth camera, and IMU sensors\r\nConfigure and implement these sensors in Gazebo simulation\r\nIntegrate sensor data with ROS 2 for robotics applications\r\nProcess and interpret sensor data for perception tasks\r\nCalibrate sensor models to match realworld performance\r\nImplement sensor fusion techniques for enhanced perception\r\nTroubleshoot common issues in sensor simulation"}),"\n",(0,s.jsx)(r.h2,{id:"theory",children:"Theory"}),"\n",(0,s.jsx)(r.h3,{id:"lidar-sensors-in-simulation",children:"LiDAR Sensors in Simulation"}),"\n",(0,s.jsx)(r.p,{children:"LiDAR (Light Detection and Ranging) sensors emit laser pulses and measure the time it takes for the light to return after reflecting off objects. In simulation, LiDAR sensors work by casting rays into the environment and detecting collisions with objects."}),"\n",(0,s.jsx)(o.A,{chart:"\ngraph TD;\n  A[LiDAR Sensor] > B[Ray Casting];\n  B > C[Range Measurement];\n  C > D[Point Cloud];\n  D > E[Object Detection];\n  E > F[Mapping];\n  \n  G[LiDAR Parameters] > H[Sample Count];\n  G > I[Field of View];\n  G > J[Range Min/Max];\n  G > K[Update Rate];\n  \n  style A fill:#4CAF50,stroke:#388E3C,color:#fff;\n  style D fill:#2196F3,stroke:#0D47A1,color:#fff;\n"}),"\n",(0,s.jsx)(r.h3,{id:"depth-cameras-in-simulation",children:"Depth Cameras in Simulation"}),"\n",(0,s.jsx)(r.p,{children:"Depth cameras, such as RGBD sensors, provide both color images and depth information. In simulation, these are typically implemented using stereo vision principles or structured light methods. The depth information is calculated by measuring the distance to objects in the scene."}),"\n",(0,s.jsx)(r.h3,{id:"imu-sensors-in-simulation",children:"IMU Sensors in Simulation"}),"\n",(0,s.jsx)(r.p,{children:"Inertial Measurement Units (IMUs) measure linear acceleration and angular velocity. In simulation, IMUs are typically modeled using:\r\nNoise models to simulate realworld sensor inaccuracies\r\nBias terms that drift over time\r\nSampling rate to match real hardware"}),"\n",(0,s.jsx)(r.h2,{id:"stepbystep-labs",children:"StepbyStep Labs"}),"\n",(0,s.jsx)(r.h3,{id:"lab-1-configuring-lidar-sensors-in-gazebo",children:"Lab 1: Configuring LiDAR Sensors in Gazebo"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create a robot model with LiDAR"})," (",(0,s.jsx)(r.code,{children:"lidar_robot.sdf"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <model name="lidar_robot">\r\n    <link name="chassis">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>1.0</mass>\r\n        <inertia>\r\n          <ixx>0.01</ixx>\r\n          <iyy>0.01</iyy>\r\n          <izz>0.02</izz>\r\n        </inertia>\r\n      </inertial>\r\n      <collision name="collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n      <visual name="visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <diffuse>0.1 0.1 0.8 1</diffuse>\r\n          <specular>0.5 0.5 0.5 1</specular>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <! LiDAR sensor >\r\n    <link name="lidar_link">\r\n      <pose>0.1 0 0.2 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.1</mass>\r\n        <inertia>\r\n          <ixx>0.001</ixx>\r\n          <iyy>0.001</iyy>\r\n          <izz>0.001</izz>\r\n        </inertia>\r\n      </inertial>\r\n      <collision name="collision">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.05</radius>\r\n            <length>0.05</length>\r\n          </cylinder>\r\n        </geometry>\r\n      </collision>\r\n      <visual name="visual">\r\n        <geometry>\r\n          <cylinder>\r\n            <radius>0.05</radius>\r\n            <length>0.05</length>\r\n          </cylinder>\r\n        </geometry>\r\n        <material>\r\n          <diffuse>0.8 0.1 0.1 1</diffuse>\r\n          <specular>0.8 0.8 0.8 1</specular>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <joint name="lidar_joint" type="fixed">\r\n      <parent>chassis</parent>\r\n      <child>lidar_link</child>\r\n      <pose>0.1 0 0.2 0 0 0</pose>\r\n    </joint>\r\n\r\n    <! 2D LiDAR sensor >\r\n    <sensor name="lidar_2d" type="ray">\r\n      <pose>0 0 0 0 0 0</pose>\r\n      <ray>\r\n        <scan>\r\n          <horizontal>\r\n            <samples>360</samples>\r\n            <resolution>1</resolution>\r\n            <min_angle>3.14159</min_angle>\r\n            <max_angle>3.14159</max_angle>\r\n          </horizontal>\r\n        </scan>\r\n        <range>\r\n          <min>0.1</min>\r\n          <max>10.0</max>\r\n          <resolution>0.01</resolution>\r\n        </range>\r\n      </ray>\r\n      <always_on>true</always_on>\r\n      <update_rate>10</update_rate>\r\n      <visualize>true</visualize>\r\n    </sensor>\r\n  </model>\r\n</sdf>\n'})}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create a world with obstacles for LiDAR testing"})," (",(0,s.jsx)(r.code,{children:"lidar_test_world.sdf"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <world name="lidar_test">\r\n    <physics type="ode">\r\n      <max_step_size>0.001</max_step_size>\r\n      <real_time_factor>1.0</real_time_factor>\r\n      <real_time_update_rate>1000.0</real_time_update_rate>\r\n    </physics>\r\n\r\n    <! Lighting >\r\n    <light name="sun" type="directional">\r\n      <cast_shadows>true</cast_shadows>\r\n      <pose>0 0 10 0 0 0</pose>\r\n      <diffuse>0.8 0.8 0.8 1</diffuse>\r\n      <specular>0.2 0.2 0.2 1</specular>\r\n      <direction>0.3 0.3 1</direction>\r\n    </light>\r\n\r\n    <! Ground plane >\r\n    <model name="ground_plane">\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <plane>\r\n              <normal>0 0 1</normal>\r\n              <size>20 20</size>\r\n            </plane>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <plane>\r\n              <normal>0 0 1</normal>\r\n              <size>20 20</size>\r\n            </plane>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.7 0.7 0.7 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <! Obstacles for LiDAR testing >\r\n    <model name="wall_1">\r\n      <pose>5 0 1 0 0 0</pose>\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <box>\r\n              <size>0.1 10 2</size>\r\n            </box>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <box>\r\n              <size>0.1 10 2</size>\r\n          </box>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.8 0.2 0.2 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <model name="obstacle_1">\r\n      <pose>2 2 0.5 0 0 0</pose>\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <cylinder>\r\n              <radius>0.3</radius>\r\n              <length>1</length>\r\n            </cylinder>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <cylinder>\r\n              <radius>0.3</radius>\r\n              <length>1</length>\r\n            </cylinder>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.2 0.8 0.2 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <model name="obstacle_2">\r\n      <pose>3 1 0.3 0 0 0</pose>\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <box>\r\n              <size>0.4 0.4 0.6</size>\r\n            </box>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <box>\r\n              <size>0.4 0.4 0.6</size>\r\n          </box>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.8 0.8 0.2 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <! Include the robot >\r\n    <include>\r\n      <uri>model://lidar_robot</uri>\r\n      <pose>0 0 0 0 0 0</pose>\r\n    </include>\r\n  </world>\r\n</sdf>\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"lab-2-configuring-depth-cameras-in-gazebo",children:"Lab 2: Configuring Depth Cameras in Gazebo"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Create a robot model with RGBD camera"})," (",(0,s.jsx)(r.code,{children:"rgbd_camera_robot.sdf"}),"):","\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <model name="rgbd_camera_robot">\r\n    <link name="chassis">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>1.0</mass>\r\n        <inertia>\r\n          <ixx>0.01</ixx>\r\n          <iyy>0.01</iyy>\r\n          <izz>0.02</izz>\r\n        </inertia>\r\n      </inertial>\r\n      <collision name="collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n      <visual name="visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <diffuse>0.1 0.8 0.1 1</diffuse>\r\n          <specular>0.5 0.5 0.5 1</specular>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <! RGBD camera >\r\n    <link name="camera_link">\r\n      <pose>0.15 0 0.2 0 0 0</pose>\r\n      <inertial>\r\n        <mass>0.05</mass>\r\n        <inertia>\r\n          <ixx>0.0001</ixx>\r\n          <iyy>0.0001</iyy>\r\n          <izz>0.0001</izz>\r\n        </inertia>\r\n      </inertial>\r\n      <collision name="collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.08 0.05</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n      <visual name="visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.05 0.08 0.05</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <diffuse>0.1 0.1 0.1 1</diffuse>\r\n          <specular>0.8 0.8 0.8 1</specular>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <joint name="camera_joint" type="fixed">\r\n      <parent>chassis</parent>\r\n      <child>camera_link</child>\r\n      <pose>0.15 0 0.2 0 0 0</pose>\r\n    </joint>\r\n\r\n    <! RGBD camera sensor >\r\n    <sensor name="rgbd_camera" type="rgbd_camera">\r\n      <pose>0 0 0 0 0 0</pose>\r\n      <camera>\r\n        <horizontal_fov>1.047</horizontal_fov> <! 60 degrees >\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.1</near>\r\n          <far>10.0</far>\r\n        </clip>\r\n      </camera>\r\n      <always_on>true</always_on>\r\n      <update_rate>15</update_rate>\r\n      <visualize>true</visualize>\r\n    </sensor>\r\n  </model>\r\n</sdf>\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"lab-3-configuring-imu-sensors-in-gazebo",children:"Lab 3: Configuring IMU Sensors in Gazebo"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create a robot model with IMU"})," (",(0,s.jsx)(r.code,{children:"imu_robot.sdf"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <model name="imu_robot">\r\n    <link name="chassis">\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n      <inertial>\r\n        <mass>1.0</mass>\r\n        <inertia>\r\n          <ixx>0.01</ixx>\r\n          <iyy>0.01</iyy>\r\n          <izz>0.02</izz>\r\n        </inertia>\r\n      </inertial>\r\n      <collision name="collision">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n      </collision>\r\n      <visual name="visual">\r\n        <geometry>\r\n          <box>\r\n            <size>0.5 0.3 0.2</size>\r\n          </box>\r\n        </geometry>\r\n        <material>\r\n          <diffuse>0.8 0.1 0.8 1</diffuse>\r\n          <specular>0.5 0.5 0.5 1</specular>\r\n        </material>\r\n      </visual>\r\n    </link>\r\n\r\n    <! IMU sensor >\r\n    <sensor name="imu_sensor" type="imu">\r\n      <pose>0 0 0 0 0 0</pose>\r\n      <topic>imu</topic>\r\n      <always_on>true</always_on>\r\n      <update_rate>100</update_rate>\r\n      <imu>\r\n        <angular_velocity>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>2e4</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>2e4</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>2e4</stddev>\r\n            </noise>\r\n          </z>\r\n        </angular_velocity>\r\n        <linear_acceleration>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>1.7e2</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>1.7e2</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>1.7e2</stddev>\r\n            </noise>\r\n          </z>\r\n        </linear_acceleration>\r\n      </imu>\r\n    </sensor>\r\n  </model>\r\n</sdf>\n'})}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create a world with IMU testing environment"})," (",(0,s.jsx)(r.code,{children:"imu_test_world.sdf"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:'<?xml version="1.0" ?>\r\n<sdf version="1.7">\r\n  <world name="imu_test">\r\n    <physics type="ode">\r\n      <max_step_size>0.001</max_step_size>\r\n      <real_time_factor>1.0</real_time_factor>\r\n      <real_time_update_rate>1000.0</real_time_update_rate>\r\n    </physics>\r\n\r\n    <! Lighting >\r\n    <light name="sun" type="directional">\r\n      <cast_shadows>true</cast_shadows>\r\n      <pose>0 0 10 0 0 0</pose>\r\n      <diffuse>0.8 0.8 0.8 1</diffuse>\r\n      <specular>0.2 0.2 0.2 1</specular>\r\n      <direction>0.3 0.3 1</direction>\r\n    </light>\r\n\r\n    <! Ground plane >\r\n    <model name="ground_plane">\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <plane>\r\n              <normal>0 0 1</normal>\r\n              <size>20 20</size>\r\n            </plane>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <plane>\r\n              <normal>0 0 1</normal>\r\n              <size>20 20</size>\r\n            </plane>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.7 0.7 0.7 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <! A ramp to test IMU on slopes >\r\n    <model name="ramp">\r\n      <pose>3 0 0.5 0 0 0</pose>\r\n      <static>true</static>\r\n      <link name="link">\r\n        <collision name="collision">\r\n          <geometry>\r\n            <mesh>\r\n              <scale>1 1 1</scale>\r\n              <uri>file://meshes/ramp.dae</uri>\r\n            </mesh>\r\n          </geometry>\r\n        </collision>\r\n        <visual name="visual">\r\n          <geometry>\r\n            <mesh>\r\n              <scale>1 1 1</scale>\r\n              <uri>file://meshes/ramp.dae</uri>\r\n            </mesh>\r\n          </geometry>\r\n          <material>\r\n            <diffuse>0.5 0.3 0.0 1</diffuse>\r\n            <specular>0.1 0.1 0.1 1</specular>\r\n          </material>\r\n        </visual>\r\n      </link>\r\n    </model>\r\n\r\n    <! Include the robot >\r\n    <include>\r\n      <uri>model://imu_robot</uri>\r\n      <pose>0 0 0.1 0 0 0</pose>\r\n    </include>\r\n  </world>\r\n</sdf>\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"lab-4-integrating-sensors-with-ros-2",children:"Lab 4: Integrating Sensors with ROS 2"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create a launch file for sensor simulation"})," (",(0,s.jsx)(r.code,{children:"launch/sensor_simulation.launch.py"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"import os\r\nfrom ament_index_python.packages import get_package_share_directory\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    # Get Gazebo ROS launch directory\r\n    gazebo_ros_share = get_package_share_directory('ros_gz_sim')\r\n    \r\n    # Declare launch arguments\r\n    world = DeclareLaunchArgument(\r\n        'world',\r\n        default_value=os.path.join(\r\n            get_package_share_directory('sensor_integration'),\r\n            'worlds',\r\n            'multi_sensor_test.sdf'\r\n        ),\r\n        description='World file to load in Gazebo'\r\n    )\r\n    \r\n    # Launch Gazebo\r\n    gazebo = IncludeLaunchDescription(\r\n        PythonLaunchDescriptionSource(\r\n            os.path.join(gazebo_ros_share, 'launch', 'gz_sim.launch.py')\r\n        ),\r\n        launch_arguments={\r\n            'gz_args': ['r ', LaunchConfiguration('world')]\r\n        }.items()\r\n    )\r\n    \r\n    # ROSGazebo bridge for sensors\r\n    bridge = Node(\r\n        package='ros_gz_bridge',\r\n        executable='parameter_bridge',\r\n        parameters=[{\r\n            'config_file': os.path.join(\r\n                get_package_share_directory('sensor_integration'),\r\n                'config',\r\n                'sensors_bridge.yaml'\r\n            )\r\n        }],\r\n        output='screen'\r\n    )\r\n    \r\n    # Create launch description\r\n    ld = LaunchDescription()\r\n    ld.add_action(world)\r\n    ld.add_action(gazebo)\r\n    ld.add_action(bridge)\r\n    \r\n    return ld\n"})}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"Create bridge configuration"})," (",(0,s.jsx)(r.code,{children:"config/sensors_bridge.yaml"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-yaml",children:'# Bridge configuration for sensors\r\n ros_topic_name: "/scan"\r\n  gz_topic_name: "/lidar_2d/scan"\r\n  ros_type_name: "sensor_msgs/msg/LaserScan"\r\n  gz_type_name: "gz.msgs.LaserScan"\r\n\r\n ros_topic_name: "/rgb_camera/image_raw"\r\n  gz_topic_name: "/rgbd_camera/image"\r\n  ros_type_name: "sensor_msgs/msg/Image"\r\n  gz_type_name: "gz.msgs.Image"\r\n\r\n ros_topic_name: "/rgb_camera/camera_info"\r\n  gz_topic_name: "/rgbd_camera/camera_info"\r\n  ros_type_name: "sensor_msgs/msg/CameraInfo"\r\n  gz_type_name: "gz.msgs.CameraInfo"\r\n\r\n ros_topic_name: "/depth_camera/image_raw"\r\n  gz_topic_name: "/rgbd_camera/depth_image"\r\n  ros_type_name: "sensor_msgs/msg/Image"\r\n  gz_type_name: "gz.msgs.Image"\r\n\r\n ros_topic_name: "/imu"\r\n  gz_topic_name: "/imu_sensor/imu"\r\n  ros_type_name: "sensor_msgs/msg/Imu"\r\n  gz_type_name: "gz.msgs.IMU"\n'})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"runnable-code-example",children:"Runnable Code Example"}),"\n",(0,s.jsx)(r.p,{children:"Here's a complete example of sensor processing in ROS 2:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, Imu\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nfrom tf_transformations import euler_from_quaternion\r\nimport math\r\n\r\nclass MultiSensorRobot(Node):\r\n    def __init__(self):\r\n        super().__init__('multi_sensor_robot')\r\n        \r\n        # Initialize CvBridge\r\n        self.bridge = CvBridge()\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        \r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.lidar_callback, 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/rgb_camera/image_raw', self.image_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, '/imu', self.imu_callback, 10)\r\n        \r\n        # Timer for control loop\r\n        self.timer = self.create_timer(0.1, self.control_loop)\r\n        \r\n        # Robot state\r\n        self.scan_data = None\r\n        self.image_data = None\r\n        self.imu_data = None\r\n        self.linear_vel = 0.3\r\n        self.angular_vel = 0.5\r\n        \r\n        self.get_logger().info('Multisensor robot node started')\r\n    \r\n    def lidar_callback(self, msg):\r\n        self.scan_data = msg\r\n    \r\n    def image_callback(self, msg):\r\n        try:\r\n            # Convert ROS Image message to OpenCV image\r\n            self.image_data = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error converting image: {e}')\r\n    \r\n    def imu_callback(self, msg):\r\n        self.imu_data = msg\r\n    \r\n    def detect_objects_in_image(self, img):\r\n        \"\"\"Simple object detection using color thresholding\"\"\"\r\n        if img is None:\r\n            return []\r\n        \r\n        # Convert BGR to HSV\r\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\r\n        \r\n        # Define range for red color\r\n        lower_red1 = np.array([0, 50, 50])\r\n        upper_red1 = np.array([10, 255, 255])\r\n        lower_red2 = np.array([170, 50, 50])\r\n        upper_red2 = np.array([180, 255, 255])\r\n        \r\n        # Create masks\r\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\r\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\r\n        mask = mask1 + mask2\r\n        \r\n        # Find contours\r\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n        \r\n        objects = []\r\n        for contour in contours:\r\n            area = cv2.contourArea(contour)\r\n            if area > 500:  # Filter small areas\r\n                # Calculate bounding box\r\n                x, y, w, h = cv2.boundingRect(contour)\r\n                objects.append((x, y, w, h, area))\r\n        \r\n        return objects\r\n    \r\n    def get_orientation_from_imu(self):\r\n        \"\"\"Extract orientation from IMU data\"\"\"\r\n        if self.imu_data is None:\r\n            return 0.0\r\n        \r\n        # Extract orientation quaternion\r\n        orientation = self.imu_data.orientation\r\n        quaternion = [orientation.x, orientation.y, orientation.z, orientation.w]\r\n        \r\n        # Convert to Euler angles\r\n        euler = euler_from_quaternion(quaternion)\r\n        \r\n        return euler[2]  # Return yaw/heading\r\n    \r\n    def control_loop(self):\r\n        cmd = Twist()\r\n        \r\n        # Process LiDAR data for obstacle avoidance\r\n        if self.scan_data is not None:\r\n            # Get frontfacing ranges (\xb130 degrees)\r\n            front_ranges = self.scan_data.ranges[\r\n                len(self.scan_data.ranges)//2  30 : \r\n                len(self.scan_data.ranges)//2 + 30]\r\n            \r\n            # Filter out invalid values\r\n            front_ranges = [r for r in front_ranges if not (math.isinf(r) or math.isnan(r))]\r\n            \r\n            if front_ranges:\r\n                min_distance = min(front_ranges)\r\n                \r\n                # Check if we need to avoid obstacles\r\n                if min_distance < 0.8:\r\n                    cmd.linear.x = 0.0\r\n                    cmd.angular.z = self.angular_vel\r\n                    self.get_logger().info(f'Obstacle detected at {min_distance:.2f}m, turning...')\r\n                else:\r\n                    # Process camera data for navigation\r\n                    if self.image_data is not None:\r\n                        objects = self.detect_objects_in_image(self.image_data)\r\n                        if objects:\r\n                            # If objects detected, turn away from the largest one\r\n                            largest_obj = max(objects, key=lambda x: x[4])  # Sort by area\r\n                            (x, y, w, h, area) = largest_obj\r\n                            \r\n                            # Determine turn direction based on object position\r\n                            center_x = x + w/2\r\n                            image_center = self.image_data.shape[1] / 2\r\n                            \r\n                            if center_x < image_center:\r\n                                cmd.angular.z = 0.3  # Turn right\r\n                            else:\r\n                                cmd.angular.z = 0.3   # Turn left\r\n                            \r\n                            cmd.linear.x = 0.0  # Stop moving forward when detecting objects\r\n                            self.get_logger().info(f'Detected object, turning to avoid')\r\n                        else:\r\n                            # No objects detected, move forward\r\n                            cmd.linear.x = self.linear_vel\r\n                            cmd.angular.z = 0.0\r\n                    else:\r\n                        # No camera data, move forward if no obstacles\r\n                        cmd.linear.x = self.linear_vel\r\n                        cmd.angular.z = 0.0\r\n            else:\r\n                # If no valid readings, stop\r\n                cmd.linear.x = 0.0\r\n                cmd.angular.z = 0.0\r\n        else:\r\n            # If no scan data yet, don't move\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n        \r\n        self.cmd_vel_pub.publish(cmd)\r\n        \r\n        # Log IMU data\r\n        if self.imu_data is not None:\r\n            orientation = self.get_orientation_from_imu()\r\n            self.get_logger().info(f'Robot orientation: {orientation:.2f} rad')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    robot = MultiSensorRobot()\r\n    \r\n    try:\r\n        rclpy.spin(robot)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        robot.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsxs)(r.h3,{id:"processing-lidar-data-for-mapping-lidar_processingpy",children:["Processing LiDAR data for mapping (",(0,s.jsx)(r.code,{children:"lidar_processing.py"}),"):"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom nav_msgs.msg import OccupancyGrid\r\nfrom geometry_msgs.msg import Pose, Point\r\nimport numpy as np\r\nimport math\r\n\r\nclass LidarMapper(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_mapper')\r\n        \r\n        # Parameters\r\n        self.declare_parameter('map_resolution', 0.1)  # meters per cell\r\n        self.declare_parameter('map_width', 20.0)      # meters\r\n        self.declare_parameter('map_height', 20.0)     # meters\r\n        \r\n        self.map_resolution = self.get_parameter('map_resolution').value\r\n        self.map_width = self.get_parameter('map_width').value\r\n        self.map_height = self.get_parameter('map_height').value\r\n        \r\n        # Calculate map dimensions in cells\r\n        self.map_width_cells = int(self.map_width / self.map_resolution)\r\n        self.map_height_cells = int(self.map_height / self.map_resolution)\r\n        \r\n        # Initialize occupancy grid\r\n        self.occupancy_map = np.full((self.map_height_cells, self.map_width_cells), 1, dtype=np.int8)  # 1 = unknown\r\n        \r\n        # Subscribers and publishers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.scan_callback, 10)\r\n        \r\n        self.map_pub = self.create_publisher(\r\n            OccupancyGrid, '/map', 10)\r\n        \r\n        # Timer to publish map\r\n        self.map_timer = self.create_timer(1.0, self.publish_map)\r\n        \r\n        self.get_logger().info('Lidar Mapper initialized')\r\n    \r\n    def scan_callback(self, msg):\r\n        \"\"\"Process incoming LiDAR scan and update occupancy grid\"\"\"\r\n        if len(msg.ranges) == 0:\r\n            return\r\n        \r\n        # Robot position in map coordinates (assuming robot is at center)\r\n        robot_x = self.map_width_cells // 2\r\n        robot_y = self.map_height_cells // 2\r\n        \r\n        # Process each range measurement\r\n        for i, range_val in enumerate(msg.ranges):\r\n            if math.isinf(range_val) or math.isnan(range_val) or range_val > msg.range_max:\r\n                continue\r\n            \r\n            # Calculate angle of this measurement\r\n            angle = msg.angle_min + i * msg.angle_increment\r\n            \r\n            # Calculate end point of this ray\r\n            end_x = robot_x + int((range_val * math.cos(angle)) / self.map_resolution)\r\n            end_y = robot_y + int((range_val * math.sin(angle)) / self.map_resolution)\r\n            \r\n            # Perform ray tracing to mark free space\r\n            self.trace_ray(robot_x, robot_y, end_x, end_y)\r\n            \r\n            # Mark endpoint as occupied (if it's within sensor range)\r\n            if range_val < msg.range_max * 0.9:  # Only mark as occupied if it's a real obstacle\r\n                if 0 <= end_x < self.map_width_cells and 0 <= end_y < self.map_height_cells:\r\n                    self.occupancy_map[end_y, end_x] = 100  # Occupied (100%)\r\n    \r\n    def trace_ray(self, start_x, start_y, end_x, end_y):\r\n        \"\"\"Ray tracing algorithm to mark free space\"\"\"\r\n        # Bresenham's line algorithm\r\n        dx = abs(end_x  start_x)\r\n        dy = abs(end_y  start_y)\r\n        sx = 1 if start_x < end_x else 1\r\n        sy = 1 if start_y < end_y else 1\r\n        err = dx  dy\r\n        \r\n        x, y = start_x, start_y\r\n        \r\n        while True:\r\n            # Check bounds\r\n            if not (0 <= x < self.map_width_cells and 0 <= y < self.map_height_cells):\r\n                break\r\n                \r\n            # Mark as free space, but only if it's not already marked as occupied\r\n            if self.occupancy_map[y, x] != 100:\r\n                self.occupancy_map[y, x] = 0  # Free space (0%)\r\n                \r\n            if x == end_x and y == end_y:\r\n                break\r\n                \r\n            e2 = 2 * err\r\n            if e2 > dy:\r\n                err = dy\r\n                x += sx\r\n            if e2 < dx:\r\n                err += dx\r\n                y += sy\r\n    \r\n    def publish_map(self):\r\n        \"\"\"Publish the occupancy grid map\"\"\"\r\n        msg = OccupancyGrid()\r\n        \r\n        # Set header\r\n        msg.header.stamp = self.get_clock().now().to_msg()\r\n        msg.header.frame_id = 'map'\r\n        \r\n        # Set map info\r\n        msg.info.resolution = self.map_resolution\r\n        msg.info.width = self.map_width_cells\r\n        msg.info.height = self.map_height_cells\r\n        \r\n        # Set origin (assuming map center is at (0,0,0))\r\n        msg.info.origin.position.x = self.map_width / 2.0\r\n        msg.info.origin.position.y = self.map_height / 2.0\r\n        msg.info.origin.position.z = 0.0\r\n        msg.info.origin.orientation.w = 1.0\r\n        \r\n        # Flatten the occupancy map for the message\r\n        msg.data = self.occupancy_map.flatten().tolist()\r\n        \r\n        self.map_pub.publish(msg)\r\n        \r\n        self.get_logger().info(f'Published map: {self.map_width_cells}x{self.map_height_cells} cells')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    mapper = LidarMapper()\r\n    \r\n    try:\r\n        rclpy.spin(mapper)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        mapper.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(r.h2,{id:"miniproject",children:"Miniproject"}),"\n",(0,s.jsx)(r.p,{children:"Create a complete sensor fusion system that:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Simulates a robot with LiDAR, RGBD camera, and IMU sensors"}),"\n",(0,s.jsx)(r.li,{children:"Processes data from all sensors simultaneously"}),"\n",(0,s.jsx)(r.li,{children:"Implements sensor fusion for improved perception"}),"\n",(0,s.jsx)(r.li,{children:"Creates a map of the environment using LiDAR data"}),"\n",(0,s.jsx)(r.li,{children:"Localizes the robot in the map using IMU data"}),"\n",(0,s.jsx)(r.li,{children:"Uses camera data to identify and track objects"}),"\n",(0,s.jsx)(r.li,{children:"Implements obstacle avoidance using fused sensor data"}),"\n",(0,s.jsx)(r.li,{children:"Provides visualization of the fused sensor information"}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"Your system should include:\r\nComplete Gazebo world with features for sensor testing\r\nRobot model with all three sensor types\r\nROS 2 nodes for sensor processing and fusion\r\nLaunch file to start the complete system\r\nPerformance metrics and logging"}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(r.p,{children:"This chapter covered the simulation, integration, and processing of key robotic sensors:"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"LiDAR Simulation"}),": Configuring 2D and 3D laser scanners in Gazebo with proper parameters\r\n",(0,s.jsx)(r.strong,{children:"Depth Camera Simulation"}),": Setting up RGBD cameras with realistic image and depth data\r\n",(0,s.jsx)(r.strong,{children:"IMU Simulation"}),": Creating inertial measurement units with noise models matching real sensors\r\n",(0,s.jsx)(r.strong,{children:"ROS 2 Integration"}),": Connecting simulated sensors to the ROS 2 ecosystem\r\n",(0,s.jsx)(r.strong,{children:"Sensor Processing"}),": Techniques for processing and interpreting sensor data\r\n",(0,s.jsx)(r.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors for enhanced perception"]}),"\n",(0,s.jsx)(r.p,{children:"These sensors form the foundation of robot perception systems, and their simulation is crucial for developing and testing robotics algorithms in safe, controlled environments before deployment on real hardware."})]})}function u(n={}){const{wrapper:r}={...(0,a.R)(),...n.components};return r?(0,s.jsx)(r,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);