---
title: Ch15 - Reinforcement Learning & Sim-to-Real
module: 3
chapter: 15
sidebar_label: Ch15: Reinforcement Learning & Sim-to-Real
description: Implementing reinforcement learning algorithms for robotics and transferring policies from simulation to reality
tags: [reinforcement-learning, rl, sim-to-real, transfer-learning, robotics, Isaac-gym, Isaac-orbit, domain-randomization]
difficulty: advanced
estimated_duration: 150
---

import MermaidDiagram from '@site/src/components/MermaidDiagram';

# Reinforcement Learning & Sim-to-Real

## Learning Outcomes
- Understand reinforcement learning applications in robotics
- Implement RL algorithms for continuous control tasks
- Apply domain randomization techniques for sim-to-real transfer
- Evaluate policy robustness across simulation-to-reality gap
- Implement system identification and dynamics randomization
- Create robust control policies for physical systems
- Assess the effectiveness of sim-to-real transfer methods

## Theory

### Reinforcement Learning in Robotics

Reinforcement Learning (RL) is particularly applicable to robotics because robots can continuously interact with their environment and learn from trial and error. In robotics, RL agents learn to perform tasks by taking actions in an environment and receiving rewards based on their performance.

<MermaidDiagram chart={`
graph TD;
    A[Robot RL Agent] --> B[Observation Space];
    A --> C[Action Space];
    A --> D[Reward Function];
    
    B --> E[Camera Images];
    B --> F[Joint States];
    B --> G[IMU Readings];
    B --> H[Force Sensors];
    
    C --> I[Joint Efforts];
    C --> J[Motor Commands];
    C --> K[End-effector Poses];
    
    D --> L[Task Completion];
    D --> M[Efficiency];
    D --> N[Safety];
    D --> O[Stability];
    
    P[Environment] --> Q[Physics Simulation];
    P --> R[Real World];
    P --> S[Domain Randomization];
    
    style A fill:#4CAF50,stroke:#388E3C,color:#fff;
    style P fill:#2196F3,stroke:#0D47A1,color:#fff;
`} />

### Types of RL Algorithms for Robotics

**Deep Deterministic Policy Gradient (DDPG)**: Actor-critic method for continuous action spaces.

**Soft Actor-Critic (SAC)**: Maximum entropy RL algorithm that balances exploration and exploitation.

**Proximal Policy Optimization (PPO)**: Policy gradient method that clips gradients to prevent large policy updates.

**Twin Delayed DDPG (TD3)**: Improved version of DDPG that addresses overestimation bias.

### Sim-to-Real Transfer Challenges

The "reality gap" refers to differences between simulation and the real world that can prevent policies trained in simulation from working on real robots:

- **Dynamics Mismatch**: Differences in friction, motor delays, actuator responses
- **Sensor Noise**: Real sensors have different noise characteristics
- **Model Imperfections**: Uncertainty in robot and environment models
- **Environmental Factors**: Lighting, texture, and physical properties

### Domain Randomization

Techniques to make policies robust to simulation imperfections:

- **Dynamics Randomization**: Varying physical parameters randomly
- **Visual Domain Randomization**: Changing textures, colors, lighting
- **Control Randomization**: Adding delays, noise to control signals

## Step-by-Step Labs

### Lab 1: Setting up Isaac Gym for RL Training

1. **Install Isaac Gym Preview 4** (the last preview version, or Isaac Orbit for newer implementations):
   ```bash
   # Isaac Gym is part of the Isaac Extensions
   # For this example, we'll assume Isaac Orbit or similar environment
   
   # Create virtual environment
   python -m venv ~/isaac_rl_env
   source ~/isaac_rl_env/bin/activate
   pip install torch torchvision
   pip install gymnasium
   pip install stable-baselines3[extra]
   pip install sb3-contrib
   ```

2. **Create a basic RL environment using Isaac Sim as physics backend**:
   ```python
   # robot_rl_env.py
   import gymnasium as gym
   from gymnasium import spaces
   import numpy as np
   import torch
   import omni
   from pxr import Gf, UsdGeom
   import carb
   
   class RobotRLEnv(gym.Env):
       """Custom Robot RL Environment that wraps Isaac Sim for training"""
       
       def __init__(self, headless=True):
           super(RobotRLEnv, self).__init__()
           
           # Define action and observation space
           # Continuous action space for joint torques
           self.action_space = spaces.Box(
               low=-1.0, high=1.0, shape=(6,), dtype=np.float32  # 6 joints
           )
           
           # Observation space: joint positions, velocities, and IMU readings
           self.observation_space = spaces.Box(
               low=-np.inf, high=np.inf, shape=(18,), dtype=np.float32  # 6 pos + 6 vel + 6 IMU
           )
           
           # Environment parameters
           self.target_pos = np.array([1.0, 0.0, 0.0])  # Target position
           self.max_episode_steps = 1000
           self.current_step = 0
           
           # Initialize Isaac Sim components
           self.headless = headless
           self.reset()
           
       def reset(self, seed=None, options=None):
           """Reset the environment to an initial state"""
           super().reset(seed=seed)
           
           # Reset robot position in Isaac Sim
           # In practice, this would involve resetting the Isaac Sim scene
           self.robot_pos = np.array([0.0, 0.0, 0.0])
           self.robot_vel = np.array([0.0, 0.0, 0.0])
           self.joint_positions = np.zeros(6)
           self.joint_velocities = np.zeros(6)
           self.imu_readings = np.zeros(6)  # [acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z]
           
           self.current_step = 0
           
           # Return initial observation
           obs = self._get_observation()
           info = {}  # Additional info dictionary
           
           return obs, info
       
       def step(self, action):
           """Execute one time step within the environment"""
           # Apply action to robot in simulation
           self._apply_action(action)
           
           # Step simulation
           self._step_simulation()
           
           # Get new state
           obs = self._get_observation()
           
           # Calculate reward
           reward = self._calculate_reward()
           
           # Check termination conditions
           terminated = self._check_termination()
           truncated = self.current_step >= self.max_episode_steps
           
           # Increment step counter
           self.current_step += 1
           
           # Additional info
           info = {
               'distance_to_target': np.linalg.norm(self.robot_pos - self.target_pos)
           }
           
           return obs, reward, terminated, truncated, info
       
       def _apply_action(self, action):
           """Apply the given action to the robot"""
           # In a real implementation, this would send commands to joints
           # For this example, we'll simulate simple dynamics
           torque_scale = 50.0  # Scale factor for torques
           scaled_action = action * torque_scale
           
           # Update joint positions (simplified Euler integration)
           dt = 0.01  # Time step
           self.joint_velocities += scaled_action * dt * 0.1  # Acceleration
           self.joint_positions += self.joint_velocities * dt  # Velocity integration
           
           # Update robot position based on joint movements (very simplified)
           self.robot_pos += np.array([action[0] * dt * 0.1, action[1] * dt * 0.1, 0.0])
       
       def _step_simulation(self):
           """Step the simulation forward"""
           # In Isaac Sim, this would trigger one simulation step
           # For this example, we'll just update the IMU readings
           self.imu_readings = np.random.normal(0.0, 0.1, size=6)  # Add noise
           
           # In a real implementation, this would step physics in Isaac Sim
       
       def _get_observation(self):
           """Get current observation from the environment"""
           return np.concatenate([
               self.joint_positions,
               self.joint_velocities,
               self.imu_readings
           ])
       
       def _calculate_reward(self):
           """Calculate reward for current state"""
           # Distance to target
           dist_to_target = np.linalg.norm(self.robot_pos - self.target_pos)
           
           # Reward based on getting closer to target
           reward = -dist_to_target  # Negative distance (closer = higher reward)
           
           # Add bonus for reaching target
           if dist_to_target < 0.1:
               reward += 100  # Large bonus for reaching target
           
           # Penalty for taking too much action (energy efficiency)
           action_penalty = -0.01 * np.sum(np.abs(self.joint_velocities))
           reward += action_penalty
           
           return reward
       
       def _check_termination(self):
           """Check if episode should terminate"""
           # Terminate if robot reaches target
           dist_to_target = np.linalg.norm(self.robot_pos - self.target_pos)
           if dist_to_target < 0.1:
               return True
           
           # Don't terminate normally - let truncated handle max steps
           return False
       
       def close(self):
           """Clean up resources"""
           # Close Isaac Sim connections in real implementation
           pass
   ```

### Lab 2: Implementing SAC Algorithm for Robot Control

1. **Create a Soft Actor-Critic implementation** (`sac_robot_controller.py`):
   ```python
   #!/usr/bin/env python3

   import numpy as np
   import torch
   import torch.nn as nn
   import torch.optim as optim
   import torch.nn.functional as F
   from torch.distributions import Normal
   import random
   import copy
   from collections import namedtuple, deque
   import gymnasium as gym
   from robot_rl_env import RobotRLEnv
   import matplotlib.pyplot as plt

   # Experience replay buffer
   Transition = namedtuple('Transition', 
                          ('state', 'action', 'next_state', 'reward', 'done'))

   class ReplayBuffer:
       def __init__(self, capacity):
           self.buffer = deque(maxlen=capacity)
       
       def push(self, *args):
           self.buffer.append(Transition(*args))
       
       def sample(self, batch_size):
           indices = np.random.choice(len(self.buffer), batch_size, replace=False)
           samples = [self.buffer[idx] for idx in indices]
           return Transition(*zip(*samples))
       
       def __len__(self):
           return len(self.buffer)

   # Neural Network architectures
   class ValueNetwork(nn.Module):
       def __init__(self, state_dim, hidden_dim=256):
           super(ValueNetwork, self).__init__()
           
           self.fc1 = nn.Linear(state_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           self.fc3 = nn.Linear(hidden_dim, 1)
           
           # Initialize weights
           nn.init.xavier_uniform_(self.fc1.weight)
           nn.init.xavier_uniform_(self.fc2.weight)
           nn.init.xavier_uniform_(self.fc3.weight)
       
       def forward(self, state):
           x = F.relu(self.fc1(state))
           x = F.relu(self.fc2(x))
           x = self.fc3(x)
           return x

   class QNetwork(nn.Module):
       def __init__(self, state_dim, action_dim, hidden_dim=256):
           super(QNetwork, self).__init__()
           
           # Q1
           self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           self.fc3 = nn.Linear(hidden_dim, 1)
           
           # Q2
           self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)
           self.fc5 = nn.Linear(hidden_dim, hidden_dim)
           self.fc6 = nn.Linear(hidden_dim, 1)
           
           # Initialize weights
           for layer in [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6]:
               nn.init.xavier_uniform_(layer.weight)
       
       def forward(self, state, action):
           sa = torch.cat([state, action], 1)
           
           q1 = F.relu(self.fc1(sa))
           q1 = F.relu(self.fc2(q1))
           q1 = self.fc3(q1)
           
           q2 = F.relu(self.fc4(sa))
           q2 = F.relu(self.fc5(q2))
           q2 = self.fc6(q2)
           
           return q1, q2

   class GaussianPolicy(nn.Module):
       def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):
           super(GaussianPolicy, self).__init__()
           
           self.log_std_min = log_std_min
           self.log_std_max = log_std_max
           
           self.fc1 = nn.Linear(state_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           
           self.fc_mean = nn.Linear(hidden_dim, action_dim)
           self.fc_log_std = nn.Linear(hidden_dim, action_dim)
           
           # Initialize weights
           nn.init.xavier_uniform_(self.fc1.weight)
           nn.init.xavier_uniform_(self.fc2.weight)
           nn.init.xavier_uniform_(self.fc_mean.weight)
           nn.init.xavier_uniform_(self.fc_log_std.weight)
       
       def forward(self, state):
           x = F.relu(self.fc1(state))
           x = F.relu(self.fc2(x))
           
           mean = self.fc_mean(x)
           log_std = self.fc_log_std(x)
           log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
           
           return mean, log_std
       
       def sample(self, state):
           mean, log_std = self.forward(state)
           std = log_std.exp()
           
           normal = Normal(mean, std)
           x_t = normal.rsample()  # Reparameterization trick
           action = torch.tanh(x_t)  # Squash to [-1, 1]
           log_prob = normal.log_prob(x_t)
           
           # Compute log probability of squashed Gaussian (corrected for tanh)
           log_prob -= torch.log(1 - action.pow(2) + 1e-6)
           log_prob = log_prob.sum(1, keepdim=True)
           
           return action, log_prob

   # Soft Actor-Critic Agent
   class SACAgent:
       def __init__(self, state_dim, action_dim, hidden_dim=256, lr=3e-4, gamma=0.99, tau=5e-3,
                    alpha=0.2, device=torch.device("cuda" if torch.cuda.is_available() else "cpu")):
           
           self.device = device
           
           # Networks
           self.actor = GaussianPolicy(state_dim, action_dim, hidden_dim).to(device)
           self.critic = QNetwork(state_dim, action_dim, hidden_dim).to(device)
           self.critic_target = QNetwork(state_dim, action_dim, hidden_dim).to(device)
           self.value = ValueNetwork(state_dim, hidden_dim).to(device)
           self.value_target = ValueNetwork(state_dim, hidden_dim).to(device)
           
           # Copy critic to target networks
           self.critic_target.load_state_dict(self.critic.state_dict())
           self.value_target.load_state_dict(self.value.state_dict())
           
           # Optimizers
           self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
           self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
           self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)
           
           # Hyperparameters
           self.gamma = gamma
           self.tau = tau
           self.alpha = alpha
           self.target_entropy = -torch.prod(torch.Tensor(action_dim).to(device)).item()
           self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
           self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)
       
       def select_action(self, state, evaluate=False):
           """Select action using the policy"""
           state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
           
           if evaluate:
               # For evaluation, use mean action (deterministic)
               mean, _ = self.actor(state)
               action = torch.tanh(mean)
           else:
               # For training, sample from distribution
               action, _ = self.actor.sample(state)
           
           return action.cpu().data.numpy().flatten()
       
       def update_parameters(self, memory, batch_size):
           """Update the network parameters"""
           # Sample a batch from memory
           transitions = memory.sample(batch_size)
           batch = Transition(*zip(*transitions))
           
           # Convert to tensors
           state_batch = torch.FloatTensor(np.vstack(batch.state)).to(self.device)
           action_batch = torch.FloatTensor(np.vstack(batch.action)).to(self.device)
           next_state_batch = torch.FloatTensor(np.vstack(batch.next_state)).to(self.device)
           reward_batch = torch.FloatTensor(np.vstack(batch.reward)).to(self.device)
           done_mask = torch.BoolTensor(np.vstack(batch.done)).to(self.device)
           
           # Critic update
           with torch.no_grad():
               next_action, next_log_prob = self.actor.sample(next_state_batch)
               next_q_values = self.critic_target(next_state_batch, next_action)
               next_q_value = torch.min(*next_q_values) - self.alpha * next_log_prob
               expected_q_value = reward_batch + (self.gamma * next_q_value * ~done_mask)
           
           # Get current Q values
           current_q_values = self.critic(state_batch, action_batch)
           critic_loss = F.mse_loss(current_q_values[0], expected_q_value) + \
                         F.mse_loss(current_q_values[1], expected_q_value)
           
           # Optimize Critic
           self.critic_optimizer.zero_grad()
           critic_loss.backward()
           self.critic_optimizer.step()
           
           # Actor update
           predicted_action, predicted_log_prob = self.actor.sample(state_batch)
           predicted_q_value = self.critic(state_batch, predicted_action)
           predicted_q_value = torch.min(*predicted_q_value)
           actor_loss = (self.alpha * predicted_log_prob - predicted_q_value).mean()
           
           # Optimize Actor
           self.actor_optimizer.zero_grad()
           actor_loss.backward()
           self.actor_optimizer.step()
           
           # Temperature parameter update
           alpha_loss = -(self.log_alpha * (predicted_log_prob + self.target_entropy).detach()).mean()
           self.alpha_optimizer.zero_grad()
           alpha_loss.backward()
           self.alpha_optimizer.step()
           self.alpha = self.log_alpha.exp()
           
           # Soft update target networks
           for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
               target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
           
           return critic_loss.item(), actor_loss.item()
       
       def save_checkpoint(self, filepath):
           """Save the model checkpoint"""
           checkpoint = {
               'actor_state_dict': self.actor.state_dict(),
               'critic_state_dict': self.critic.state_dict(),
               'value_state_dict': self.value.state_dict(),
               'critic_target_state_dict': self.critic_target.state_dict(),
               'value_target_state_dict': self.value_target.state_dict(),
               'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
               'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
               'value_optimizer_state_dict': self.value_optimizer.state_dict(),
               'alpha': self.alpha,
               'log_alpha': self.log_alpha
           }
           torch.save(checkpoint, filepath)
       
       def load_checkpoint(self, filepath):
           """Load the model checkpoint"""
           checkpoint = torch.load(filepath)
           self.actor.load_state_dict(checkpoint['actor_state_dict'])
           self.critic.load_state_dict(checkpoint['critic_state_dict'])
           self.value.load_state_dict(checkpoint['value_state_dict'])
           self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])
           self.value_target.load_state_dict(checkpoint['value_target_state_dict'])
           self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
           self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
           self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])
           self.alpha = checkpoint['alpha']
           self.log_alpha = checkpoint['log_alpha']

   def train_sac_agent(env, episodes=1000, max_steps=1000):
       """Train the SAC agent"""
       # Get state and action dimensions
       state_dim = env.observation_space.shape[0]
       action_dim = env.action_space.shape[0]
       
       # Initialize agent
       agent = SACAgent(state_dim, action_dim)
       
       # Initialize replay buffer
       replay_buffer = ReplayBuffer(capacity=100000)
       
       # Training parameters
       batch_size = 256
       update_every = 1
       scores = []
       avg_scores = []
       
       for episode in range(episodes):
           state, _ = env.reset()
           total_reward = 0
           
           for step in range(max_steps):
               # Select action
               action = agent.select_action(state)
               
               # Take action in environment
               next_state, reward, terminated, truncated, info = env.step(action)
               done = terminated or truncated
               
               # Store transition in replay buffer
               replay_buffer.push(state, action, next_state, reward, done)
               
               # Update state
               state = next_state
               total_reward += reward
               
               # Update network parameters if enough samples available
               if len(replay_buffer) > batch_size and step % update_every == 0:
                   agent.update_parameters(replay_buffer, batch_size)
               
               if done:
                   break
           
           scores.append(total_reward)
           
           # Calculate average score over last 100 episodes
           if len(scores) >= 100:
               avg_score = np.mean(scores[-100:])
               avg_scores.append(avg_score)
           else:
               avg_scores.append(np.mean(scores))
           
           # Log progress
           if episode % 10 == 0:
               print(f'Episode {episode}, Average Score: {avg_scores[-1]:.2f}')
           
           # Stop if solved
           if avg_scores[-1] >= 90 and len(avg_scores) > 10:  # Adjustable threshold
               print(f'Solved in {episode} episodes!')
               break
       
       # Plot training progress
       plt.figure(figsize=(12, 5))
       
       plt.subplot(1, 2, 1)
       plt.plot(scores)
       plt.title('Training Scores')
       plt.xlabel('Episode')
       plt.ylabel('Score')
       
       plt.subplot(1, 2, 2)
       plt.plot(avg_scores)
       plt.title('Average Scores (100 Episode Window)')
       plt.xlabel('Episode')
       plt.ylabel('Average Score')
       
       plt.tight_layout()
       plt.savefig('sac_training_progress.png')
       plt.show()
       
       return agent, scores, avg_scores

   def main():
       """Main training function"""
       # Create environment
       env = RobotRLEnv(headless=True)
       
       print("Starting SAC training...")
       print(f"State dimension: {env.observation_space.shape[0]}")
       print(f"Action dimension: {env.action_space.shape[0]}")
       
       # Train the agent
       agent, scores, avg_scores = train_sac_agent(env, episodes=500, max_steps=500)
       
       # Save trained agent
       agent.save_checkpoint('sac_robot_agent.pth')
       print("Training complete. Model saved.")
       
       # Close environment
       env.close()

   if __name__ == "__main__":
       main()
   ```

### Lab 3: Domain Randomization for Sim-to-Real Transfer

1. **Create a domain randomization wrapper** (`domain_randomization_wrapper.py`):
   ```python
   #!/usr/bin/env python3

   import gymnasium as gym
   from gymnasium import spaces
   import numpy as np
   import random

   class DomainRandomizationWrapper(gym.Wrapper):
       """Domain randomization wrapper for RL environment"""
       
       def __init__(self, env, randomization_params=None):
           super().__init__(env)
           
           if randomization_params is None:
               # Default randomization parameters
               self.randomization_params = {
                   'friction_range': (0.1, 1.0),           # Range for friction coefficient
                   'mass_range': (0.8, 1.2),              # Range for link masses
                   'com_range': (-0.02, 0.02),            # Range for center of mass offsets
                   'motor_delay_range': (0.0, 0.02),       # Motor delay in seconds
                   'sensor_noise_std': 0.01,              # Standard deviation for sensor noise
                   'control_delay_range': (0.0, 0.01),     # Control delay in seconds
                   'gravity_range': (-1, 1),              # Range for gravity variation (m/sÂ²)
                   'visual_randomization': True            # Enable visual domain randomization
               }
           else:
               self.randomization_params = randomization_params
           
           # Store initial parameters
           self.initial_env_params = {}
           
           # Initialize randomized parameters
           self.current_randomization = {}
           
           # Apply initial randomization
           self.randomize_environment()
       
       def reset(self, seed=None, options=None):
           """Reset with randomization"""
           # Apply new randomization
           self.randomize_environment()
           
           # Reset the wrapped environment
           return self.env.reset(seed=seed, options=options)
       
       def step(self, action):
           """Step with potential control delay and noise"""
           # Add random control delay
           if random.random() < 0.1:  # 10% chance of delayed action
               delay = random.uniform(
                   self.randomization_params['control_delay_range'][0],
                   self.randomization_params['control_delay_range'][1]
               )
               # In a real implementation, we'd handle delayed action application
               pass
           
           # Apply action (possibly with modification for randomization)
           obs, reward, terminated, truncated, info = self.env.step(action)
           
           # Add sensor noise
           obs = self.add_sensor_noise(obs)
           
           # Modify reward based on randomization (to simulate reality gap)
           reward = self.modify_reward_for_realism(reward)
           
           return obs, reward, terminated, truncated, info
       
       def randomize_environment(self):
           """Randomize environment parameters"""
           # Randomize friction
           friction_coeff = random.uniform(
               self.randomization_params['friction_range'][0],
               self.randomization_params['friction_range'][1]
           )
           self.current_randomization['friction'] = friction_coeff
           
           # Randomize masses
           mass_multipliers = {}
           for i in range(6):  # Assuming 6 links
               multiplier = random.uniform(
                   self.randomization_params['mass_range'][0],
                   self.randomization_params['mass_range'][1]
               )
               mass_multipliers[f'link_{i}_mass'] = multiplier
           self.current_randomization['mass_multipliers'] = mass_multipliers
           
           # Randomize center of mass
           com_offsets = {}
           for i in range(6):
               offset = random.uniform(
                   self.randomization_params['com_range'][0],
                   self.randomization_params['com_range'][1]
               )
               com_offsets[f'link_{i}_com_offset'] = offset
           self.current_randomization['com_offsets'] = com_offsets
           
           # Randomize gravity
           gravity_variation = random.uniform(
               self.randomization_params['gravity_range'][0],
               self.randomization_params['gravity_range'][1]
           )
           self.current_randomization['gravity_variation'] = gravity_variation
           
           # Apply randomization to Isaac Sim or simulation parameters
           self.apply_randomization_to_simulation()
       
       def add_sensor_noise(self, obs):
           """Add noise to observations"""
           noise_std = self.randomization_params['sensor_noise_std']
           noise = np.random.normal(0, noise_std, size=obs.shape)
           return obs + noise
       
       def modify_reward_for_realism(self, reward):
           """Modify reward to account for sim-to-real considerations"""
           # In sim-to-real transfer, we might want to penalize behaviors that don't
           # translate well to the real world (e.g., overly aggressive motions)
           return reward
       
       def apply_randomization_to_simulation(self):
           """Apply randomization to the underlying simulation environment"""
           # In a real implementation, this would interface with Isaac Sim to modify:
           # - Friction parameters
           # - Link masses and inertias
           # - Center of mass offsets
           # - Joint damping
           # - Actuator properties
           # etc.
           pass
       
       def get_current_randomization(self):
           """Get current randomization parameters"""
           return self.current_randomization
       
       def sample_randomizations(self, num_samples=100):
           """Sample multiple randomizations for training diversity"""
           samples = []
           for _ in range(num_samples):
               # Temporarily store current randomization
               old_randomization = self.current_randomization.copy()
               
               # Apply new randomization
               self.randomize_environment()
               samples.append(self.current_randomization.copy())
               
               # Restore old randomization
               self.current_randomization = old_randomization
           
           return samples

   # Enhanced robot environment with domain randomization
   class RandomizedRobotRLEnv(DomainRandomizationWrapper):
       """Robot environment with domain randomization for sim-to-real transfer"""
       
       def __init__(self, headless=True, randomization_params=None):
           # Create base environment
           base_env = RobotRLEnv(headless=headless)
           
           # Wrap with domain randomization
           super().__init__(base_env, randomization_params)
           
           # Store original observation space and modify if needed
           self.original_observation_space = base_env.observation_space
       
       def get_randomization_info(self):
           """Get information about current randomization parameters"""
           return {
               'randomization_active': True,
               'current_params': self.get_current_randomization(),
               'param_ranges': self.randomization_params
           }

   # Example of how to train with domain randomization
   def train_with_domain_randomization():
       """Train an agent with domain randomization enabled"""
       # Define randomization parameters
       randomization_params = {
           'friction_range': (0.1, 1.5),
           'mass_range': (0.7, 1.3),
           'com_range': (-0.03, 0.03),
           'motor_delay_range': (0.0, 0.03),
           'sensor_noise_std': 0.015,
           'control_delay_range': (0.0, 0.015),
           'gravity_range': (-1.5, 1.5),
           'visual_randomization': True
       }
       
       # Create randomized environment
       env = RandomizedRobotRLEnv(headless=True, randomization_params=randomization_params)
       
       print("Environment with domain randomization created.")
       print(f"Randomization info: {env.get_randomization_info()}")
       
       # Proceed with training - the same SAC training code can now work with
       # the randomized environment
       
       # Example: Get randomization info during training
       obs, info = env.reset()
       print(f"Environment parameters randomized: {env.get_current_randomization()}")
       
       # Take a few steps to see how randomization affects the environment
       for i in range(5):
           action = env.action_space.sample()
           obs, reward, terminated, truncated, info = env.step(action)
           print(f"Step {i}: Obs shape={obs.shape}, Reward={reward:.2f}")
           
           if terminated or truncated:
               env.reset()
       
       env.close()
       print("Domain randomization training environment test complete.")

   if __name__ == "__main__":
       train_with_domain_randomization()
   ```

### Lab 4: Evaluating Sim-to-Real Transfer

1. **Create a transfer evaluation system** (`transfer_evaluation.py`):
   ```python
   #!/usr/bin/env python3

   import numpy as np
   import matplotlib.pyplot as plt
   from scipy.spatial.distance import pdist, squareform
   from sklearn.decomposition import PCA
   from sklearn.manifold import TSNE
   import seaborn as sns
   from collections import defaultdict
   import pandas as pd

   class TransferEvaluator:
       """Evaluate sim-to-real transfer effectiveness"""
       
       def __init__(self):
           self.simulation_episodes = []
           self.real_world_episodes = []
           self.transfer_metrics = {}
       
       def collect_simulation_data(self, agent, sim_env, num_episodes=100):
           """Collect data from simulation environment"""
           print(f"Collecting {num_episodes} episodes from simulation...")
           
           for ep in range(num_episodes):
               obs, _ = sim_env.reset()
               episode_data = {
                   'states': [],
                   'actions': [],
                   'rewards': [],
                   'next_states': [],
                   'dones': []
               }
               
               step_count = 0
               while True:
                   # Use trained agent to select action
                   action = agent.select_action(obs)
                   next_obs, reward, terminated, truncated, info = sim_env.step(action)
                   done = terminated or truncated
                   
                   # Store data
                   episode_data['states'].append(obs.copy())
                   episode_data['actions'].append(action.copy())
                   episode_data['rewards'].append(reward)
                   episode_data['next_states'].append(next_obs.copy())
                   episode_data['dones'].append(done)
                   
                   obs = next_obs
                   step_count += 1
                   
                   if done or step_count > 500:  # Max steps
                       break
               
               self.simulation_episodes.append(episode_data)
               
               if ep % 20 == 0:
                   print(f"  Collected {ep+1}/{num_episodes} simulation episodes")
           
           print(f"Simulation data collected for {len(self.simulation_episodes)} episodes")
       
       def collect_real_world_data(self, real_env, num_episodes=20):
           """Collect data from real robot (or a more realistic simulation)"""
           print(f"Collecting {num_episodes} episodes from real world...")
           
           for ep in range(num_episodes):
               obs, _ = real_env.reset()
               episode_data = {
                   'states': [],
                   'actions': [],
                   'rewards': [],
                   'next_states': [],
                   'dones': []
               }
               
               step_count = 0
               while True:
                   # For real evaluation, you might use a different control strategy
                   # or human demonstration - this is a placeholder
                   action = real_env.action_space.sample()  # Random action in this example
                   next_obs, reward, terminated, truncated, info = real_env.step(action)
                   done = terminated or truncated
                   
                   # Store data
                   episode_data['states'].append(obs.copy())
                   episode_data['actions'].append(action.copy())
                   episode_data['rewards'].append(reward)
                   episode_data['next_states'].append(next_obs.copy())
                   episode_data['dones'].append(done)
                   
                   obs = next_obs
                   step_count += 1
                   
                   if done or step_count > 500:  # Max steps
                       break
               
               self.real_world_episodes.append(episode_data)
               
               if ep % 5 == 0:
                   print(f"  Collected {ep+1}/{num_episodes} real world episodes")
           
           print(f"Real world data collected for {len(self.real_world_episodes)} episodes")
       
       def compute_transfer_score(self, agent, source_env, target_env, num_eval_episodes=10):
           """Compute transfer score by evaluating agent on target environment"""
           print("Computing transfer score...")
           
           total_rewards = []
           success_count = 0
           total_steps = 0
           
           for episode in range(num_eval_episodes):
               obs, _ = target_env.reset()
               episode_reward = 0
               steps = 0
               
               while True:
                   action = agent.select_action(obs, evaluate=True)
                   obs, reward, terminated, truncated, info = target_env.step(action)
                   done = terminated or truncated
                   
                   episode_reward += reward
                   steps += 1
                   total_steps += 1
                   
                   # Check for success condition (customizable)
                   if hasattr(target_env, 'is_successful') and target_env.is_successful():
                       success_count += 1
                   
                   if done or steps > 1000:  # Max steps
                       break
               
               total_rewards.append(episode_reward)
           
           avg_reward = np.mean(total_rewards)
           success_rate = success_count / num_eval_episodes
           avg_length = total_steps / num_eval_episodes
           
           self.transfer_metrics = {
               'avg_reward': avg_reward,
               'success_rate': success_rate,
               'avg_episode_length': avg_length,
               'eval_episodes': num_eval_episodes
           }
           
           print(f"Transfer Score Results:")
           print(f"  Average Reward: {avg_reward:.2f}")
           print(f"  Success Rate: {success_rate:.2f}")
           print(f"  Avg Episode Length: {avg_length:.2f}")
           
           return avg_reward, success_rate
       
       def analyze_dynamics_difference(self):
           """Analyze differences in dynamics between sim and real"""
           if not self.simulation_episodes or not self.real_world_episodes:
               print("Need to collect both sim and real data first")
               return
           
           # Compute state similarities
           sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])
           real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])
           
           # Use statistical tests to compare distributions
           sim_means = np.mean(sim_states, axis=0)
           real_means = np.mean(real_states, axis=0)
           
           sim_stds = np.std(sim_states, axis=0)
           real_stds = np.std(real_states, axis=0)
           
           differences = {
               'mean_diff': np.abs(sim_means - real_means),
               'std_diff': np.abs(sim_stds - real_stds),
               'relative_mean_diff': np.abs(sim_means - real_means) / np.abs(sim_means + 1e-8),
               'relative_std_diff': np.abs(sim_stds - real_stds) / np.abs(sim_stds + 1e-8)
           }
           
           print("\nDynamics Difference Analysis:")
           print(f"  Mean state difference: {np.mean(differences['mean_diff']):.4f}")
           print(f"  STD state difference: {np.mean(differences['std_diff']):.4f}")
           print(f"  Relative mean difference: {np.mean(differences['relative_mean_diff']):.4f}")
           print(f"  Relative STD difference: {np.mean(differences['relative_std_diff']):.4f}")
           
           return differences
       
       def visualize_state_space_alignment(self):
           """Visualize state space alignment between sim and real"""
           if not self.simulation_episodes or not self.real_world_episodes:
               print("Need to collect both sim and real data first")
               return
           
           # Sample states from both environments
           sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])
           real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])
           
           # Sample equal number of states
           min_len = min(len(sim_states), len(real_states))
           sim_sample = sim_states[np.random.choice(len(sim_states), min_len, replace=False)]
           real_sample = real_states[np.random.choice(len(real_states), min_len, replace=False)]
           
           # Combine samples for visualization
           all_states = np.vstack([sim_sample, real_sample])
           labels = ['Simulation'] * min_len + ['Real'] * min_len
           
           # Apply dimensionality reduction (PCA) for visualization
           pca = PCA(n_components=2)
           pca_result = pca.fit_transform(all_states)
           
           plt.figure(figsize=(12, 5))
           
           # PCA visualization
           plt.subplot(1, 2, 1)
           sim_mask = np.array(labels) == 'Simulation'
           real_mask = np.array(labels) == 'Real'
           
           plt.scatter(pca_result[sim_mask, 0], pca_result[sim_mask, 1], 
                      alpha=0.6, label='Simulation', c='blue')
           plt.scatter(pca_result[real_mask, 0], pca_result[real_mask, 1], 
                      alpha=0.6, label='Real', c='red')
           plt.title('State Space Alignment (PCA)')
           plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
           plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
           plt.legend()
           
           # Action comparison (if available)
           if self.simulation_episodes and self.real_world_episodes:
               sim_actions = np.concatenate([ep['actions'] for ep in self.simulation_episodes])
               real_actions = np.concatenate([ep['actions'] for ep in self.real_world_episodes])
               
               plt.subplot(1, 2, 2)
               plt.hist(sim_actions.flatten(), bins=50, alpha=0.5, label='Simulation Actions', density=True)
               plt.hist(real_actions.flatten(), bins=50, alpha=0.5, label='Real Actions', density=True)
               plt.title('Action Distribution Comparison')
               plt.xlabel('Action Values')
               plt.ylabel('Density')
               plt.legend()
           
           plt.tight_layout()
           plt.savefig('transfer_analysis.png')
           plt.show()
       
       def compute_divergence_metrics(self):
           """Compute divergence between sim and real distributions"""
           if not self.simulation_episodes or not self.real_world_episodes:
               print("Need to collect both sim and real data first")
               return
           
           # Sample states from both environments
           sim_states = np.concatenate([ep['states'] for ep in self.simulation_episodes])
           real_states = np.concatenate([ep['states'] for ep in self.real_world_episodes])
           
           # Compute MMD (Maximum Mean Discrepancy) approximation
           # This is a simplified version - in practice, use kernel methods
           sample_size = min(len(sim_states), len(real_states), 1000)
           sim_subsample = sim_states[np.random.choice(len(sim_states), sample_size, replace=False)]
           real_subsample = real_states[np.random.choice(len(real_states), sample_size, replace=False)]
           
           # Euclidean distance between samples
           distances = np.linalg.norm(sim_subsample[:, np.newaxis, :] - real_subsample[np.newaxis, :, :], axis=2)
           mmd_approx = np.mean(distances)  # Simplified MMD approximation
           
           # Store metrics
           self.divergence_metrics = {
               'mmd_approx': mmd_approx,
               'sim_mean': np.mean(sim_states, axis=0),
               'real_mean': np.mean(real_states, axis=0),
               'sim_std': np.std(sim_states, axis=0),
               'real_std': np.std(real_states, axis=0)
           }
           
           print(f"\nDivergence Metrics:")
           print(f"  MMD Approximation: {mmd_approx:.4f}")
           print(f"  Mean state difference: {np.mean(np.abs(self.divergence_metrics['sim_mean'] - self.divergence_metrics['real_mean'])):.4f}")
           
           return self.divergence_metrics

   def evaluate_transfer(agent, sim_env, real_env, num_eval_episodes=10):
       """Full transfer evaluation pipeline"""
       evaluator = TransferEvaluator()
       
       print("=== Sim-to-Real Transfer Evaluation ===")
       
       # Optionally collect fresh data (in practice, you'd already have this)
       # evaluator.collect_simulation_data(agent, sim_env)
       # evaluator.collect_real_world_data(real_env)
       
       # Compute transfer score
       avg_reward, success_rate = evaluator.compute_transfer_score(
           agent, sim_env, real_env, num_eval_episodes
       )
       
       # Perform analysis
       evaluator.analyze_dynamics_difference()
       evaluator.visualize_state_space_alignment()
       evaluator.compute_divergence_metrics()
       
       print(f"\n=== Summary ===")
       print(f"Transfer Success Rate: {success_rate:.2f}")
       print(f"Average Reward in Target Domain: {avg_reward:.2f}")
       
       return evaluator

   if __name__ == "__main__":
       # Example usage would require actual trained agent and environments
       print("Transfer evaluation module loaded.")
       print("Use evaluate_transfer() with trained agent and environments.")
   ```

## Runnable Code Example

Here's a complete example that demonstrates the entire RL training and evaluation pipeline:

```python
#!/usr/bin/env python3
# complete_rl_transfer_example.py

import numpy as np
import torch
import gymnasium as gym
from robot_rl_env import RobotRLEnv
from sac_robot_controller import SACAgent, train_sac_agent
from domain_randomization_wrapper import RandomizedRobotRLEnv
from transfer_evaluation import evaluate_transfer, TransferEvaluator

def create_environments_with_randomization():
    """Create environments with different levels of randomization"""
    
    # Base environment parameters
    base_params = {
        'friction_range': (0.1, 1.0),
        'mass_range': (0.8, 1.2),
        'com_range': (-0.02, 0.02),
        'sensor_noise_std': 0.01,
        'gravity_range': (-1, 1)
    }
    
    # High randomization environment (for training)
    high_random_params = {
        'friction_range': (0.05, 1.5),
        'mass_range': (0.6, 1.4),
        'com_range': (-0.05, 0.05),
        'sensor_noise_std': 0.03,
        'gravity_range': (-2, 2),
        'motor_delay_range': (0.0, 0.03),
        'control_delay_range': (0.0, 0.02)
    }
    
    # Low randomization environment (closer to real)
    low_random_params = {
        'friction_range': (0.7, 1.1),
        'mass_range': (0.9, 1.1),
        'com_range': (-0.01, 0.01),
        'sensor_noise_std': 0.005,
        'gravity_range': (-0.5, 0.5)
    }
    
    # Create environments
    base_env = RobotRLEnv(headless=True)
    high_random_env = RandomizedRobotRLEnv(headless=True, randomization_params=high_random_params)
    low_random_env = RandomizedRobotRLEnv(headless=True, randomization_params=low_random_params)
    
    return base_env, high_random_env, low_random_env

def train_with_domain_randomization():
    """Train agent with domain randomization for better sim-to-real transfer"""
    print("=== Training with Domain Randomization ===")
    
    # Create randomized environment
    _, high_random_env, _ = create_environments_with_randomization()
    
    print("Training agent with high domain randomization...")
    
    # Get state and action dimensions
    state_dim = high_random_env.observation_space.shape[0]
    action_dim = high_random_env.action_space.shape[0]
    
    # Initialize agent
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = SACAgent(state_dim, action_dim, device=device)
    
    # Training parameters
    episodes = 300
    max_steps = 300
    
    # Training loop
    replay_buffer = []  # Simplified placeholder
    batch_size = 256
    scores = []
    
    for episode in range(episodes):
        state, _ = high_random_env.reset()
        total_reward = 0
        
        for step in range(max_steps):
            # Select action with exploration noise
            action = agent.select_action(state, evaluate=False)
            
            # Take action in randomized environment
            next_state, reward, terminated, truncated, info = high_random_env.step(action)
            done = terminated or truncated
            
            # Store transition (simplified - in real impl, add to replay buffer)
            # replay_buffer.push(state, action, next_state, reward, done)
            
            state = next_state
            total_reward += reward
            
            # Update network periodically (simplified)
            if len(replay_buffer) > batch_size and step % 10 == 0:
                # agent.update_parameters(replay_buffer, batch_size)
                pass
            
            if done:
                break
        
        scores.append(total_reward)
        
        # Log progress
        if episode % 20 == 0:
            avg_score = np.mean(scores[-20:]) if len(scores) >= 20 else np.mean(scores)
            print(f'Episode {episode}, Average Score: {avg_score:.2f}')
            print(f'  Current randomization: {high_random_env.get_current_randomization()}')
    
    print(f"Training completed. Final average score: {np.mean(scores[-50:]):.2f}")
    
    return agent, high_random_env, scores

def run_complete_transfer_pipeline():
    """Run complete pipeline: train, evaluate transfer"""
    print("=== Complete RL Sim-to-Real Pipeline ===")
    
    # Step 1: Train with domain randomization
    agent, train_env, training_scores = train_with_domain_randomization()
    
    # Step 2: Create different environments for evaluation
    base_env, _, low_random_env = create_environments_with_randomization()
    
    # Step 3: Evaluate transfer performance
    evaluator = evaluate_transfer(agent, train_env, low_random_env, num_eval_episodes=5)
    
    # Step 4: Fine-tune if needed (transfer learning)
    print("\n=== Transfer Learning Adjustment ===")
    print("If transfer score is low, consider:")
    print("- Increasing domain randomization range")
    print("- Adding more realistic sensor noise models")
    print("- Implementing system identification for dynamics")
    print("- Using domain adaptation techniques")
    print("- Applying domain randomization curriculum")
    
    # Step 5: Final evaluation
    print("\n=== Final Results ===")
    print(f"Training completed with {len(training_scores)} episodes")
    print(f"Final training score: {training_scores[-1]:.2f}")
    print("Transfer evaluation completed successfully")
    
    # Calculate improvement metrics
    if len(training_scores) > 100:
        early_perf = np.mean(training_scores[:100])
        late_perf = np.mean(training_scores[-100:])
        improvement = (late_perf - early_perf) / (early_perf + 1e-8) * 100
        print(f"Learning improvement: {improvement:.2f}%")
    
    return agent, evaluator

def main():
    """Main function to run complete RL pipeline"""
    print("Starting Complete Reinforcement Learning & Sim-to-Real Pipeline")
    
    try:
        agent, evaluator = run_complete_transfer_pipeline()
        
        print("\nPipeline execution completed successfully!")
        print("The trained agent can now be tested on real hardware")
        print("(following safety protocols and with proper physical constraints)")
        
        # Save final model
        agent.save_checkpoint('final_rl_agent.pth')
        print("Final model saved as 'final_rl_agent.pth'")
        
    except Exception as e:
        print(f"Error during execution: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

## Mini-project

Create a complete reinforcement learning system for a specific robotic task (e.g., hopper locomotion, manipulator reaching) that:

1. Implements domain randomization techniques to improve sim-to-real transfer
2. Trains multiple RL algorithms (PPO, SAC, DDPG) and compares their performance
3. Evaluates the sim-to-real transfer gap using multiple metrics
4. Implements system identification to characterize the real robot's dynamics
5. Applies domain adaptation techniques to improve transfer
6. Tests the policy on a physical robot or realistic simulation
7. Documents the transfer performance and identifies key factors affecting success

Your project should include:
- Complete RL training pipeline with domain randomization
- Multiple RL algorithm implementations
- Transfer evaluation system with metrics
- Dynamics characterization and adaptation
- Performance comparison across algorithms
- Recommendations for improving sim-to-real transfer

## Summary

This chapter covered reinforcement learning and sim-to-real transfer:

- **RL Algorithms**: Deep reinforcement learning methods suitable for robotics
- **Domain Randomization**: Techniques to make sim-to-real transfer more robust
- **Transfer Evaluation**: Methods to assess the effectiveness of sim-to-real policies
- **Dynamics Modeling**: Approaches to characterize and adapt to reality gaps
- **System Identification**: Techniques to understand real robot dynamics
- **Practical Considerations**: Safety and practical constraints for real-world deployment

Successful sim-to-real transfer requires careful attention to the differences between simulation and reality, with domain randomization being one of the most effective techniques to create robust policies that generalize to the real world.